{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halnegheimish/ForcedInvalidation/blob/main/notebooks/naqanet_FI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code based on https://github.com/allenai/allennlp-reading-comprehension/blob/master/allennlp_rc/models/naqanet.py, with changes to account for Forced Invalidation"
      ],
      "metadata": {
        "id": "tIW_F_sP1Nxz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_18-vSQ75h7Q"
      },
      "outputs": [],
      "source": [
        "!pip install allennlp==2.1.0 allennlp-models==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWWAoRFiriU4"
      },
      "outputs": [],
      "source": [
        "!pip install spacy-transformers==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy --folder https://drive.google.com/drive/folders/1m-Q7M-yvwHSI11peuMkwssphQ6J4q6-N?usp=share_link"
      ],
      "metadata": {
        "id": "IiFx5WCmvnWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install subversion"
      ],
      "metadata": {
        "id": "LBufOIzeWo3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!svn checkout https://github.com/halnegheimish/ForcedInvalidation/trunk/data/eval/drop/\n"
      ],
      "metadata": {
        "id": "lNysATAcW0Eu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRyvM0C9_lGV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "seed = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "serialization_dir= f\"naqanet_invalid_seed_{seed}\"\n",
        "os.makedirs(serialization_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "ORNi_736xsUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQtrNbeU_7ay"
      },
      "outputs": [],
      "source": [
        "augmented_train_data_path= 'drop/drop_aug_train_ngrams.json'\n",
        "augmented_dev_data_path=   'drop/drop_aug_val_ngrams.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aZZ5lYZ_jBM"
      },
      "source": [
        "#code "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqGVUGMf5vyZ"
      },
      "outputs": [],
      "source": [
        "#naqanet imports\n",
        "from typing import Any, Dict, List, Optional\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "\n",
        "from allennlp.data import Vocabulary\n",
        "from allennlp.models.model import Model\n",
        "from allennlp.modules import Highway\n",
        "from allennlp.nn.activations import Activation\n",
        "from allennlp.modules.feedforward import FeedForward\n",
        "from allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\n",
        "from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\n",
        "from allennlp.nn import util, InitializerApplicator, RegularizerApplicator\n",
        "from allennlp.nn.util import masked_softmax\n",
        "\n",
        "from allennlp_models.rc.models.utils import (\n",
        "    get_best_span,\n",
        "    replace_masked_values_with_big_negative_number,\n",
        ")\n",
        "from allennlp_models.rc.metrics.drop_em_and_f1 import DropEmAndF1\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "#train util imports\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "#set random seeds\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "#based on original drop reader https://github.com/allenai/allennlp-models/blob/main/allennlp_models/rc/dataset_readers/drop.py\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Union, Tuple, Any\n",
        "\n",
        "from overrides import overrides\n",
        "from word2number.w2n import word_to_num\n",
        "\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.fields import (\n",
        "    Field,\n",
        "    TextField,\n",
        "    MetadataField,\n",
        "    LabelField,\n",
        "    ListField,\n",
        "    SequenceLabelField,\n",
        "    SpanField,\n",
        "    IndexField,\n",
        ")\n",
        "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
        "from allennlp.data.instance import Instance\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\n",
        "from allennlp.data.tokenizers import Token, Tokenizer, SpacyTokenizer\n",
        "\n",
        "from allennlp_models.rc.dataset_readers.utils import (\n",
        "    IGNORED_TOKENS,\n",
        "    STRIPPED_CHARACTERS,\n",
        "    make_reading_comprehension_instance,\n",
        "    split_tokens_by_hyphen,\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "WORD_NUMBER_MAP = {\n",
        "    \"zero\": 0,\n",
        "    \"one\": 1,\n",
        "    \"two\": 2,\n",
        "    \"three\": 3,\n",
        "    \"four\": 4,\n",
        "    \"five\": 5,\n",
        "    \"six\": 6,\n",
        "    \"seven\": 7,\n",
        "    \"eight\": 8,\n",
        "    \"nine\": 9,\n",
        "    \"ten\": 10,\n",
        "    \"eleven\": 11,\n",
        "    \"twelve\": 12,\n",
        "    \"thirteen\": 13,\n",
        "    \"fourteen\": 14,\n",
        "    \"fifteen\": 15,\n",
        "    \"sixteen\": 16,\n",
        "    \"seventeen\": 17,\n",
        "    \"eighteen\": 18,\n",
        "    \"nineteen\": 19,\n",
        "}\n",
        "\n",
        "class DropShuffReaderInvalid(DatasetReader):\n",
        "    \"\"\"\n",
        "    Reads a JSON-formatted DROP dataset file and returns instances in a few different possible\n",
        "    formats.  The input format is complicated; see the test fixture for an example of what it looks\n",
        "    like.  The output formats all contain a question ``TextField``, a passage ``TextField``, and\n",
        "    some kind of answer representation.  Because DROP has instances with several different kinds of\n",
        "    answers, this dataset reader allows you to filter out questions that do not have answers of a\n",
        "    particular type (e.g., remove questions that have numbers as answers, if you model can only\n",
        "    give passage spans as answers).  We typically return all possible ways of arriving at a given\n",
        "    answer string, and expect models to marginalize over these possibilities.\n",
        "    # Parameters\n",
        "    tokenizer : `Tokenizer`, optional (default=`SpacyTokenizer()`)\n",
        "        We use this `Tokenizer` for both the question and the passage.  See :class:`Tokenizer`.\n",
        "        Default is `SpacyTokenizer()`.\n",
        "    token_indexers : `Dict[str, TokenIndexer]`, optional\n",
        "        We similarly use this for both the question and the passage.  See :class:`TokenIndexer`.\n",
        "        Default is `{\"tokens\": SingleIdTokenIndexer()}`.\n",
        "    passage_length_limit : `int`, optional (default=`None`)\n",
        "        If specified, we will cut the passage if the length of passage exceeds this limit.\n",
        "    question_length_limit : `int`, optional (default=`None`)\n",
        "        If specified, we will cut the question if the length of passage exceeds this limit.\n",
        "    skip_when_all_empty: `List[str]`, optional (default=`None`)\n",
        "        In some cases such as preparing for training examples, you may want to skip some examples\n",
        "        when there are no gold labels. You can specify on what condition should the examples be\n",
        "        skipped. Currently, you can put \"passage_span\", \"question_span\", \"addition_subtraction\",\n",
        "        or \"counting\" in this list, to tell the reader skip when there are no such label found.\n",
        "        If not specified, we will keep all the examples.\n",
        "    instance_format: `str`, optional (default=`\"drop\"`)\n",
        "        We try to be generous in providing a few different formats for the instances in DROP,\n",
        "        in terms of the `Fields` that we return for each `Instance`, to allow for several\n",
        "        different kinds of models.  \"drop\" format will do processing to detect numbers and\n",
        "        various ways those numbers can be arrived at from the passage, and return `Fields`\n",
        "        related to that.  \"bert\" format only allows passage spans as answers, and provides a\n",
        "        \"question_and_passage\" field with the two pieces of text joined as BERT expects.\n",
        "        \"squad\" format provides the same fields that our BiDAF and other SQuAD models expect.\n",
        "    relaxed_span_match_for_finding_labels : `bool`, optional (default=`True`)\n",
        "        DROP dataset contains multi-span answers, and the date-type answers are usually hard to\n",
        "        find exact span matches for, also.  In order to use as many examples as possible\n",
        "        to train the model, we may not want a strict match for such cases when finding the gold\n",
        "        span labels. If this argument is true, we will treat every span in the multi-span\n",
        "        answers as correct, and every token in the date answer as correct, too.  Because models\n",
        "        trained on DROP typically marginalize over all possible answer positions, this is just\n",
        "        being a little more generous in what is being marginalized.  Note that this will not\n",
        "        affect evaluation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer = None,\n",
        "        token_indexers: Dict[str, TokenIndexer] = None,\n",
        "        passage_length_limit: int = None,\n",
        "        question_length_limit: int = None,\n",
        "        skip_when_all_empty: List[str] = None,\n",
        "        instance_format: str = \"drop\",\n",
        "        relaxed_span_match_for_finding_labels: bool = True,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self._tokenizer = tokenizer or SpacyTokenizer()\n",
        "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
        "        self.passage_length_limit = passage_length_limit\n",
        "        self.question_length_limit = question_length_limit\n",
        "        self.skip_when_all_empty = skip_when_all_empty if skip_when_all_empty is not None else []\n",
        "        for item in self.skip_when_all_empty:\n",
        "            assert item in [\n",
        "                \"passage_span\",\n",
        "                \"question_span\",\n",
        "                \"addition_subtraction\",\n",
        "                \"counting\",\n",
        "                \"invalid\",\n",
        "            ], f\"Unsupported skip type: {item}\"\n",
        "        self.instance_format = instance_format\n",
        "        self.relaxed_span_match_for_finding_labels = relaxed_span_match_for_finding_labels\n",
        "\n",
        "    @overrides\n",
        "    def _read(self, file_path: str):\n",
        "        # if `file_path` is a URL, redirect to the cache\n",
        "        file_path = cached_path(file_path, extract_archive=True)\n",
        "        logger.info(\"Reading file at %s\", file_path)\n",
        "        with open(file_path) as dataset_file:\n",
        "            dataset = json.load(dataset_file)\n",
        "        logger.info(\"Reading the dataset\")\n",
        "        kept_count, skip_count = 0, 0\n",
        "        for passage_id, passage_info in dataset.items():\n",
        "            passage_text = passage_info[\"passage\"]\n",
        "            passage_tokens = self._tokenizer.tokenize(passage_text)\n",
        "            passage_tokens = split_tokens_by_hyphen(passage_tokens)\n",
        "            for question_answer in passage_info[\"qa_pairs\"]:\n",
        "                question_id = question_answer[\"query_id\"]\n",
        "                question_text = question_answer[\"question\"].strip()\n",
        "                \n",
        "\n",
        "                answer_annotations = []\n",
        "                if \"answer\" in question_answer:\n",
        "                    answer_annotations.append(question_answer[\"answer\"])\n",
        "                if \"validated_answers\" in question_answer:\n",
        "                    answer_annotations += question_answer[\"validated_answers\"]\n",
        "\n",
        "                instance = self.text_to_instance(\n",
        "                    question_text,\n",
        "                    passage_text,\n",
        "                    question_id,\n",
        "                    passage_id,\n",
        "                    answer_annotations,\n",
        "                    passage_tokens,\n",
        "                )\n",
        "                if instance is not None:\n",
        "                    kept_count += 1\n",
        "                    yield instance\n",
        "                else:\n",
        "                    skip_count += 1\n",
        "        print(f\"Skipped {skip_count} questions, kept {kept_count} questions.\")\n",
        "        logger.info(f\"Skipped {skip_count} questions, kept {kept_count} questions.\")\n",
        "\n",
        "    @overrides\n",
        "    def text_to_instance(\n",
        "        self,  # type: ignore\n",
        "        question_text: str,\n",
        "        passage_text: str,\n",
        "        question_id: str = None,\n",
        "        passage_id: str = None,\n",
        "        answer_annotations: List[Dict] = None,\n",
        "        passage_tokens: List[Token] = None,\n",
        "    ) -> Union[Instance, None]:\n",
        "\n",
        "        if not passage_tokens:\n",
        "            passage_tokens = self._tokenizer.tokenize(passage_text)\n",
        "            passage_tokens = split_tokens_by_hyphen(passage_tokens)\n",
        "        question_tokens = self._tokenizer.tokenize(question_text)\n",
        "        question_tokens = split_tokens_by_hyphen(question_tokens)\n",
        "\n",
        "        if self.passage_length_limit is not None:\n",
        "            passage_tokens = passage_tokens[: self.passage_length_limit]\n",
        "        if self.question_length_limit is not None:\n",
        "            question_tokens = question_tokens[: self.question_length_limit]\n",
        "\n",
        "        answer_type: str = None\n",
        "        answer_texts: List[str] = []\n",
        "        if answer_annotations:\n",
        "\n",
        "            answer_type, answer_texts = self.extract_answer_info_from_annotation(\n",
        "                answer_annotations[0]\n",
        "            )\n",
        "\n",
        "        # Tokenize the answer text in order to find the matched span based on token\n",
        "        tokenized_answer_texts = []\n",
        "        for answer_text in answer_texts:\n",
        "            answer_tokens = self._tokenizer.tokenize(answer_text)\n",
        "            answer_tokens = split_tokens_by_hyphen(answer_tokens)\n",
        "            tokenized_answer_texts.append(\" \".join(token.text for token in answer_tokens))\n",
        "\n",
        "        if self.instance_format == \"squad\":\n",
        "            valid_passage_spans = (\n",
        "                self.find_valid_spans(passage_tokens, tokenized_answer_texts)\n",
        "                if tokenized_answer_texts\n",
        "                else []\n",
        "            )\n",
        "            if not valid_passage_spans:\n",
        "                if \"passage_span\" in self.skip_when_all_empty:\n",
        "                    return None\n",
        "                else:\n",
        "                    valid_passage_spans.append((len(passage_tokens) - 1, len(passage_tokens) - 1))\n",
        "            return make_reading_comprehension_instance(\n",
        "                question_tokens,\n",
        "                passage_tokens,\n",
        "                self._token_indexers,\n",
        "                passage_text,\n",
        "                valid_passage_spans,\n",
        "                answer_texts,\n",
        "                additional_metadata={\n",
        "                    \"original_passage\": passage_text,\n",
        "                    \"original_question\": question_text,\n",
        "                    \"passage_id\": passage_id,\n",
        "                    \"question_id\": question_id,\n",
        "                    \"valid_passage_spans\": valid_passage_spans,\n",
        "                    \"answer_annotations\": answer_annotations,\n",
        "                },\n",
        "            )\n",
        "        elif self.instance_format == \"bert\":\n",
        "            question_concat_passage_tokens = question_tokens + [Token(\"[SEP]\")] + passage_tokens\n",
        "            valid_passage_spans = []\n",
        "            for span in self.find_valid_spans(passage_tokens, tokenized_answer_texts):\n",
        "                # This span is for `question + [SEP] + passage`.\n",
        "                valid_passage_spans.append(\n",
        "                    (span[0] + len(question_tokens) + 1, span[1] + len(question_tokens) + 1)\n",
        "                )\n",
        "            if not valid_passage_spans:\n",
        "                if \"passage_span\" in self.skip_when_all_empty:\n",
        "                    return None\n",
        "                else:\n",
        "                    valid_passage_spans.append(\n",
        "                        (\n",
        "                            len(question_concat_passage_tokens) - 1,\n",
        "                            len(question_concat_passage_tokens) - 1,\n",
        "                        )\n",
        "                    )\n",
        "            answer_info = {\n",
        "                \"answer_texts\": answer_texts,  \n",
        "                \"answer_passage_spans\": valid_passage_spans,\n",
        "            }\n",
        "            return self.make_bert_drop_instance(\n",
        "                question_tokens,\n",
        "                passage_tokens,\n",
        "                question_concat_passage_tokens,\n",
        "                self._token_indexers,\n",
        "                passage_text,\n",
        "                answer_info,\n",
        "                additional_metadata={\n",
        "                    \"original_passage\": passage_text,\n",
        "                    \"original_question\": question_text,\n",
        "                    \"passage_id\": passage_id,\n",
        "                    \"question_id\": question_id,\n",
        "                    \"answer_annotations\": answer_annotations,\n",
        "                },\n",
        "            )\n",
        "\n",
        "        elif self.instance_format == \"drop\":\n",
        "            numbers_in_passage = []\n",
        "            number_indices = []\n",
        "            for token_index, token in enumerate(passage_tokens):\n",
        "                number = self.convert_word_to_number(token.text)\n",
        "                if number is not None:\n",
        "                    numbers_in_passage.append(number)\n",
        "                    number_indices.append(token_index)\n",
        "            # hack to guarantee minimal length of padded number\n",
        "            numbers_in_passage.append(0)\n",
        "            number_indices.append(-1)\n",
        "            numbers_as_tokens = [Token(str(number)) for number in numbers_in_passage]\n",
        "\n",
        "            valid_passage_spans = (\n",
        "                self.find_valid_spans(passage_tokens, tokenized_answer_texts)\n",
        "                if tokenized_answer_texts\n",
        "                else []\n",
        "            )\n",
        "            valid_question_spans = (\n",
        "                self.find_valid_spans(question_tokens, tokenized_answer_texts)\n",
        "                if tokenized_answer_texts\n",
        "                else []\n",
        "            )\n",
        "\n",
        "\n",
        "            target_numbers = []\n",
        "            for answer_text in answer_texts:\n",
        "                number = self.convert_word_to_number(answer_text)\n",
        "                if number is not None:\n",
        "                    target_numbers.append(number)\n",
        "            valid_signs_for_add_sub_expressions: List[List[int]] = []\n",
        "            valid_counts: List[int] = []\n",
        "            invalid_answer: List[int] = []\n",
        "            if answer_type in [\"number\", \"date\"]:\n",
        "                valid_signs_for_add_sub_expressions = self.find_valid_add_sub_expressions(\n",
        "                    numbers_in_passage, target_numbers\n",
        "                )\n",
        "            if answer_type in [\"number\"]:\n",
        "                # Currently we only support count number 0 ~ 9\n",
        "                numbers_for_count = list(range(10))\n",
        "                valid_counts = self.find_valid_counts(numbers_for_count, target_numbers)\n",
        "\n",
        "            if answer_type in ['invalid']:\n",
        "                invalid_answer= target_numbers\n",
        "            \n",
        "            type_to_answer_map = {\n",
        "                \"passage_span\": valid_passage_spans,\n",
        "                \"question_span\": valid_question_spans,\n",
        "                \"addition_subtraction\": valid_signs_for_add_sub_expressions,\n",
        "                \"counting\": valid_counts,\n",
        "                \"invalid\": invalid_answer,\n",
        "            }\n",
        "\n",
        "            if self.skip_when_all_empty and not any(\n",
        "                type_to_answer_map[skip_type] for skip_type in self.skip_when_all_empty\n",
        "            ):\n",
        "                return None\n",
        "\n",
        "            \n",
        "\n",
        "            answer_info = {\n",
        "                \"answer_texts\": answer_texts,  \n",
        "                \"answer_passage_spans\": valid_passage_spans,\n",
        "                \"answer_question_spans\": valid_question_spans,\n",
        "                \"signs_for_add_sub_expressions\": valid_signs_for_add_sub_expressions,\n",
        "                \"counts\": valid_counts,\n",
        "                \"invalid\":invalid_answer,\n",
        "            }\n",
        "\n",
        "            return self.make_marginal_drop_instance(\n",
        "                question_tokens,\n",
        "                passage_tokens,\n",
        "                numbers_as_tokens,\n",
        "                number_indices,\n",
        "                self._token_indexers,\n",
        "                passage_text,\n",
        "                answer_info,\n",
        "                additional_metadata={\n",
        "                    \"original_passage\": passage_text,\n",
        "                    \"original_question\": question_text,\n",
        "                    \"original_numbers\": numbers_in_passage,\n",
        "                    \"passage_id\": passage_id,\n",
        "                    \"question_id\": question_id,\n",
        "                    \"answer_info\": answer_info,\n",
        "                    \"answer_annotations\": answer_annotations,\n",
        "                },\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f'Expect the instance format to be \"drop\", \"squad\" or \"bert\", '\n",
        "                f\"but got {self.instance_format}\"\n",
        "            )\n",
        "\n",
        "   \n",
        "\n",
        "    @staticmethod\n",
        "    def make_marginal_drop_instance(\n",
        "        question_tokens: List[Token],\n",
        "        passage_tokens: List[Token],\n",
        "        number_tokens: List[Token],\n",
        "        number_indices: List[int],\n",
        "        token_indexers: Dict[str, TokenIndexer],\n",
        "        passage_text: str,\n",
        "        answer_info: Dict[str, Any] = None,\n",
        "        additional_metadata: Dict[str, Any] = None,\n",
        "    ) -> Instance:\n",
        "        additional_metadata = additional_metadata or {}\n",
        "        fields: Dict[str, Field] = {}\n",
        "        passage_offsets = [(token.idx, token.idx + len(token.text)) for token in passage_tokens]\n",
        "        question_offsets = [(token.idx, token.idx + len(token.text)) for token in question_tokens]\n",
        "\n",
        "        passage_field = TextField(passage_tokens, token_indexers)\n",
        "        question_field = TextField(question_tokens, token_indexers)\n",
        "\n",
        "        fields[\"passage\"] = passage_field\n",
        "        fields[\"question\"] = question_field\n",
        "\n",
        "        number_index_fields: List[Field] = [\n",
        "            IndexField(index, passage_field) for index in number_indices\n",
        "        ]\n",
        "        fields[\"number_indices\"] = ListField(number_index_fields)\n",
        "      \n",
        "        numbers_in_passage_field = TextField(number_tokens, token_indexers)\n",
        "        metadata = {\n",
        "            \"original_passage\": passage_text,\n",
        "            \"passage_token_offsets\": passage_offsets,\n",
        "            \"question_token_offsets\": question_offsets,\n",
        "            \"question_tokens\": [token.text for token in question_tokens],\n",
        "            \"passage_tokens\": [token.text for token in passage_tokens],\n",
        "            \"number_tokens\": [token.text for token in number_tokens],\n",
        "            \"number_indices\": number_indices,\n",
        "        }\n",
        "        if answer_info:\n",
        "            metadata[\"answer_texts\"] = answer_info[\"answer_texts\"]\n",
        "\n",
        "            passage_span_fields: List[Field] = [\n",
        "                SpanField(span[0], span[1], passage_field)\n",
        "                for span in answer_info[\"answer_passage_spans\"]\n",
        "            ]\n",
        "            if not passage_span_fields:\n",
        "                passage_span_fields.append(SpanField(-1, -1, passage_field))\n",
        "            fields[\"answer_as_passage_spans\"] = ListField(passage_span_fields)\n",
        "\n",
        "            question_span_fields: List[Field] = [\n",
        "                SpanField(span[0], span[1], question_field)\n",
        "                for span in answer_info[\"answer_question_spans\"]\n",
        "            ]\n",
        "            if not question_span_fields:\n",
        "                question_span_fields.append(SpanField(-1, -1, question_field))\n",
        "            fields[\"answer_as_question_spans\"] = ListField(question_span_fields)\n",
        "\n",
        "            add_sub_signs_field: List[Field] = []\n",
        "            for signs_for_one_add_sub_expression in answer_info[\"signs_for_add_sub_expressions\"]:\n",
        "                add_sub_signs_field.append(\n",
        "                    SequenceLabelField(signs_for_one_add_sub_expression, numbers_in_passage_field)\n",
        "                )\n",
        "            if not add_sub_signs_field:\n",
        "                add_sub_signs_field.append(\n",
        "                    SequenceLabelField([0] * len(number_tokens), numbers_in_passage_field)\n",
        "                )\n",
        "            fields[\"answer_as_add_sub_expressions\"] = ListField(add_sub_signs_field)\n",
        "\n",
        "            count_fields: List[Field] = [\n",
        "                LabelField(count_label, skip_indexing=True) for count_label in answer_info[\"counts\"]\n",
        "            ]\n",
        "            if not count_fields:\n",
        "                count_fields.append(LabelField(-1, skip_indexing=True))\n",
        "            fields[\"answer_as_counts\"] = ListField(count_fields)\n",
        "\n",
        "            \n",
        "            invalid_fields: List[Field] = [\n",
        "                LabelField(invalid_label, skip_indexing=True) for invalid_label in answer_info[\"invalid\"]\n",
        "            ]\n",
        "            if not invalid_fields:\n",
        "                invalid_fields.append(LabelField(-1, skip_indexing=True))\n",
        "            fields[\"answer_invalid\"] = ListField(invalid_fields)\n",
        "\n",
        "        metadata.update(additional_metadata)\n",
        "        fields[\"metadata\"] = MetadataField(metadata)\n",
        "        return Instance(fields)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_bert_drop_instance(\n",
        "        question_tokens: List[Token],\n",
        "        passage_tokens: List[Token],\n",
        "        question_concat_passage_tokens: List[Token],\n",
        "        token_indexers: Dict[str, TokenIndexer],\n",
        "        passage_text: str,\n",
        "        answer_info: Dict[str, Any] = None,\n",
        "        additional_metadata: Dict[str, Any] = None,\n",
        "    ) -> Instance:\n",
        "        additional_metadata = additional_metadata or {}\n",
        "        fields: Dict[str, Field] = {}\n",
        "        passage_offsets = [(token.idx, token.idx + len(token.text)) for token in passage_tokens]\n",
        "\n",
        "        # This is separate so we can reference it later with a known type.\n",
        "        passage_field = TextField(passage_tokens, token_indexers)\n",
        "        question_field = TextField(question_tokens, token_indexers)\n",
        "        fields[\"passage\"] = passage_field\n",
        "        fields[\"question\"] = question_field\n",
        "        question_and_passage_field = TextField(question_concat_passage_tokens, token_indexers)\n",
        "        fields[\"question_and_passage\"] = question_and_passage_field\n",
        "\n",
        "        metadata = {\n",
        "            \"original_passage\": passage_text,\n",
        "            \"passage_token_offsets\": passage_offsets,\n",
        "            \"question_tokens\": [token.text for token in question_tokens],\n",
        "            \"passage_tokens\": [token.text for token in passage_tokens],\n",
        "        }\n",
        "\n",
        "        if answer_info:\n",
        "            metadata[\"answer_texts\"] = answer_info[\"answer_texts\"]\n",
        "\n",
        "            passage_span_fields: List[Field] = [\n",
        "                SpanField(span[0], span[1], question_and_passage_field)\n",
        "                for span in answer_info[\"answer_passage_spans\"]\n",
        "            ]\n",
        "            if not passage_span_fields:\n",
        "                passage_span_fields.append(SpanField(-1, -1, question_and_passage_field))\n",
        "            fields[\"answer_as_passage_spans\"] = ListField(passage_span_fields)\n",
        "\n",
        "        metadata.update(additional_metadata)\n",
        "        fields[\"metadata\"] = MetadataField(metadata)\n",
        "        return Instance(fields)\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_answer_info_from_annotation(\n",
        "        answer_annotation: Dict[str, Any]\n",
        "    ) -> Tuple[str, List[str]]:\n",
        "        answer_type = None\n",
        "        if answer_annotation[\"spans\"]:\n",
        "            answer_type = \"spans\"\n",
        "        elif answer_annotation[\"number\"]:\n",
        "            answer_type = \"number\"\n",
        "        elif any(answer_annotation[\"date\"].values()):\n",
        "            answer_type = \"date\"\n",
        "        elif answer_annotation[\"invalid\"]:\n",
        "            answer_type= \"invalid\"\n",
        "\n",
        "        answer_content = answer_annotation[answer_type] if answer_type is not None else None\n",
        "\n",
        "        answer_texts: List[str] = []\n",
        "        if answer_type is None:  # No answer\n",
        "            pass\n",
        "        elif answer_type == \"spans\":\n",
        "            # answer_content is a list of string in this case\n",
        "            answer_texts = answer_content\n",
        "        elif answer_type == \"date\":\n",
        "            # answer_content is a dict with \"month\", \"day\", \"year\" as the keys\n",
        "            date_tokens = [\n",
        "                answer_content[key]\n",
        "                for key in [\"month\", \"day\", \"year\"]\n",
        "                if key in answer_content and answer_content[key]\n",
        "            ]\n",
        "            answer_texts = date_tokens\n",
        "        elif answer_type == \"number\":\n",
        "            # answer_content is a string of number\n",
        "            answer_texts = [answer_content]\n",
        "\n",
        "        elif answer_type == \"invalid\":\n",
        "            answer_texts = [answer_content]\n",
        "\n",
        "        return answer_type, answer_texts\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_word_to_number(word: str, try_to_include_more_numbers=False):\n",
        "        \"\"\"\n",
        "        Currently we only support limited types of conversion.\n",
        "        \"\"\"\n",
        "        if try_to_include_more_numbers:\n",
        "            # strip all punctuations from the sides of the word, except for the negative sign\n",
        "            punctruations = string.punctuation.replace(\"-\", \"\")\n",
        "            word = word.strip(punctruations)\n",
        "            # some words may contain the comma as deliminator\n",
        "            word = word.replace(\",\", \"\")\n",
        "            # word2num will convert hundred, thousand ... to number, but we skip it.\n",
        "            if word in [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]:\n",
        "                return None\n",
        "            try:\n",
        "                number = word_to_num(word)\n",
        "            except ValueError:\n",
        "                try:\n",
        "                    number = int(word)\n",
        "                except ValueError:\n",
        "                    try:\n",
        "                        number = float(word)\n",
        "                    except ValueError:\n",
        "                        number = None\n",
        "            return number\n",
        "        else:\n",
        "            no_comma_word = word.replace(\",\", \"\")\n",
        "            if no_comma_word in WORD_NUMBER_MAP:\n",
        "                number = WORD_NUMBER_MAP[no_comma_word]\n",
        "            else:\n",
        "                try:\n",
        "                    number = int(no_comma_word)\n",
        "                except ValueError:\n",
        "                    number = None\n",
        "            return number\n",
        "\n",
        "    @staticmethod\n",
        "    def find_valid_spans(\n",
        "        passage_tokens: List[Token], answer_texts: List[str]\n",
        "    ) -> List[Tuple[int, int]]:\n",
        "        normalized_tokens = [\n",
        "            token.text.lower().strip(STRIPPED_CHARACTERS) for token in passage_tokens\n",
        "        ]\n",
        "        word_positions: Dict[str, List[int]] = defaultdict(list)\n",
        "        for i, token in enumerate(normalized_tokens):\n",
        "            word_positions[token].append(i)\n",
        "        spans = []\n",
        "        for answer_text in answer_texts:\n",
        "            answer_tokens = answer_text.lower().strip(STRIPPED_CHARACTERS).split()\n",
        "            num_answer_tokens = len(answer_tokens)\n",
        "            if answer_tokens[0] not in word_positions:\n",
        "                continue\n",
        "            for span_start in word_positions[answer_tokens[0]]:\n",
        "                span_end = span_start  # span_end is _inclusive_\n",
        "                answer_index = 1\n",
        "                while answer_index < num_answer_tokens and span_end + 1 < len(normalized_tokens):\n",
        "                    token = normalized_tokens[span_end + 1]\n",
        "                    if answer_tokens[answer_index].strip(STRIPPED_CHARACTERS) == token:\n",
        "                        answer_index += 1\n",
        "                        span_end += 1\n",
        "                    elif token in IGNORED_TOKENS:\n",
        "                        span_end += 1\n",
        "                    else:\n",
        "                        break\n",
        "                if num_answer_tokens == answer_index:\n",
        "                    spans.append((span_start, span_end))\n",
        "        return spans\n",
        "\n",
        "    @staticmethod\n",
        "    def find_valid_add_sub_expressions(\n",
        "        numbers: List[int], targets: List[int], max_number_of_numbers_to_consider: int = 2\n",
        "    ) -> List[List[int]]:\n",
        "        valid_signs_for_add_sub_expressions = []\n",
        "        for number_of_numbers_to_consider in range(2, max_number_of_numbers_to_consider + 1):\n",
        "            possible_signs = list(itertools.product((-1, 1), repeat=number_of_numbers_to_consider))\n",
        "            for number_combination in itertools.combinations(\n",
        "                enumerate(numbers), number_of_numbers_to_consider\n",
        "            ):\n",
        "                indices = [it[0] for it in number_combination]\n",
        "                values = [it[1] for it in number_combination]\n",
        "                for signs in possible_signs:\n",
        "                    eval_value = sum(sign * value for sign, value in zip(signs, values))\n",
        "                    if eval_value in targets:\n",
        "                        labels_for_numbers = [0] * len(numbers)  \n",
        "                        for index, sign in zip(indices, signs):\n",
        "                            labels_for_numbers[index] = (\n",
        "                                1 if sign == 1 else 2\n",
        "                            )  # 1 for positive, 2 for negative\n",
        "                        valid_signs_for_add_sub_expressions.append(labels_for_numbers)\n",
        "        return valid_signs_for_add_sub_expressions\n",
        "\n",
        "    @staticmethod\n",
        "    def find_valid_counts(count_numbers: List[int], targets: List[int]) -> List[int]:\n",
        "        valid_indices = []\n",
        "        for index, number in enumerate(count_numbers):\n",
        "            if number in targets:\n",
        "                valid_indices.append(index)\n",
        "        return valid_indices\n",
        "\n",
        "from allennlp.nn.util import dist_reduce_sum\n",
        "from allennlp_models.rc.tools.squad import metric_max_over_ground_truths\n",
        "from allennlp_models.rc.tools.drop import (\n",
        "    get_metrics as drop_em_and_f1\n",
        ")\n",
        "\n",
        "def custom_answer_json_to_strings(answer: Dict[str, Any]) -> Tuple[Tuple[str, ...], str]:\n",
        "    \"\"\"\n",
        "    Takes an answer JSON blob from the DROP data release and converts it into strings used for\n",
        "    evaluation.\n",
        "    \"\"\"\n",
        "    if \"number\" in answer and answer[\"number\"]:\n",
        "        return tuple([str(answer[\"number\"])]), \"number\"\n",
        "    elif \"spans\" in answer and answer[\"spans\"]:\n",
        "        return tuple(answer[\"spans\"]), \"span\" if len(answer[\"spans\"]) == 1 else \"spans\"\n",
        "    elif \"invalid\" in answer and answer[\"invalid\"]:\n",
        "        return tuple([str(answer[\"invalid\"])]), 'invalid'\n",
        "    elif \"date\" in answer:\n",
        "        return (\n",
        "            tuple(\n",
        "                [\n",
        "                    \"{0} {1} {2}\".format(\n",
        "                        answer[\"date\"][\"day\"], answer[\"date\"][\"month\"], answer[\"date\"][\"year\"]\n",
        "                    )\n",
        "                ]\n",
        "            ),\n",
        "            \"date\",\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Answer type not found, should be one of number, spans or date at: {json.dumps(answer)}\"\n",
        "        )\n",
        "        \n",
        "class CustomDropEmAndF1(DropEmAndF1):\n",
        "\n",
        "\n",
        "  def __call__(self, prediction: Union[str, List], ground_truths: List):  # type: ignore\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        prediction: ``Union[str, List]``\n",
        "            The predicted answer from the model evaluated. This could be a string, or a list of string\n",
        "            when multiple spans are predicted as answer.\n",
        "        ground_truths: ``List``\n",
        "            All the ground truth answer annotations.\n",
        "        \"\"\"\n",
        "        # If you wanted to split this out by answer type, you could look at [1] here and group by\n",
        "        # that, instead of only keeping [0].\n",
        "        ground_truth_answer_strings = [\n",
        "            custom_answer_json_to_strings(annotation)[0] for annotation in ground_truths\n",
        "        ]\n",
        "        exact_match, f1_score = metric_max_over_ground_truths(\n",
        "            drop_em_and_f1, prediction, ground_truth_answer_strings\n",
        "        )\n",
        "\n",
        "        # Converting to int here, since we want to count the number of exact matches.\n",
        "        self._total_em += dist_reduce_sum(int(exact_match))\n",
        "        self._total_f1 += dist_reduce_sum(f1_score)\n",
        "        self._count += dist_reduce_sum(1)\n",
        "\n",
        "class FINumericallyAugmentedQaNet(Model):\n",
        "    \"\"\"\n",
        "    This class augments the QANet model with some rudimentary numerical reasoning abilities, as\n",
        "    published in the original DROP paper.\n",
        "    The main idea here is that instead of just predicting a passage span after doing all of the\n",
        "    QANet modeling stuff, we add several different \"answer abilities\": predicting a span from the\n",
        "    question, predicting a count, or predicting an arithmetic expression, in addition to predicting\n",
        "    whether the input contains invalid sequences.  Near the end of the QANet model, we have a variable \n",
        "    that predicts what kind of answer type we need, and each branch has separate modeling logic to \n",
        "    predict that answer type.  We then marginalize over all possible ways of getting to the right \n",
        "    answer through each of these answer types.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Vocabulary,\n",
        "        text_field_embedder: TextFieldEmbedder,\n",
        "        num_highway_layers: int,\n",
        "        phrase_layer: Seq2SeqEncoder,\n",
        "        matrix_attention_layer: MatrixAttention,\n",
        "        modeling_layer: Seq2SeqEncoder,\n",
        "        dropout_prob: float = 0.1,\n",
        "        initializer: InitializerApplicator = InitializerApplicator(),\n",
        "        regularizer: Optional[RegularizerApplicator] = None,\n",
        "        answering_abilities: List[str] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(vocab, regularizer)\n",
        "\n",
        "        if answering_abilities is None:\n",
        "            self.answering_abilities = [\n",
        "                \"passage_span_extraction\",\n",
        "                \"question_span_extraction\",\n",
        "                \"addition_subtraction\",\n",
        "                \"counting\",\n",
        "                \"invalid\",\n",
        "            ]\n",
        "        else:\n",
        "            self.answering_abilities = answering_abilities\n",
        "\n",
        "        text_embed_dim = text_field_embedder.get_output_dim()\n",
        "        encoding_in_dim = phrase_layer.get_input_dim()\n",
        "        encoding_out_dim = phrase_layer.get_output_dim()\n",
        "        modeling_in_dim = modeling_layer.get_input_dim()\n",
        "        modeling_out_dim = modeling_layer.get_output_dim()\n",
        "\n",
        "        self._text_field_embedder = text_field_embedder\n",
        "\n",
        "        self._embedding_proj_layer = torch.nn.Linear(text_embed_dim, encoding_in_dim)\n",
        "        self._highway_layer = Highway(encoding_in_dim, num_highway_layers)\n",
        "\n",
        "        self._encoding_proj_layer = torch.nn.Linear(encoding_in_dim, encoding_in_dim, bias=True)\n",
        "        self._phrase_layer = phrase_layer\n",
        "\n",
        "        self._matrix_attention = matrix_attention_layer\n",
        "\n",
        "        self._modeling_proj_layer = torch.nn.Linear(\n",
        "            encoding_out_dim * 4, modeling_in_dim, bias=True\n",
        "        )\n",
        "        self._modeling_layer = modeling_layer\n",
        "\n",
        "        self._passage_weights_predictor = torch.nn.Linear(modeling_out_dim, 1)\n",
        "        self._question_weights_predictor = torch.nn.Linear(encoding_out_dim, 1)\n",
        "\n",
        "        if len(self.answering_abilities) > 1:\n",
        "            self._answer_ability_predictor = FeedForward(\n",
        "                modeling_out_dim + encoding_out_dim,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, len(self.answering_abilities)],\n",
        "                num_layers=2,\n",
        "                dropout=dropout_prob,\n",
        "            )\n",
        "\n",
        "        if \"passage_span_extraction\" in self.answering_abilities:\n",
        "            self._passage_span_extraction_index = self.answering_abilities.index(\n",
        "                \"passage_span_extraction\"\n",
        "            )\n",
        "            self._passage_span_start_predictor = FeedForward(\n",
        "                modeling_out_dim * 2,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 1],\n",
        "                num_layers=2,\n",
        "            )\n",
        "            self._passage_span_end_predictor = FeedForward(\n",
        "                modeling_out_dim * 2,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 1],\n",
        "                num_layers=2,\n",
        "            )\n",
        "\n",
        "        if \"question_span_extraction\" in self.answering_abilities:\n",
        "            self._question_span_extraction_index = self.answering_abilities.index(\n",
        "                \"question_span_extraction\"\n",
        "            )\n",
        "            self._question_span_start_predictor = FeedForward(\n",
        "                modeling_out_dim * 2,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 1],\n",
        "                num_layers=2,\n",
        "            )\n",
        "            self._question_span_end_predictor = FeedForward(\n",
        "                modeling_out_dim * 2,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 1],\n",
        "                num_layers=2,\n",
        "            )\n",
        "\n",
        "        if \"addition_subtraction\" in self.answering_abilities:\n",
        "            self._addition_subtraction_index = self.answering_abilities.index(\n",
        "                \"addition_subtraction\"\n",
        "            )\n",
        "            self._number_sign_predictor = FeedForward(\n",
        "                modeling_out_dim * 3,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 3],\n",
        "                num_layers=2,\n",
        "            )\n",
        "\n",
        "        if \"counting\" in self.answering_abilities:\n",
        "            self._counting_index = self.answering_abilities.index(\"counting\")\n",
        "            self._count_number_predictor = FeedForward(\n",
        "                modeling_out_dim,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 10],\n",
        "                num_layers=2,\n",
        "            )\n",
        "\n",
        "        if \"invalid\" in self.answering_abilities:\n",
        "            self._invalid_index = self.answering_abilities.index(\"invalid\")\n",
        "            \n",
        "            self._invalid_predictor = FeedForward(\n",
        "                modeling_out_dim + encoding_out_dim, \n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 2],\n",
        "                num_layers=2,\n",
        "            )\n",
        "\n",
        "        self._drop_metrics = CustomDropEmAndF1()\n",
        "        self._dropout = torch.nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        initializer(self)\n",
        "\n",
        "\n",
        "    def forward( \n",
        "        self,\n",
        "        question: Dict[str, torch.LongTensor],\n",
        "        passage: Dict[str, torch.LongTensor],\n",
        "        number_indices: torch.LongTensor,\n",
        "        answer_as_passage_spans: torch.LongTensor = None,\n",
        "        answer_as_question_spans: torch.LongTensor = None,\n",
        "        answer_as_add_sub_expressions: torch.LongTensor = None,\n",
        "        answer_as_counts: torch.LongTensor = None,\n",
        "        answer_invalid: torch.LongTensor = None, \n",
        "        metadata: List[Dict[str, Any]] = None,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        question_mask = util.get_text_field_mask(question)\n",
        "        passage_mask = util.get_text_field_mask(passage)\n",
        "        embedded_question = self._dropout(self._text_field_embedder(question))\n",
        "        embedded_passage = self._dropout(self._text_field_embedder(passage))\n",
        "        embedded_question = self._highway_layer(self._embedding_proj_layer(embedded_question))\n",
        "        embedded_passage = self._highway_layer(self._embedding_proj_layer(embedded_passage))\n",
        "\n",
        "        batch_size = embedded_question.size(0)\n",
        "\n",
        "        projected_embedded_question = self._encoding_proj_layer(embedded_question)\n",
        "        projected_embedded_passage = self._encoding_proj_layer(embedded_passage)\n",
        "\n",
        "        encoded_question = self._dropout(\n",
        "            self._phrase_layer(projected_embedded_question, question_mask)\n",
        "        )\n",
        "        encoded_passage = self._dropout(\n",
        "            self._phrase_layer(projected_embedded_passage, passage_mask)\n",
        "        )\n",
        "\n",
        "        # Shape: (batch_size, passage_length, question_length)\n",
        "        passage_question_similarity = self._matrix_attention(encoded_passage, encoded_question)\n",
        "        # Shape: (batch_size, passage_length, question_length)\n",
        "        passage_question_attention = masked_softmax(\n",
        "            passage_question_similarity, question_mask, memory_efficient=True\n",
        "        )\n",
        "        # Shape: (batch_size, passage_length, encoding_dim)\n",
        "        passage_question_vectors = util.weighted_sum(encoded_question, passage_question_attention)\n",
        "\n",
        "        # Shape: (batch_size, question_length, passage_length)\n",
        "        question_passage_attention = masked_softmax(\n",
        "            passage_question_similarity.transpose(1, 2), passage_mask, memory_efficient=True\n",
        "        )\n",
        "\n",
        "        # Shape: (batch_size, passage_length, passage_length)\n",
        "        passsage_attention_over_attention = torch.bmm(\n",
        "            passage_question_attention, question_passage_attention\n",
        "        )\n",
        "        # Shape: (batch_size, passage_length, encoding_dim)\n",
        "        passage_passage_vectors = util.weighted_sum(\n",
        "            encoded_passage, passsage_attention_over_attention\n",
        "        )\n",
        "\n",
        "        # Shape: (batch_size, passage_length, encoding_dim * 4)\n",
        "        merged_passage_attention_vectors = self._dropout(\n",
        "            torch.cat(\n",
        "                [\n",
        "                    encoded_passage,\n",
        "                    passage_question_vectors,\n",
        "                    encoded_passage * passage_question_vectors,\n",
        "                    encoded_passage * passage_passage_vectors,\n",
        "                ],\n",
        "                dim=-1,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # The recurrent modeling layers. Since these layers share the same parameters,\n",
        "        # we don't construct them conditioned on answering abilities.\n",
        "        modeled_passage_list = [self._modeling_proj_layer(merged_passage_attention_vectors)]\n",
        "        for _ in range(4):\n",
        "            modeled_passage = self._dropout(\n",
        "                self._modeling_layer(modeled_passage_list[-1], passage_mask)\n",
        "            )\n",
        "            modeled_passage_list.append(modeled_passage)\n",
        "        # Pop the first one, which is input\n",
        "        modeled_passage_list.pop(0)\n",
        "\n",
        "        # The first modeling layer is used to calculate the vector representation of passage\n",
        "        passage_weights = self._passage_weights_predictor(modeled_passage_list[0]).squeeze(-1)\n",
        "        passage_weights = masked_softmax(passage_weights, passage_mask)\n",
        "        passage_vector = util.weighted_sum(modeled_passage_list[0], passage_weights)\n",
        "        # The vector representation of question is calculated based on the unmatched encoding,\n",
        "        # because we may want to infer the answer ability only based on the question words.\n",
        "        question_weights = self._question_weights_predictor(encoded_question).squeeze(-1)\n",
        "        question_weights = masked_softmax(question_weights, question_mask)\n",
        "        question_vector = util.weighted_sum(encoded_question, question_weights)\n",
        "\n",
        "        if len(self.answering_abilities) > 1:\n",
        "            # Shape: (batch_size, number_of_abilities)\n",
        "            answer_ability_logits = self._answer_ability_predictor(\n",
        "                torch.cat([passage_vector, question_vector], -1)\n",
        "            )\n",
        "            answer_ability_log_probs = torch.nn.functional.log_softmax(answer_ability_logits, -1)\n",
        "            best_answer_ability = torch.argmax(answer_ability_log_probs, 1)\n",
        "\n",
        "        if \"invalid\" in self.answering_abilities:\n",
        "            # Shape: (batch_size, number_of_abilities)\n",
        "            invalid_logits = self._invalid_predictor(\n",
        "                torch.cat([passage_vector, question_vector], -1)\n",
        "            )\n",
        "            invalid_log_probs = torch.nn.functional.log_softmax(invalid_logits, -1)\n",
        "\n",
        "            best_is_invalid_pred = torch.argmax(invalid_log_probs, -1)\n",
        "            best_invalid_log_prob = torch.gather(\n",
        "                invalid_log_probs, 1, best_is_invalid_pred.unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                best_invalid_log_prob += answer_ability_log_probs[:, self._invalid_index]\n",
        "\n",
        "\n",
        "        if \"counting\" in self.answering_abilities:\n",
        "            # Shape: (batch_size, 10)\n",
        "            count_number_logits = self._count_number_predictor(passage_vector)\n",
        "            count_number_log_probs = torch.nn.functional.log_softmax(count_number_logits, -1)\n",
        "            # Info about the best count number prediction\n",
        "            # Shape: (batch_size,)\n",
        "            best_count_number = torch.argmax(count_number_log_probs, -1)\n",
        "            best_count_log_prob = torch.gather(\n",
        "                count_number_log_probs, 1, best_count_number.unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                best_count_log_prob += answer_ability_log_probs[:, self._counting_index]\n",
        "\n",
        "        if \"passage_span_extraction\" in self.answering_abilities:\n",
        "            # Shape: (batch_size, passage_length, modeling_dim * 2))\n",
        "            passage_for_span_start = torch.cat(\n",
        "                [modeled_passage_list[0], modeled_passage_list[1]], dim=-1\n",
        "            )\n",
        "            # Shape: (batch_size, passage_length)\n",
        "            passage_span_start_logits = self._passage_span_start_predictor(\n",
        "                passage_for_span_start\n",
        "            ).squeeze(-1)\n",
        "            # Shape: (batch_size, passage_length, modeling_dim * 2)\n",
        "            passage_for_span_end = torch.cat(\n",
        "                [modeled_passage_list[0], modeled_passage_list[2]], dim=-1\n",
        "            )\n",
        "            # Shape: (batch_size, passage_length)\n",
        "            passage_span_end_logits = self._passage_span_end_predictor(\n",
        "                passage_for_span_end\n",
        "            ).squeeze(-1)\n",
        "            # Shape: (batch_size, passage_length)\n",
        "            passage_span_start_log_probs = util.masked_log_softmax(\n",
        "                passage_span_start_logits, passage_mask\n",
        "            )\n",
        "            passage_span_end_log_probs = util.masked_log_softmax(\n",
        "                passage_span_end_logits, passage_mask\n",
        "            )\n",
        "\n",
        "            # Info about the best passage span prediction\n",
        "            passage_span_start_logits = replace_masked_values_with_big_negative_number(\n",
        "                passage_span_start_logits, passage_mask\n",
        "            )\n",
        "            passage_span_end_logits = replace_masked_values_with_big_negative_number(\n",
        "                passage_span_end_logits, passage_mask\n",
        "            )\n",
        "            # Shape: (batch_size, 2)\n",
        "            best_passage_span = get_best_span(passage_span_start_logits, passage_span_end_logits)\n",
        "            # Shape: (batch_size, 2)\n",
        "            best_passage_start_log_probs = torch.gather(\n",
        "                passage_span_start_log_probs, 1, best_passage_span[:, 0].unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            best_passage_end_log_probs = torch.gather(\n",
        "                passage_span_end_log_probs, 1, best_passage_span[:, 1].unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            # Shape: (batch_size,)\n",
        "            best_passage_span_log_prob = best_passage_start_log_probs + best_passage_end_log_probs\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                best_passage_span_log_prob += answer_ability_log_probs[\n",
        "                    :, self._passage_span_extraction_index\n",
        "                ]\n",
        "\n",
        "        if \"question_span_extraction\" in self.answering_abilities:\n",
        "            # Shape: (batch_size, question_length)\n",
        "            encoded_question_for_span_prediction = torch.cat(\n",
        "                [\n",
        "                    encoded_question,\n",
        "                    passage_vector.unsqueeze(1).repeat(1, encoded_question.size(1), 1),\n",
        "                ],\n",
        "                -1,\n",
        "            )\n",
        "            question_span_start_logits = self._question_span_start_predictor(\n",
        "                encoded_question_for_span_prediction\n",
        "            ).squeeze(-1)\n",
        "            # Shape: (batch_size, question_length)\n",
        "            question_span_end_logits = self._question_span_end_predictor(\n",
        "                encoded_question_for_span_prediction\n",
        "            ).squeeze(-1)\n",
        "            question_span_start_log_probs = util.masked_log_softmax(\n",
        "                question_span_start_logits, question_mask\n",
        "            )\n",
        "            question_span_end_log_probs = util.masked_log_softmax(\n",
        "                question_span_end_logits, question_mask\n",
        "            )\n",
        "\n",
        "            # Info about the best question span prediction\n",
        "            question_span_start_logits = replace_masked_values_with_big_negative_number(\n",
        "                question_span_start_logits, question_mask\n",
        "            )\n",
        "            question_span_end_logits = replace_masked_values_with_big_negative_number(\n",
        "                question_span_end_logits, question_mask\n",
        "            )\n",
        "            # Shape: (batch_size, 2)\n",
        "            best_question_span = get_best_span(question_span_start_logits, question_span_end_logits)\n",
        "            # Shape: (batch_size, 2)\n",
        "            best_question_start_log_probs = torch.gather(\n",
        "                question_span_start_log_probs, 1, best_question_span[:, 0].unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            best_question_end_log_probs = torch.gather(\n",
        "                question_span_end_log_probs, 1, best_question_span[:, 1].unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            # Shape: (batch_size,)\n",
        "            best_question_span_log_prob = (\n",
        "                best_question_start_log_probs + best_question_end_log_probs\n",
        "            )\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                best_question_span_log_prob += answer_ability_log_probs[\n",
        "                    :, self._question_span_extraction_index\n",
        "                ]\n",
        "\n",
        "        if \"addition_subtraction\" in self.answering_abilities:\n",
        "            # Shape: (batch_size, # of numbers in the passage)\n",
        "            number_indices = number_indices.squeeze(-1)\n",
        "            number_mask = number_indices != -1\n",
        "            clamped_number_indices = util.replace_masked_values(number_indices, number_mask, 0)\n",
        "            encoded_passage_for_numbers = torch.cat(\n",
        "                [modeled_passage_list[0], modeled_passage_list[3]], dim=-1\n",
        "            )\n",
        "            # Shape: (batch_size, # of numbers in the passage, encoding_dim)\n",
        "            encoded_numbers = torch.gather(\n",
        "                encoded_passage_for_numbers,\n",
        "                1,\n",
        "                clamped_number_indices.unsqueeze(-1).expand(\n",
        "                    -1, -1, encoded_passage_for_numbers.size(-1)\n",
        "                ),\n",
        "            )\n",
        "            # Shape: (batch_size, # of numbers in the passage)\n",
        "            encoded_numbers = torch.cat(\n",
        "                [\n",
        "                    encoded_numbers,\n",
        "                    passage_vector.unsqueeze(1).repeat(1, encoded_numbers.size(1), 1),\n",
        "                ],\n",
        "                -1,\n",
        "            )\n",
        "\n",
        "            # Shape: (batch_size, # of numbers in the passage, 3)\n",
        "            number_sign_logits = self._number_sign_predictor(encoded_numbers)\n",
        "            number_sign_log_probs = torch.nn.functional.log_softmax(number_sign_logits, -1)\n",
        "\n",
        "            # Shape: (batch_size, # of numbers in passage).\n",
        "            best_signs_for_numbers = torch.argmax(number_sign_log_probs, -1)\n",
        "            # For padding numbers, the best sign masked as 0 (not included).\n",
        "            best_signs_for_numbers = util.replace_masked_values(\n",
        "                best_signs_for_numbers, number_mask, 0\n",
        "            )\n",
        "            # Shape: (batch_size, # of numbers in passage)\n",
        "            best_signs_log_probs = torch.gather(\n",
        "                number_sign_log_probs, 2, best_signs_for_numbers.unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            # the probs of the masked positions should be 1 so that it will not affect the joint probability\n",
        "            # TODO: this is not quite right, since if there are many numbers in the passage,\n",
        "            # TODO: the joint probability would be very small.\n",
        "            best_signs_log_probs = util.replace_masked_values(best_signs_log_probs, number_mask, 0)\n",
        "            # Shape: (batch_size,)\n",
        "            best_combination_log_prob = best_signs_log_probs.sum(-1)\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                best_combination_log_prob += answer_ability_log_probs[\n",
        "                    :, self._addition_subtraction_index\n",
        "                ]\n",
        "\n",
        "        output_dict = {}\n",
        "\n",
        "        # If answer is given, compute the loss.\n",
        "        if (\n",
        "            answer_as_passage_spans is not None\n",
        "            or answer_as_question_spans is not None\n",
        "            or answer_as_add_sub_expressions is not None\n",
        "            or answer_as_counts is not None\n",
        "            or answer_invalid is not None\n",
        "        ):\n",
        "\n",
        "            log_marginal_likelihood_list = []\n",
        "\n",
        "            for answering_ability in self.answering_abilities:\n",
        "                if answering_ability == \"passage_span_extraction\":\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    gold_passage_span_starts = answer_as_passage_spans[:, :, 0]\n",
        "                    gold_passage_span_ends = answer_as_passage_spans[:, :, 1]\n",
        "                    # Some spans are padded with index -1,\n",
        "                    # so we clamp those paddings to 0 and then mask after `torch.gather()`.\n",
        "                    gold_passage_span_mask = gold_passage_span_starts != -1\n",
        "                    clamped_gold_passage_span_starts = util.replace_masked_values(\n",
        "                        gold_passage_span_starts, gold_passage_span_mask, 0\n",
        "                    )\n",
        "                    clamped_gold_passage_span_ends = util.replace_masked_values(\n",
        "                        gold_passage_span_ends, gold_passage_span_mask, 0\n",
        "                    )\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    log_likelihood_for_passage_span_starts = torch.gather(\n",
        "                        passage_span_start_log_probs, 1, clamped_gold_passage_span_starts\n",
        "                    )\n",
        "                    log_likelihood_for_passage_span_ends = torch.gather(\n",
        "                        passage_span_end_log_probs, 1, clamped_gold_passage_span_ends\n",
        "                    )\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    log_likelihood_for_passage_spans = (\n",
        "                        log_likelihood_for_passage_span_starts\n",
        "                        + log_likelihood_for_passage_span_ends\n",
        "                    )\n",
        "                    # For those padded spans, we set their log probabilities to be very small negative value\n",
        "                    log_likelihood_for_passage_spans = (\n",
        "                        replace_masked_values_with_big_negative_number(\n",
        "                            log_likelihood_for_passage_spans,\n",
        "                            gold_passage_span_mask,\n",
        "                        )\n",
        "                    )\n",
        "                    # Shape: (batch_size, )\n",
        "                    log_marginal_likelihood_for_passage_span = util.logsumexp(\n",
        "                        log_likelihood_for_passage_spans\n",
        "                    )\n",
        "                    log_marginal_likelihood_list.append(log_marginal_likelihood_for_passage_span)\n",
        "\n",
        "                elif answering_ability == \"question_span_extraction\":\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    gold_question_span_starts = answer_as_question_spans[:, :, 0]\n",
        "                    gold_question_span_ends = answer_as_question_spans[:, :, 1]\n",
        "                    # Some spans are padded with index -1,\n",
        "                    # so we clamp those paddings to 0 and then mask after `torch.gather()`.\n",
        "                    gold_question_span_mask = gold_question_span_starts != -1\n",
        "                    clamped_gold_question_span_starts = util.replace_masked_values(\n",
        "                        gold_question_span_starts, gold_question_span_mask, 0\n",
        "                    )\n",
        "                    clamped_gold_question_span_ends = util.replace_masked_values(\n",
        "                        gold_question_span_ends, gold_question_span_mask, 0\n",
        "                    )\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    log_likelihood_for_question_span_starts = torch.gather(\n",
        "                        question_span_start_log_probs, 1, clamped_gold_question_span_starts\n",
        "                    )\n",
        "                    log_likelihood_for_question_span_ends = torch.gather(\n",
        "                        question_span_end_log_probs, 1, clamped_gold_question_span_ends\n",
        "                    )\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    log_likelihood_for_question_spans = (\n",
        "                        log_likelihood_for_question_span_starts\n",
        "                        + log_likelihood_for_question_span_ends\n",
        "                    )\n",
        "                    # For those padded spans, we set their log probabilities to be very small negative value\n",
        "                    log_likelihood_for_question_spans = (\n",
        "                        replace_masked_values_with_big_negative_number(\n",
        "                            log_likelihood_for_question_spans,\n",
        "                            gold_question_span_mask,\n",
        "                        )\n",
        "                    )\n",
        "                    # Shape: (batch_size, )\n",
        "\n",
        "                    log_marginal_likelihood_for_question_span = util.logsumexp(\n",
        "                        log_likelihood_for_question_spans\n",
        "                    )\n",
        "                    log_marginal_likelihood_list.append(log_marginal_likelihood_for_question_span)\n",
        "\n",
        "                elif answering_ability == \"addition_subtraction\":\n",
        "                    # The padded add-sub combinations use 0 as the signs for all numbers, and we mask them here.\n",
        "                    # Shape: (batch_size, # of combinations)\n",
        "                    gold_add_sub_mask = answer_as_add_sub_expressions.sum(-1) > 0\n",
        "                    # Shape: (batch_size, # of numbers in the passage, # of combinations)\n",
        "                    gold_add_sub_signs = answer_as_add_sub_expressions.transpose(1, 2)\n",
        "                    # Shape: (batch_size, # of numbers in the passage, # of combinations)\n",
        "                    log_likelihood_for_number_signs = torch.gather(\n",
        "                        number_sign_log_probs, 2, gold_add_sub_signs\n",
        "                    )\n",
        "                    # the log likelihood of the masked positions should be 0\n",
        "                    # so that it will not affect the joint probability\n",
        "                    log_likelihood_for_number_signs = util.replace_masked_values(\n",
        "                        log_likelihood_for_number_signs, number_mask.unsqueeze(-1), 0\n",
        "                    )\n",
        "                    # Shape: (batch_size, # of combinations)\n",
        "                    log_likelihood_for_add_subs = log_likelihood_for_number_signs.sum(1)\n",
        "                    # For those padded combinations, we set their log probabilities to be very small negative value\n",
        "                    log_likelihood_for_add_subs = replace_masked_values_with_big_negative_number(\n",
        "                        log_likelihood_for_add_subs, gold_add_sub_mask\n",
        "                    )\n",
        "                    # Shape: (batch_size, )\n",
        "                    log_marginal_likelihood_for_add_sub = util.logsumexp(\n",
        "                        log_likelihood_for_add_subs\n",
        "                    )\n",
        "                    log_marginal_likelihood_list.append(log_marginal_likelihood_for_add_sub)\n",
        "\n",
        "                elif answering_ability == \"counting\":\n",
        "                    # Count answers are padded with label -1,\n",
        "                    # so we clamp those paddings to 0 and then mask after `torch.gather()`.\n",
        "                    # Shape: (batch_size, # of count answers)\n",
        "                    gold_count_mask = answer_as_counts != -1\n",
        "                    # Shape: (batch_size, # of count answers)\n",
        "                    clamped_gold_counts = util.replace_masked_values(\n",
        "                        answer_as_counts, gold_count_mask, 0\n",
        "                    )\n",
        "                    log_likelihood_for_counts = torch.gather(\n",
        "                        count_number_log_probs, 1, clamped_gold_counts\n",
        "                    )\n",
        "                    # For those padded spans, we set their log probabilities to be very small negative value\n",
        "                    log_likelihood_for_counts = replace_masked_values_with_big_negative_number(\n",
        "                        log_likelihood_for_counts, gold_count_mask\n",
        "                    )\n",
        "                    # Shape: (batch_size, )\n",
        "                    log_marginal_likelihood_for_count = util.logsumexp(log_likelihood_for_counts)\n",
        "                    log_marginal_likelihood_list.append(log_marginal_likelihood_for_count)\n",
        "                    \n",
        "                elif answering_ability == \"invalid\":\n",
        "                    gold_invalid_mask = answer_invalid != -1\n",
        "                    # Shape: (batch_size, # of count answers)\n",
        "                    clamped_gold_invalid = util.replace_masked_values(\n",
        "                        answer_invalid, gold_invalid_mask, 0\n",
        "                    ) \n",
        "                    log_likelihood_for_invalid = torch.gather(\n",
        "                        invalid_log_probs, 1, clamped_gold_invalid\n",
        "                    )\n",
        "                    # For those padded spans, we set their log probabilities to be very small negative value\n",
        "                    log_likelihood_for_invalid = replace_masked_values_with_big_negative_number(\n",
        "                        log_likelihood_for_invalid, gold_invalid_mask\n",
        "                    )\n",
        "                    # Shape: (batch_size, )\n",
        "                    log_marginal_likelihood_for_invalid = util.logsumexp(log_likelihood_for_invalid)\n",
        "                    log_marginal_likelihood_list.append(log_marginal_likelihood_for_invalid)\n",
        "\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported answering ability: {answering_ability}\")\n",
        "\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                # Add the ability probabilities if there are more than one abilities\n",
        "                all_log_marginal_likelihoods = torch.stack(log_marginal_likelihood_list, dim=-1)\n",
        "                all_log_marginal_likelihoods = (\n",
        "                    all_log_marginal_likelihoods + answer_ability_log_probs\n",
        "                )\n",
        "                marginal_log_likelihood = util.logsumexp(all_log_marginal_likelihoods)\n",
        "            else:\n",
        "                marginal_log_likelihood = log_marginal_likelihood_list[0]\n",
        "\n",
        "            output_dict[\"loss\"] = -marginal_log_likelihood.mean()\n",
        "\n",
        "        # Compute the metrics and add the tokenized input to the output.\n",
        "        if metadata is not None:\n",
        "            output_dict[\"question_id\"] = []\n",
        "            output_dict[\"answer\"] = []\n",
        "            question_tokens = []\n",
        "            passage_tokens = []\n",
        "            for i in range(batch_size):\n",
        "                question_tokens.append(metadata[i][\"question_tokens\"])\n",
        "                passage_tokens.append(metadata[i][\"passage_tokens\"])\n",
        "\n",
        "                if len(self.answering_abilities) > 1:\n",
        "                    predicted_ability_str = self.answering_abilities[\n",
        "                        best_answer_ability[i].detach().cpu().numpy()\n",
        "                    ]\n",
        "                else:\n",
        "                    predicted_ability_str = self.answering_abilities[0]\n",
        "\n",
        "                answer_json: Dict[str, Any] = {}\n",
        "\n",
        "                # We did not consider multi-mention answers here\n",
        "                if predicted_ability_str == \"passage_span_extraction\":\n",
        "                    answer_json[\"answer_type\"] = \"passage_span\"\n",
        "                    passage_str = metadata[i][\"original_passage\"]\n",
        "                    offsets = metadata[i][\"passage_token_offsets\"]\n",
        "                    predicted_span = tuple(best_passage_span[i].detach().cpu().numpy())\n",
        "                    start_offset = offsets[predicted_span[0]][0]\n",
        "                    end_offset = offsets[predicted_span[1]][1]\n",
        "                    predicted_answer = passage_str[start_offset:end_offset]\n",
        "                    answer_json[\"value\"] = predicted_answer\n",
        "                    answer_json[\"spans\"] = [(start_offset, end_offset)]\n",
        "                elif predicted_ability_str == \"question_span_extraction\":\n",
        "                    answer_json[\"answer_type\"] = \"question_span\"\n",
        "                    question_str = metadata[i][\"original_question\"]\n",
        "                    offsets = metadata[i][\"question_token_offsets\"]\n",
        "                    predicted_span = tuple(best_question_span[i].detach().cpu().numpy())\n",
        "                    start_offset = offsets[predicted_span[0]][0]\n",
        "                    end_offset = offsets[predicted_span[1]][1]\n",
        "                    predicted_answer = question_str[start_offset:end_offset]\n",
        "                    answer_json[\"value\"] = predicted_answer\n",
        "                    answer_json[\"spans\"] = [(start_offset, end_offset)]\n",
        "                elif (\n",
        "                    predicted_ability_str == \"addition_subtraction\"\n",
        "                ):  # plus_minus combination answer\n",
        "                    answer_json[\"answer_type\"] = \"arithmetic\"\n",
        "                    original_numbers = metadata[i][\"original_numbers\"]\n",
        "                    sign_remap = {0: 0, 1: 1, 2: -1}\n",
        "                    predicted_signs = [\n",
        "                        sign_remap[it] for it in best_signs_for_numbers[i].detach().cpu().numpy()\n",
        "                    ]\n",
        "                    result = sum(\n",
        "                        [sign * number for sign, number in zip(predicted_signs, original_numbers)]\n",
        "                    )\n",
        "                    predicted_answer = str(result)\n",
        "                    offsets = metadata[i][\"passage_token_offsets\"]\n",
        "                    number_indices = metadata[i][\"number_indices\"]\n",
        "                    number_positions = [offsets[index] for index in number_indices]\n",
        "                    answer_json[\"numbers\"] = []\n",
        "                    for offset, value, sign in zip(\n",
        "                        number_positions, original_numbers, predicted_signs\n",
        "                    ):\n",
        "                        answer_json[\"numbers\"].append(\n",
        "                            {\"span\": offset, \"value\": value, \"sign\": sign}\n",
        "                        )\n",
        "                    if number_indices[-1] == -1:\n",
        "                        # There is a dummy 0 number at position -1 added in some cases; we are\n",
        "                        # removing that here.\n",
        "                        answer_json[\"numbers\"].pop()\n",
        "                    answer_json[\"value\"] = result\n",
        "                elif predicted_ability_str == \"counting\":\n",
        "                    answer_json[\"answer_type\"] = \"count\"\n",
        "                    predicted_count = best_count_number[i].detach().cpu().numpy()\n",
        "                    predicted_answer = str(predicted_count)\n",
        "                    answer_json[\"count\"] = predicted_count\n",
        "                elif predicted_ability_str == \"invalid\":\n",
        "                    answer_json[\"answer_type\"] = \"invalid\"\n",
        "                    predicted_is_invalid = best_is_invalid_pred[i].detach().cpu().numpy()\n",
        "                    predicted_answer = str(predicted_is_invalid)\n",
        "                    answer_json[\"invalid\"] = predicted_is_invalid\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported answer ability: {predicted_ability_str}\")\n",
        "\n",
        "                output_dict[\"question_id\"].append(metadata[i][\"question_id\"])\n",
        "                output_dict[\"answer\"].append(answer_json)\n",
        "                answer_annotations = metadata[i].get(\"answer_annotations\", [])\n",
        "                if answer_annotations:\n",
        "                    self._drop_metrics(predicted_answer, answer_annotations)\n",
        "            # This is used for the demo.\n",
        "            output_dict[\"passage_question_attention\"] = passage_question_attention\n",
        "            output_dict[\"question_tokens\"] = question_tokens\n",
        "            output_dict[\"passage_tokens\"] = passage_tokens\n",
        "        return output_dict\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        exact_match, f1_score = self._drop_metrics.get_metric(reset)\n",
        "        return {\"em\": exact_match, \"f1\": f1_score}\n",
        "\n",
        "    default_predictor = \"reading_comprehension\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Hf5WxLAUJ_"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyVK7MATAQ62"
      },
      "outputs": [],
      "source": [
        "#from training util\n",
        "from allennlp.data import Instance, Vocabulary, Batch, DataLoader\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "\n",
        "#me\n",
        "from allennlp_models.rc.dataset_readers.drop import DropReader\n",
        "from allennlp.data.token_indexers import TokenCharactersIndexer, SingleIdTokenIndexer\n",
        "\n",
        "from typing import Any, Dict, Iterable, Optional, Union, Tuple, Set, List\n",
        "\n",
        "\n",
        "\n",
        "##data reader\n",
        "dataset_reader= DropShuffReaderInvalid(token_indexers= {'tokens':SingleIdTokenIndexer(lowercase_tokens=True), 'token_characters': TokenCharactersIndexer(min_padding_length=5)}, \\\n",
        "                           passage_length_limit= 400, question_length_limit=50, \\\n",
        "                           skip_when_all_empty=[\"passage_span\", \"question_span\", \"addition_subtraction\", \"counting\", 'invalid'], \\\n",
        "                           instance_format=\"drop\")\n",
        "\n",
        "validation_dataset_reader= DropShuffReaderInvalid(token_indexers= {'tokens':SingleIdTokenIndexer(lowercase_tokens=True), 'token_characters': TokenCharactersIndexer(min_padding_length=5)}, \\\n",
        "                           passage_length_limit= 1000, question_length_limit=100, \\\n",
        "                           skip_when_all_empty=[], \\\n",
        "                           instance_format=\"drop\")\n",
        "\n",
        "import random\n",
        "from allennlp.data.samplers.bucket_batch_sampler import BucketBatchSampler\n",
        "from allennlp.data.data_loaders.multiprocess_data_loader import MultiProcessDataLoader\n",
        "\n",
        "dataloaders={}\n",
        "#changed batch size from 16 to 8\n",
        "\n",
        "logger.info(\"Reading original training data from %s\", augmented_train_data_path)\n",
        "dataloaders['train_o']=MultiProcessDataLoader(reader= dataset_reader, \\\n",
        "                                              data_path=augmented_train_data_path, batch_sampler= BucketBatchSampler(batch_size=16), #8\n",
        "                                              cuda_device=torch.cuda.current_device())\n",
        "\n",
        "\n",
        "logger.info(\"Reading original dev data from %s\", augmented_dev_data_path)\n",
        "dataloaders['dev_o']=MultiProcessDataLoader(reader= validation_dataset_reader, \\\n",
        "                                            data_path=augmented_dev_data_path, batch_sampler= BucketBatchSampler(batch_size=16), #8\n",
        "                                            cuda_device=torch.cuda.current_device())\n",
        "\n",
        "\n",
        "##voca\n",
        "vocab_dir = os.path.join(serialization_dir, \"vocabulary\")\n",
        "\n",
        "if os.path.isdir(vocab_dir) and os.listdir(vocab_dir) is not None:\n",
        "  raise ConfigurationError(\n",
        "      \"The 'vocabulary' directory in the provided serialization directory is non-empty\"\n",
        "  )\n",
        "\n",
        "datasets_for_vocab_creation=None\n",
        "\n",
        "instances: Iterable[Instance] = (\n",
        "        instance\n",
        "        for key, data_loader in dataloaders.items()\n",
        "        if datasets_for_vocab_creation is None or key in datasets_for_vocab_creation\n",
        "        for instance in data_loader.iter_instances()\n",
        "    )\n",
        "vocab = Vocabulary.from_instances(min_count= {\"token_characters\": 200}, \\\n",
        "                  pretrained_files= {\"tokens\": \"https://allennlp.s3.amazonaws.com/datasets/glove/glove.840B.300d.lower.converted.zip\"}, \\\n",
        "                  only_include_pretrained_words= True,\n",
        "                  instances=instances)\n",
        "\n",
        "logger.info(f\"writing the vocabulary to {vocab_dir}.\")\n",
        "vocab.save_to_files(vocab_dir)\n",
        "logger.info(\"done creating vocab\")\n",
        "\n",
        "##\n",
        "from allennlp.nn.regularizers.regularizer_applicator import RegularizerApplicator\n",
        "from allennlp.nn.regularizers.regularizers import L2Regularizer\n",
        "from allennlp_models.rc.modules.seq2seq_encoders.qanet_encoder import QaNetEncoder\n",
        "from allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\n",
        "from allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention\n",
        "from allennlp.modules.text_field_embedders.basic_text_field_embedder import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding, TokenCharactersEncoder\n",
        "\n",
        "logger.info(\"indexing dataloaders with vocab\")\n",
        "\n",
        "dataloaders['train_o'].index_with(vocab)\n",
        "dataloaders['dev_o'].index_with(vocab)\n",
        "\n",
        "##model creation\n",
        "model=FINumericallyAugmentedQaNet(\n",
        "        vocab=vocab,\n",
        "        text_field_embedder=  BasicTextFieldEmbedder(\n",
        "        {\"tokens\": Embedding(embedding_dim=300, trainable=False, \\\n",
        "          pretrained_file=\"https://allennlp.s3.amazonaws.com/datasets/glove/glove.840B.300d.lower.converted.zip\",\n",
        "           vocab=vocab), \n",
        "         \"token_characters\": TokenCharactersEncoder(embedding=Embedding(embedding_dim=64, \n",
        "                                                                         vocab=vocab), \n",
        "                        encoder=CnnEncoder(embedding_dim= 64,\n",
        "                        num_filters= 200,\n",
        "                        ngram_filter_sizes=[5]))}),\n",
        "        num_highway_layers=2, \n",
        "        phrase_layer= QaNetEncoder(input_dim=128,\n",
        "            hidden_dim= 128,\n",
        "            attention_projection_dim=128,\n",
        "            feedforward_hidden_dim=128,\n",
        "            num_blocks= 1,\n",
        "            num_convs_per_block= 4,\n",
        "            conv_kernel_size= 7,\n",
        "            num_attention_heads= 8,\n",
        "            dropout_prob= 0.1,\n",
        "            layer_dropout_undecayed_prob= 0.1,\n",
        "            attention_dropout_prob= 0) ,\n",
        "        matrix_attention_layer= LinearMatrixAttention(tensor_1_dim= 128,\n",
        "            tensor_2_dim= 128,\n",
        "            combination= \"x,y,x*y\"),\n",
        "        modeling_layer= QaNetEncoder(input_dim=128,\n",
        "            hidden_dim= 128,\n",
        "            attention_projection_dim=128,\n",
        "            feedforward_hidden_dim=128,\n",
        "            num_blocks= 6,\n",
        "            num_convs_per_block= 2,\n",
        "            conv_kernel_size= 5,\n",
        "            num_attention_heads= 8,\n",
        "            dropout_prob= 0.1,\n",
        "            layer_dropout_undecayed_prob= 0.1,\n",
        "            attention_dropout_prob= 0) ,\n",
        "        dropout_prob = 0.1,\n",
        "        regularizer= RegularizerApplicator(regexes=[(\".*\", L2Regularizer(alpha= 1e-07))]) ,\n",
        "        answering_abilities= [\n",
        "            \"passage_span_extraction\",\n",
        "            \"question_span_extraction\",\n",
        "            \"addition_subtraction\",\n",
        "            \"counting\",\n",
        "            \"invalid\"]\n",
        "    )\n",
        "\n",
        "from allennlp.training.trainer import Trainer, GradientDescentTrainer\n",
        "from allennlp.training.optimizers import AdamOptimizer, SgdOptimizer\n",
        "from allennlp.training.moving_average import ExponentialMovingAverage\n",
        "\n",
        "adam=AdamOptimizer(model_parameters=model.named_parameters(), lr=5e-4, \n",
        "              betas=[0.8, 0.999], eps= 1e-07)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "trainer= GradientDescentTrainer(model=model, optimizer=adam, data_loader= dataloaders['train_o'], \n",
        "                                patience=10, validation_metric=\"+f1\", validation_data_loader=dataloaders['dev_o'], \n",
        "                                num_epochs=50, serialization_dir=serialization_dir, grad_norm=5,\n",
        "                                moving_average=ExponentialMovingAverage(model.named_parameters(), decay=0.9999)\n",
        "                              )\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-mivy-NAe1J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM6INVp2aDKE"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nYqnpemaPpp"
      },
      "outputs": [],
      "source": [
        "path= serialization_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNSQy9vXoulI"
      },
      "outputs": [],
      "source": [
        "from allennlp.data import Instance, Vocabulary, Batch, DataLoader\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "from allennlp_models.rc.dataset_readers.drop import DropReader\n",
        "from allennlp.data.token_indexers import TokenCharactersIndexer, SingleIdTokenIndexer\n",
        "\n",
        "from typing import Any, Dict, Iterable, Optional, Union, Tuple, Set, List\n",
        "import os\n",
        "from allennlp.nn.regularizers.regularizer_applicator import RegularizerApplicator\n",
        "from allennlp.nn.regularizers.regularizers import L2Regularizer\n",
        "from allennlp_models.rc.modules.seq2seq_encoders.qanet_encoder import QaNetEncoder\n",
        "from allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\n",
        "from allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention\n",
        "from allennlp.modules.text_field_embedders.basic_text_field_embedder import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding, TokenCharactersEncoder\n",
        "import random\n",
        "from allennlp.data.samplers.bucket_batch_sampler import BucketBatchSampler\n",
        "from allennlp.data.data_loaders.multiprocess_data_loader import MultiProcessDataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akCKjpEVaFzH"
      },
      "outputs": [],
      "source": [
        "vocab= Vocabulary.from_files(path+'/vocabulary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09ESgt49a2_2"
      },
      "outputs": [],
      "source": [
        "pretrained_dict=torch.load(path+'/best.th')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azmTwWv_aEXz"
      },
      "outputs": [],
      "source": [
        "validation_dataset_reader= DropShuffReaderInvalid(token_indexers= {'tokens':SingleIdTokenIndexer(lowercase_tokens=True), 'token_characters': TokenCharactersIndexer(min_padding_length=5)}, \\\n",
        "                           passage_length_limit= 1000, question_length_limit=100, \\\n",
        "                           skip_when_all_empty=[], \\\n",
        "                           instance_format=\"drop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NajFmdtr-9V"
      },
      "outputs": [],
      "source": [
        "dataset_reader=validation_dataset_reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPj9vM3oaut7"
      },
      "outputs": [],
      "source": [
        "model=FINumericallyAugmentedQaNet(\n",
        "        vocab=vocab,\n",
        "        text_field_embedder=  BasicTextFieldEmbedder(\n",
        "        {\"tokens\": Embedding(embedding_dim=300, trainable=False, \\\n",
        "          pretrained_file=\"https://allennlp.s3.amazonaws.com/datasets/glove/glove.840B.300d.lower.converted.zip\",\n",
        "           vocab=vocab), #num_embeddings=vocab.get_vocab_size(\"tokens\")),\n",
        "         \"token_characters\": TokenCharactersEncoder(embedding=Embedding(embedding_dim=64, \n",
        "                                                                         vocab=vocab), \n",
        "                        encoder=CnnEncoder(embedding_dim= 64,\n",
        "                        num_filters= 200,\n",
        "                        ngram_filter_sizes=[5]))}),\n",
        "        num_highway_layers=2, \n",
        "        phrase_layer= QaNetEncoder(input_dim=128,\n",
        "            hidden_dim= 128,\n",
        "            attention_projection_dim=128,\n",
        "            feedforward_hidden_dim=128,\n",
        "            num_blocks= 1,\n",
        "            num_convs_per_block= 4,\n",
        "            conv_kernel_size= 7,\n",
        "            num_attention_heads= 8,\n",
        "            dropout_prob= 0.1,\n",
        "            layer_dropout_undecayed_prob= 0.1,\n",
        "            attention_dropout_prob= 0) ,\n",
        "        matrix_attention_layer= LinearMatrixAttention(tensor_1_dim= 128,\n",
        "            tensor_2_dim= 128,\n",
        "            combination= \"x,y,x*y\"),\n",
        "        modeling_layer= QaNetEncoder(input_dim=128,\n",
        "            hidden_dim= 128,\n",
        "            attention_projection_dim=128,\n",
        "            feedforward_hidden_dim=128,\n",
        "            num_blocks= 6,\n",
        "            num_convs_per_block= 2,\n",
        "            conv_kernel_size= 5,\n",
        "            num_attention_heads= 8,\n",
        "            dropout_prob= 0.1,\n",
        "            layer_dropout_undecayed_prob= 0.1,\n",
        "            attention_dropout_prob= 0) ,\n",
        "        dropout_prob = 0.1,\n",
        "        regularizer= RegularizerApplicator(regexes=[(\".*\", L2Regularizer(alpha= 1e-07))]) ,\n",
        "        answering_abilities= [\n",
        "            \"passage_span_extraction\",\n",
        "            \"question_span_extraction\",\n",
        "            \"addition_subtraction\",\n",
        "            \"counting\",\n",
        "            \"invalid\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpGjnRaha9QK",
        "outputId": "de412eeb-7c5b-4b0f-ee83-83178666628c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model.load_state_dict(pretrained_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v2ZFrXMa91L",
        "outputId": "f67e81da-f91d-43f4-f08d-c321a13172f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SiameseNumericallyAugmentedQaNet(\n",
              "  (_text_field_embedder): BasicTextFieldEmbedder(\n",
              "    (token_embedder_tokens): Embedding()\n",
              "    (token_embedder_token_characters): TokenCharactersEncoder(\n",
              "      (_embedding): TimeDistributed(\n",
              "        (_module): Embedding()\n",
              "      )\n",
              "      (_encoder): TimeDistributed(\n",
              "        (_module): CnnEncoder(\n",
              "          (_activation): ReLU()\n",
              "          (conv_layer_0): Conv1d(64, 200, kernel_size=(5,), stride=(1,))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (_embedding_proj_layer): Linear(in_features=500, out_features=128, bias=True)\n",
              "  (_highway_layer): Highway(\n",
              "    (_layers): ModuleList(\n",
              "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (_encoding_proj_layer): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (_phrase_layer): QaNetEncoder(\n",
              "    (_encoder_blocks): ModuleList(\n",
              "      (0): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(3, 3), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(3, 3), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (2): Sequential(\n",
              "            (0): ConstantPad1d(padding=(3, 3), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (3): Sequential(\n",
              "            (0): ConstantPad1d(padding=(3, 3), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (_matrix_attention): LinearMatrixAttention(\n",
              "    (_activation): Linear()\n",
              "  )\n",
              "  (_modeling_proj_layer): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (_modeling_layer): QaNetEncoder(\n",
              "    (_encoder_blocks): ModuleList(\n",
              "      (0): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "      (1): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "      (2): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "      (3): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "      (4): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "      (5): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (_passage_weights_predictor): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (_question_weights_predictor): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (_answer_ability_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=5, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.1, inplace=False)\n",
              "      (1): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_passage_span_start_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_passage_span_end_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_question_span_start_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_question_span_end_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_number_sign_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=3, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_count_number_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=10, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_invalid_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=2, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs5QsS44bDlc"
      },
      "outputs": [],
      "source": [
        "from allennlp.training.util import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfP1_iUMbEPI"
      },
      "outputs": [],
      "source": [
        "dataset_reader= validation_dataset_reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paIL9JQ6bHQz"
      },
      "outputs": [],
      "source": [
        "prediction_dir='predictions/'\n",
        "def evaluate_dataset(model, data_path, model_name=None):\n",
        "  logger.info(\"Reading data from %s\", data_path)\n",
        "  file_name=os.path.basename(data_path)[:-5]+\"_preds.json\"\n",
        "  predictions_path=os.path.join(prediction_dir, model_name, file_name)\n",
        "  dataloader=MultiProcessDataLoader(reader= dataset_reader, \\\n",
        "                                              data_path=data_path, batch_sampler= BucketBatchSampler(batch_size=8), \n",
        "                                              cuda_device=torch.cuda.current_device())\n",
        "  dataloader.index_with(vocab)\n",
        "  return evaluate(model, dataloader, cuda_device=0, \\\n",
        "                  predictions_output_file=predictions_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZPiLQW4bZsP"
      },
      "outputs": [],
      "source": [
        "original_num_data_path='drop/drop_dataset_num.json'\n",
        "shuffled_1g_dev_data_path='drop/drop_dataset_num_sh_q_1gram.json'\n",
        "shuffled_2g_dev_data_path='drop/drop_dataset_num_sh_q_2gram.json'\n",
        "shuffled_3g_dev_data_path='drop/drop_dataset_num_sh_q_3gram.json'\n",
        "sh_p_3g= 'drop/drop_dataset_num_sh_p_3g.json'\n",
        "sh_p_2g= 'drop/drop_dataset_num_sh_p_2g.json'\n",
        "sh_p_1g= 'drop/drop_dataset_num_sh_p_1g.json'\n",
        "dev= 'drop/drop_dataset_dev.json'\n",
        "contrast_sets= 'drop/drop_contrast_sets_test.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49uR2bkHfFDF",
        "outputId": "5ea6f0ec-aefd-4d62-a490-652091727858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:50, 135.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.47, f1: 0.48, loss: inf ||: : 857it [01:36,  8.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of numset original\n",
            "{'em': 0.46795152577018545, 'f1': 0.47773835596437486, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, original_num_data_path,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of numset original')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SAIzdFmtBbn",
        "outputId": "ee96af2d-a826-4cd9-c02d-3f925756bda9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 9536it [01:07, 140.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 9536 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.45, f1: 0.49, loss: inf ||: : 1192it [02:13,  8.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  original devset\n",
            "{'em': 0.4540687919463087, 'f1': 0.48862730704698026, 'loss': inf}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, dev,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  original devset')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQVK7obpwG_r",
        "outputId": "bf0e2bdc-02f9-4018-91b0-c74f066b2361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 947it [00:09, 95.22it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 947 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.28, f1: 0.34, loss: inf ||: : 119it [00:13,  8.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  original contrast sets\n",
            "{'em': 0.27666314677930304, 'f1': 0.34233368532206965, 'loss': inf}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, contrast_sets,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  original contrast sets')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1BonVENwX0O",
        "outputId": "37149362-9e9e-479c-ae74-be23c8d4a6e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:48, 140.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.07, f1: 0.07, loss: inf ||: : 857it [01:34,  9.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled q 1g\n",
            "{'em': 0.06760110965104395, 'f1': 0.06782595999415973, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, shuffled_1g_dev_data_path,  f'naqanet_ngrams_final_seed_{seed}')\n",
        "print('results of  shuffled q 1g')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an5-naCqwg0d",
        "outputId": "e8f58f23-d589-476c-9a1e-86d198ad4f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:48, 140.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.08, f1: 0.08, loss: inf ||: : 857it [01:34,  9.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled q 2g\n",
            "{'em': 0.07563147904803622, 'f1': 0.07684041465907433, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, shuffled_2g_dev_data_path,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  shuffled q 2g')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqf61jg5wkhK",
        "outputId": "1fe48cd9-c6c4-4d01-b2a1-6d3187e160c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:48, 140.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.09, f1: 0.09, loss: inf ||: : 857it [01:34,  9.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled q 3g\n",
            "{'em': 0.08745802306906118, 'f1': 0.08944955467951524, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, shuffled_3g_dev_data_path,  f'naqanet_ngrams_final_seed_{seed}')\n",
        "print('results of  shuffled q 3g')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oINCtUqzwv0j",
        "outputId": "848ffbe5-cfa9-454a-9cf8-eb3ea29d98bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:48, 140.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.01, f1: 0.01, loss: inf ||: : 857it [01:33,  9.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled p 1g\n",
            "{'em': 0.012994597751496568, 'f1': 0.013067601109651043, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, sh_p_1g,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  shuffled p 1g')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9b3yNeiwxnt",
        "outputId": "afa8c94a-c9ea-4bd7-c637-09da10655c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:49, 138.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.01, f1: 0.01, loss: inf ||: : 857it [01:34,  9.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled p 2g\n",
            "{'em': 0.012994597751496568, 'f1': 0.013067601109651043, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, sh_p_2g,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  shuffled p 2g')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbNGmSBFwx3k",
        "outputId": "a0cd3d17-9399-407d-833f-b3adf358d639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [01:10, 96.48it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.02, f1: 0.02, loss: inf ||: : 857it [01:33,  9.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled p 3g\n",
            "{'em': 0.017082785808147174, 'f1': 0.01715578916630165, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, sh_p_3g,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  shuffled p 3g')\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMA5SsGDhYilI2sfF3OP7cn",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}