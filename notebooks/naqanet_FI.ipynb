{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/halnegheimish/ForcedInvalidation/blob/main/notebooks/naqanet_FI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Code based on https://github.com/allenai/allennlp-reading-comprehension/blob/master/allennlp_rc/models/naqanet.py, with changes to account for Forced Invalidation"
      ],
      "metadata": {
        "id": "tIW_F_sP1Nxz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_18-vSQ75h7Q",
        "outputId": "e757311a-963b-4cc7-f3c1-244a0d9f306a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting allennlp==2.1.0\n",
            "  Downloading allennlp-2.1.0-py3-none-any.whl (585 kB)\n",
            "\u001b[K     |████████████████████████████████| 585 kB 25.9 MB/s \n",
            "\u001b[?25hCollecting allennlp-models==2.1.0\n",
            "  Downloading allennlp_models-2.1.0-py3-none-any.whl (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 109.8 MB/s \n",
            "\u001b[?25hCollecting spacy<3.1,>=2.1.0\n",
            "  Downloading spacy-3.0.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.0 MB 97.6 MB/s \n",
            "\u001b[?25hCollecting torch<1.8.0,>=1.6.0\n",
            "  Downloading torch-1.7.1-cp38-cp38-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 4.9 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.18 in /usr/local/lib/python3.8/site-packages (from allennlp==2.1.0) (2.22.0)\n",
            "Collecting jsonpickle\n",
            "  Downloading jsonpickle-3.0.1-py2.py3-none-any.whl (40 kB)\n",
            "\u001b[K     |████████████████████████████████| 40 kB 6.9 MB/s \n",
            "\u001b[?25hCollecting lmdb\n",
            "  Downloading lmdb-1.4.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (298 kB)\n",
            "\u001b[K     |████████████████████████████████| 298 kB 105.8 MB/s \n",
            "\u001b[?25hCollecting more-itertools\n",
            "  Downloading more_itertools-9.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 3.3 MB/s \n",
            "\u001b[?25hCollecting pytest\n",
            "  Downloading pytest-7.3.0-py3-none-any.whl (320 kB)\n",
            "\u001b[K     |████████████████████████████████| 320 kB 107.0 MB/s \n",
            "\u001b[?25hCollecting filelock<3.1,>=3.0\n",
            "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.8 MB 60 kB/s \n",
            "\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n",
            "  Downloading jsonnet-0.19.1.tar.gz (593 kB)\n",
            "\u001b[K     |████████████████████████████████| 593 kB 87.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.8/site-packages (from allennlp==2.1.0) (4.42.1)\n",
            "Collecting torchvision<0.9.0,>=0.8.1\n",
            "  Downloading torchvision-0.8.2-cp38-cp38-manylinux1_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 79.1 MB/s \n",
            "\u001b[?25hCollecting boto3<2.0,>=1.14\n",
            "  Downloading boto3-1.26.110-py3-none-any.whl (135 kB)\n",
            "\u001b[K     |████████████████████████████████| 135 kB 119.0 MB/s \n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.97-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 80.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/site-packages (from allennlp==2.1.0) (1.22.3)\n",
            "Collecting h5py\n",
            "  Downloading h5py-3.8.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 88.2 MB/s \n",
            "\u001b[?25hCollecting nltk\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5 MB 82.9 MB/s \n",
            "\u001b[?25hCollecting tensorboardX>=1.2\n",
            "  Downloading tensorboardX-2.6-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[K     |████████████████████████████████| 114 kB 110.6 MB/s \n",
            "\u001b[?25hCollecting scipy\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 34.5 MB 101.6 MB/s \n",
            "\u001b[?25hCollecting transformers<4.4,>=4.1\n",
            "  Downloading transformers-4.3.3-py3-none-any.whl (1.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9 MB 91.2 MB/s \n",
            "\u001b[?25hCollecting overrides==3.1.0\n",
            "  Downloading overrides-3.1.0.tar.gz (11 kB)\n",
            "Collecting conllu==4.4\n",
            "  Downloading conllu-4.4-py2.py3-none-any.whl (15 kB)\n",
            "Collecting py-rouge==1.1\n",
            "  Downloading py_rouge-1.1-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 6.0 MB/s \n",
            "\u001b[?25hCollecting ftfy\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[K     |████████████████████████████████| 53 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting word2number>=1.1\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Downloading preshed-3.0.8-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 115.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (45.2.0.post20200210)\n",
            "Collecting pathy>=0.3.5\n",
            "  Downloading pathy-0.10.1-py3-none-any.whl (48 kB)\n",
            "\u001b[K     |████████████████████████████████| 48 kB 7.0 MB/s \n",
            "\u001b[?25hCollecting typer<0.4.0,>=0.3.0\n",
            "  Downloading typer-0.3.2-py3-none-any.whl (21 kB)\n",
            "Collecting thinc<8.1.0,>=8.0.3\n",
            "  Downloading thinc-8.0.17-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (671 kB)\n",
            "\u001b[K     |████████████████████████████████| 671 kB 102.8 MB/s \n",
            "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
            "  Downloading murmurhash-1.0.9-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.1\n",
            "  Downloading srsly-2.4.6-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
            "\u001b[K     |████████████████████████████████| 493 kB 108.3 MB/s \n",
            "\u001b[?25hCollecting smart-open<7.0.0,>=5.2.1\n",
            "  Downloading smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
            "\u001b[K     |████████████████████████████████| 56 kB 6.1 MB/s \n",
            "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
            "  Downloading cymem-2.0.7-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (3.1.2)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
            "  Downloading pydantic-1.8.2-cp38-cp38-manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7 MB 93.1 MB/s \n",
            "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.5\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Collecting wasabi<1.1.0,>=0.8.1\n",
            "  Downloading wasabi-0.10.1-py3-none-any.whl (26 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.4\n",
            "  Downloading catalogue-2.0.8-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from spacy<3.1,>=2.1.0->allennlp==2.1.0) (23.0)\n",
            "Collecting blis<0.8.0,>=0.4.0\n",
            "  Downloading blis-0.7.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.2 MB 95.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch<1.8.0,>=1.6.0->allennlp==2.1.0) (4.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests>=2.18->allennlp==2.1.0) (2022.12.7)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests>=2.18->allennlp==2.1.0) (2.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests>=2.18->allennlp==2.1.0) (1.25.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests>=2.18->allennlp==2.1.0) (3.0.4)\n",
            "Requirement already satisfied: tomli>=1.0.0; python_version < \"3.11\" in /usr/local/lib/python3.8/site-packages (from pytest->allennlp==2.1.0) (2.0.1)\n",
            "Collecting iniconfig\n",
            "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc8; python_version < \"3.11\"\n",
            "  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\n",
            "Collecting pluggy<2.0,>=0.12\n",
            "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting joblib>=1.1.1\n",
            "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[K     |████████████████████████████████| 297 kB 84.0 MB/s \n",
            "\u001b[?25hCollecting pillow>=4.1.1\n",
            "  Downloading Pillow-9.5.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 74.9 MB/s \n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting botocore<1.30.0,>=1.29.110\n",
            "  Downloading botocore-1.29.110-py3-none-any.whl (10.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 10.6 MB 97.4 MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.7.0,>=0.6.0\n",
            "  Downloading s3transfer-0.6.0-py3-none-any.whl (79 kB)\n",
            "\u001b[K     |████████████████████████████████| 79 kB 10.7 MB/s \n",
            "\u001b[?25hCollecting regex>=2021.8.3\n",
            "  Downloading regex-2023.3.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (771 kB)\n",
            "\u001b[K     |████████████████████████████████| 771 kB 91.9 MB/s \n",
            "\u001b[?25hCollecting click\n",
            "  Downloading click-8.1.3-py3-none-any.whl (96 kB)\n",
            "\u001b[K     |████████████████████████████████| 96 kB 7.5 MB/s \n",
            "\u001b[?25hCollecting protobuf<4,>=3.8.0\n",
            "  Downloading protobuf-3.20.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 73.8 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[K     |████████████████████████████████| 880 kB 90.5 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 92.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.8/site-packages (from ftfy->allennlp-models==2.1.0) (0.2.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from jinja2->spacy<3.1,>=2.1.0->allennlp==2.1.0) (2.1.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.8/site-packages (from botocore<1.30.0,>=1.29.110->boto3<2.0,>=1.14->allennlp==2.1.0) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from sacremoses->transformers<4.4,>=4.1->allennlp==2.1.0) (1.14.0)\n",
            "Building wheels for collected packages: jsonnet, overrides, word2number, sacremoses\n",
            "  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jsonnet: filename=jsonnet-0.19.1-cp38-cp38-linux_x86_64.whl size=6339840 sha256=08e3f9c84ea1ee0370caf872b341b128fa44b45f05d7609af8aea27c302f996c\n",
            "  Stored in directory: /root/.cache/pip/wheels/64/ec/56/de861aae102c449ade2378772abbf9eb7e9acfe9a80f3e6036\n",
            "  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for overrides: filename=overrides-3.1.0-py3-none-any.whl size=10173 sha256=5ed3eaface6c3f4b2821fc5ae0cace7ac727674628fd125d1e4c1c8cbb29d0d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/4f/72/28857f75625b263e2e3f5ab2fc4416c0a85960ac6485007eaa\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5586 sha256=b6fe6f32b46a81f20578ebf6aa76f8b99414748258342a8b29bb1be35ab1bee9\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/f3/5a/d88198fdeb46781ddd7e7f2653061af83e7adb2a076d8886d6\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895254 sha256=3fc611f8ca25f5772b23f17136dca4c5db869f404746df53fb0eb4d83035a0cf\n",
            "  Stored in directory: /root/.cache/pip/wheels/82/ab/9b/c15899bf659ba74f623ac776e861cf2eb8608c1825ddec66a4\n",
            "Successfully built jsonnet overrides word2number sacremoses\n",
            "\u001b[31mERROR: typer 0.3.2 has requirement click<7.2.0,>=7.1.1, but you'll have click 8.1.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: cymem, murmurhash, preshed, click, typer, smart-open, pathy, wasabi, catalogue, pydantic, blis, srsly, thinc, spacy-legacy, spacy, torch, jsonpickle, lmdb, more-itertools, iniconfig, exceptiongroup, pluggy, pytest, filelock, scipy, threadpoolctl, joblib, scikit-learn, jsonnet, pillow, torchvision, jmespath, botocore, s3transfer, boto3, sentencepiece, h5py, regex, nltk, protobuf, tensorboardX, sacremoses, tokenizers, transformers, overrides, allennlp, conllu, py-rouge, ftfy, word2number, allennlp-models\n",
            "Successfully installed allennlp-2.1.0 allennlp-models-2.1.0 blis-0.7.9 boto3-1.26.110 botocore-1.29.110 catalogue-2.0.8 click-8.1.3 conllu-4.4 cymem-2.0.7 exceptiongroup-1.1.1 filelock-3.0.12 ftfy-6.1.1 h5py-3.8.0 iniconfig-2.0.0 jmespath-1.0.1 joblib-1.2.0 jsonnet-0.19.1 jsonpickle-3.0.1 lmdb-1.4.1 more-itertools-9.1.0 murmurhash-1.0.9 nltk-3.8.1 overrides-3.1.0 pathy-0.10.1 pillow-9.5.0 pluggy-1.0.0 preshed-3.0.8 protobuf-3.20.3 py-rouge-1.1 pydantic-1.8.2 pytest-7.3.0 regex-2023.3.23 s3transfer-0.6.0 sacremoses-0.0.53 scikit-learn-1.2.2 scipy-1.10.1 sentencepiece-0.1.97 smart-open-6.3.0 spacy-3.0.9 spacy-legacy-3.0.12 srsly-2.4.6 tensorboardX-2.6 thinc-8.0.17 threadpoolctl-3.1.0 tokenizers-0.10.3 torch-1.7.1 torchvision-0.8.2 transformers-4.3.3 typer-0.3.2 wasabi-0.10.1 word2number-1.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install allennlp==2.1.0 allennlp-models==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWWAoRFiriU4",
        "outputId": "ce431a82-b362-4eb3-e850-f344aaea8fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy-transformers==1.2.1\n",
            "  Downloading spacy_transformers-1.2.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
            "\u001b[K     |████████████████████████████████| 193 kB 26.4 MB/s \n",
            "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
            "  Downloading spacy_alignments-0.9.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 84.7 MB/s \n",
            "\u001b[?25hCollecting spacy<4.0.0,>=3.4.0\n",
            "  Downloading spacy-3.5.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 101.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: srsly<3.0.0,>=2.4.0 in /usr/local/lib/python3.8/site-packages (from spacy-transformers==1.2.1) (2.4.6)\n",
            "Requirement already satisfied: transformers<4.27.0,>=3.4.0 in /usr/local/lib/python3.8/site-packages (from spacy-transformers==1.2.1) (4.3.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/site-packages (from spacy-transformers==1.2.1) (1.22.3)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.8/site-packages (from spacy-transformers==1.2.1) (1.7.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (3.0.8)\n",
            "Collecting thinc<8.2.0,>=8.1.8\n",
            "  Downloading thinc-8.1.9-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (925 kB)\n",
            "\u001b[K     |████████████████████████████████| 925 kB 97.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (2.22.0)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (0.10.1)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (1.8.2)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (0.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (2.0.7)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (6.3.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (0.10.1)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
            "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (3.0.12)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (1.0.9)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (3.1.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (4.42.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (2.0.8)\n",
            "Collecting langcodes<4.0.0,>=3.2.0\n",
            "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 110.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (45.2.0.post20200210)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers==1.2.1) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.8/site-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers==1.2.1) (0.0.53)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.8/site-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers==1.2.1) (0.10.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/site-packages (from transformers<4.27.0,>=3.4.0->spacy-transformers==1.2.1) (2023.3.23)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/site-packages (from torch>=1.6.0->spacy-transformers==1.2.1) (4.4.0)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/site-packages (from thinc<8.2.0,>=8.1.8->spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (0.7.9)\n",
            "Collecting confection<1.0.0,>=0.0.1\n",
            "  Downloading confection-0.0.4-py3-none-any.whl (32 kB)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (1.25.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (2.8)\n",
            "Collecting click<7.2.0,>=7.1.1\n",
            "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
            "\u001b[K     |████████████████████████████████| 82 kB 1.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.8/site-packages (from jinja2->spacy<4.0.0,>=3.4.0->spacy-transformers==1.2.1) (2.1.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/site-packages (from sacremoses->transformers<4.27.0,>=3.4.0->spacy-transformers==1.2.1) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/site-packages (from sacremoses->transformers<4.27.0,>=3.4.0->spacy-transformers==1.2.1) (1.14.0)\n",
            "\u001b[31mERROR: allennlp 2.1.0 has requirement spacy<3.1,>=2.1.0, but you'll have spacy 3.5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: spacy-alignments, confection, thinc, spacy-loggers, langcodes, spacy, spacy-transformers, click\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.0.17\n",
            "    Uninstalling thinc-8.0.17:\n",
            "      Successfully uninstalled thinc-8.0.17\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.0.9\n",
            "    Uninstalling spacy-3.0.9:\n",
            "      Successfully uninstalled spacy-3.0.9\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.3\n",
            "    Uninstalling click-8.1.3:\n",
            "      Successfully uninstalled click-8.1.3\n",
            "Successfully installed click-7.1.2 confection-0.0.4 langcodes-3.3.0 spacy-3.5.1 spacy-alignments-0.9.0 spacy-loggers-1.0.4 spacy-transformers-1.2.1 thinc-8.1.9\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy-transformers==1.2.1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --fuzzy --folder https://drive.google.com/drive/folders/1m-Q7M-yvwHSI11peuMkwssphQ6J4q6-N?usp=share_link"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiFx5WCmvnWt",
        "outputId": "ffd3f56a-c1b9-4bbe-9985-738ac104b935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Processing file 1-TBN0fb8P8eYrRHBGXhayWLrlLfP8oJY drop_aug_train_ngrams.json\n",
            "Processing file 1-YWDM1RNPxUxmpYsRBO5KbuxSCdlUe3- drop_aug_val_ngrams.json\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-TBN0fb8P8eYrRHBGXhayWLrlLfP8oJY\n",
            "To: /content/data/drop/drop_aug_train_ngrams.json\n",
            "100% 175M/175M [00:00<00:00, 301MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-YWDM1RNPxUxmpYsRBO5KbuxSCdlUe3-\n",
            "To: /content/data/drop/drop_aug_val_ngrams.json\n",
            "100% 19.6M/19.6M [00:00<00:00, 233MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget 'https://github.com/halnegheimish/ForcedInvalidation/raw/main/data/eval/drop/drop_dataset_dev.json'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxpiD6O04GLT",
        "outputId": "cf5fb832-f379-4925-c433-1743aafeda1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-11 05:14:45--  https://github.com/halnegheimish/ForcedInvalidation/raw/main/data/eval/drop/drop_dataset_dev.json\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/halnegheimish/ForcedInvalidation/main/data/eval/drop/drop_dataset_dev.json [following]\n",
            "--2023-04-11 05:14:45--  https://raw.githubusercontent.com/halnegheimish/ForcedInvalidation/main/data/eval/drop/drop_dataset_dev.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13741143 (13M) [text/plain]\n",
            "Saving to: ‘drop_dataset_dev.json’\n",
            "\n",
            "drop_dataset_dev.js 100%[===================>]  13.10M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2023-04-11 05:14:45 (420 MB/s) - ‘drop_dataset_dev.json’ saved [13741143/13741143]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install subversion"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBufOIzeWo3O",
        "outputId": "8c46fe66-0e66-4eb6-872f-bd1ce6d41922"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1 libutf8proc2\n",
            "Suggested packages:\n",
            "  db5.3-util libapache2-mod-svn subversion-tools\n",
            "The following NEW packages will be installed:\n",
            "  libapr1 libaprutil1 libserf-1-1 libsvn1 libutf8proc2 subversion\n",
            "0 upgraded, 6 newly installed, 0 to remove and 24 not upgraded.\n",
            "Need to get 2,355 kB of archives.\n",
            "After this operation, 10.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu focal/main amd64 libapr1 amd64 1.6.5-1ubuntu1 [91.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libaprutil1 amd64 1.6.1-4ubuntu2.1 [84.9 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu focal/universe amd64 libserf-1-1 amd64 1.3.9-8build1 [45.2 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu focal/universe amd64 libutf8proc2 amd64 2.5.0-1 [50.0 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 libsvn1 amd64 1.13.0-3ubuntu0.2 [1,260 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 subversion amd64 1.13.0-3ubuntu0.2 [824 kB]\n",
            "Fetched 2,355 kB in 3s (911 kB/s)\n",
            "Selecting previously unselected package libapr1:amd64.\n",
            "(Reading database ... 122349 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libapr1_1.6.5-1ubuntu1_amd64.deb ...\n",
            "Unpacking libapr1:amd64 (1.6.5-1ubuntu1) ...\n",
            "Selecting previously unselected package libaprutil1:amd64.\n",
            "Preparing to unpack .../1-libaprutil1_1.6.1-4ubuntu2.1_amd64.deb ...\n",
            "Unpacking libaprutil1:amd64 (1.6.1-4ubuntu2.1) ...\n",
            "Selecting previously unselected package libserf-1-1:amd64.\n",
            "Preparing to unpack .../2-libserf-1-1_1.3.9-8build1_amd64.deb ...\n",
            "Unpacking libserf-1-1:amd64 (1.3.9-8build1) ...\n",
            "Selecting previously unselected package libutf8proc2:amd64.\n",
            "Preparing to unpack .../3-libutf8proc2_2.5.0-1_amd64.deb ...\n",
            "Unpacking libutf8proc2:amd64 (2.5.0-1) ...\n",
            "Selecting previously unselected package libsvn1:amd64.\n",
            "Preparing to unpack .../4-libsvn1_1.13.0-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking libsvn1:amd64 (1.13.0-3ubuntu0.2) ...\n",
            "Selecting previously unselected package subversion.\n",
            "Preparing to unpack .../5-subversion_1.13.0-3ubuntu0.2_amd64.deb ...\n",
            "Unpacking subversion (1.13.0-3ubuntu0.2) ...\n",
            "Setting up libutf8proc2:amd64 (2.5.0-1) ...\n",
            "Setting up libapr1:amd64 (1.6.5-1ubuntu1) ...\n",
            "Setting up libaprutil1:amd64 (1.6.1-4ubuntu2.1) ...\n",
            "Setting up libserf-1-1:amd64 (1.3.9-8build1) ...\n",
            "Setting up libsvn1:amd64 (1.13.0-3ubuntu0.2) ...\n",
            "Setting up subversion (1.13.0-3ubuntu0.2) ...\n",
            "Processing triggers for libc-bin (2.31-0ubuntu9.9) ...\n",
            "Processing triggers for man-db (2.9.1-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!svn checkout https://github.com/halnegheimish/ForcedInvalidation/trunk/data/eval/drop/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNysATAcW0Eu",
        "outputId": "ed6dbf0b-756f-4524-e712-14592ff4255f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A    drop/drop_contrast_sets_test.json\n",
            "A    drop/drop_dataset_dev.json\n",
            "A    drop/drop_dataset_num.json\n",
            "A    drop/drop_dataset_num_sh_p_1g.json\n",
            "A    drop/drop_dataset_num_sh_p_2g.json\n",
            "A    drop/drop_dataset_num_sh_p_3g.json\n",
            "A    drop/drop_dataset_num_sh_q_1gram.json\n",
            "A    drop/drop_dataset_num_sh_q_2gram.json\n",
            "A    drop/drop_dataset_num_sh_q_3gram.json\n",
            "Checked out revision 9.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tRyvM0C9_lGV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "seed = 42\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "serialization_dir= f\"naqanet_invalid_seed_{seed}\"\n",
        "os.makedirs(serialization_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "ORNi_736xsUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQtrNbeU_7ay"
      },
      "outputs": [],
      "source": [
        "augmented_train_data_path= 'drop/drop_aug_train_ngrams.json'\n",
        "augmented_dev_data_path=   'drop/drop_aug_val_ngrams.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aZZ5lYZ_jBM"
      },
      "source": [
        "#code "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqGVUGMf5vyZ"
      },
      "outputs": [],
      "source": [
        "#naqanet imports\n",
        "from typing import Any, Dict, List, Optional\n",
        "import logging\n",
        "\n",
        "import torch\n",
        "\n",
        "from allennlp.data import Vocabulary\n",
        "from allennlp.models.model import Model\n",
        "from allennlp.modules import Highway\n",
        "from allennlp.nn.activations import Activation\n",
        "from allennlp.modules.feedforward import FeedForward\n",
        "from allennlp.modules import Seq2SeqEncoder, TextFieldEmbedder\n",
        "from allennlp.modules.matrix_attention.matrix_attention import MatrixAttention\n",
        "from allennlp.nn import util, InitializerApplicator, RegularizerApplicator\n",
        "from allennlp.nn.util import masked_softmax\n",
        "\n",
        "from allennlp_models.rc.models.utils import (\n",
        "    get_best_span,\n",
        "    replace_masked_values_with_big_negative_number,\n",
        ")\n",
        "from allennlp_models.rc.metrics.drop_em_and_f1 import DropEmAndF1\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "#train util imports\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "#set random seeds\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "\n",
        "#based on original drop reader https://github.com/allenai/allennlp-models/blob/main/allennlp_models/rc/dataset_readers/drop.py\n",
        "import itertools\n",
        "import json\n",
        "import logging\n",
        "import string\n",
        "from collections import defaultdict\n",
        "from typing import Dict, List, Union, Tuple, Any\n",
        "\n",
        "from overrides import overrides\n",
        "from word2number.w2n import word_to_num\n",
        "\n",
        "from allennlp.common.file_utils import cached_path\n",
        "from allennlp.data.fields import (\n",
        "    Field,\n",
        "    TextField,\n",
        "    MetadataField,\n",
        "    LabelField,\n",
        "    ListField,\n",
        "    SequenceLabelField,\n",
        "    SpanField,\n",
        "    IndexField,\n",
        ")\n",
        "from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n",
        "from allennlp.data.instance import Instance\n",
        "from allennlp.data.token_indexers import SingleIdTokenIndexer, TokenIndexer\n",
        "from allennlp.data.tokenizers import Token, Tokenizer, SpacyTokenizer\n",
        "\n",
        "from allennlp_models.rc.dataset_readers.utils import (\n",
        "    IGNORED_TOKENS,\n",
        "    STRIPPED_CHARACTERS,\n",
        "    make_reading_comprehension_instance,\n",
        "    split_tokens_by_hyphen,\n",
        ")\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "WORD_NUMBER_MAP = {\n",
        "    \"zero\": 0,\n",
        "    \"one\": 1,\n",
        "    \"two\": 2,\n",
        "    \"three\": 3,\n",
        "    \"four\": 4,\n",
        "    \"five\": 5,\n",
        "    \"six\": 6,\n",
        "    \"seven\": 7,\n",
        "    \"eight\": 8,\n",
        "    \"nine\": 9,\n",
        "    \"ten\": 10,\n",
        "    \"eleven\": 11,\n",
        "    \"twelve\": 12,\n",
        "    \"thirteen\": 13,\n",
        "    \"fourteen\": 14,\n",
        "    \"fifteen\": 15,\n",
        "    \"sixteen\": 16,\n",
        "    \"seventeen\": 17,\n",
        "    \"eighteen\": 18,\n",
        "    \"nineteen\": 19,\n",
        "}\n",
        "\n",
        "class DropShuffReaderInvalid(DatasetReader):\n",
        "    \"\"\"\n",
        "    Reads a JSON-formatted DROP dataset file and returns instances in a few different possible\n",
        "    formats.  The input format is complicated; see the test fixture for an example of what it looks\n",
        "    like.  The output formats all contain a question ``TextField``, a passage ``TextField``, and\n",
        "    some kind of answer representation.  Because DROP has instances with several different kinds of\n",
        "    answers, this dataset reader allows you to filter out questions that do not have answers of a\n",
        "    particular type (e.g., remove questions that have numbers as answers, if you model can only\n",
        "    give passage spans as answers).  We typically return all possible ways of arriving at a given\n",
        "    answer string, and expect models to marginalize over these possibilities.\n",
        "    # Parameters\n",
        "    tokenizer : `Tokenizer`, optional (default=`SpacyTokenizer()`)\n",
        "        We use this `Tokenizer` for both the question and the passage.  See :class:`Tokenizer`.\n",
        "        Default is `SpacyTokenizer()`.\n",
        "    token_indexers : `Dict[str, TokenIndexer]`, optional\n",
        "        We similarly use this for both the question and the passage.  See :class:`TokenIndexer`.\n",
        "        Default is `{\"tokens\": SingleIdTokenIndexer()}`.\n",
        "    passage_length_limit : `int`, optional (default=`None`)\n",
        "        If specified, we will cut the passage if the length of passage exceeds this limit.\n",
        "    question_length_limit : `int`, optional (default=`None`)\n",
        "        If specified, we will cut the question if the length of passage exceeds this limit.\n",
        "    skip_when_all_empty: `List[str]`, optional (default=`None`)\n",
        "        In some cases such as preparing for training examples, you may want to skip some examples\n",
        "        when there are no gold labels. You can specify on what condition should the examples be\n",
        "        skipped. Currently, you can put \"passage_span\", \"question_span\", \"addition_subtraction\",\n",
        "        or \"counting\" in this list, to tell the reader skip when there are no such label found.\n",
        "        If not specified, we will keep all the examples.\n",
        "    instance_format: `str`, optional (default=`\"drop\"`)\n",
        "        We try to be generous in providing a few different formats for the instances in DROP,\n",
        "        in terms of the `Fields` that we return for each `Instance`, to allow for several\n",
        "        different kinds of models.  \"drop\" format will do processing to detect numbers and\n",
        "        various ways those numbers can be arrived at from the passage, and return `Fields`\n",
        "        related to that.  \"bert\" format only allows passage spans as answers, and provides a\n",
        "        \"question_and_passage\" field with the two pieces of text joined as BERT expects.\n",
        "        \"squad\" format provides the same fields that our BiDAF and other SQuAD models expect.\n",
        "    relaxed_span_match_for_finding_labels : `bool`, optional (default=`True`)\n",
        "        DROP dataset contains multi-span answers, and the date-type answers are usually hard to\n",
        "        find exact span matches for, also.  In order to use as many examples as possible\n",
        "        to train the model, we may not want a strict match for such cases when finding the gold\n",
        "        span labels. If this argument is true, we will treat every span in the multi-span\n",
        "        answers as correct, and every token in the date answer as correct, too.  Because models\n",
        "        trained on DROP typically marginalize over all possible answer positions, this is just\n",
        "        being a little more generous in what is being marginalized.  Note that this will not\n",
        "        affect evaluation.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        tokenizer: Tokenizer = None,\n",
        "        token_indexers: Dict[str, TokenIndexer] = None,\n",
        "        passage_length_limit: int = None,\n",
        "        question_length_limit: int = None,\n",
        "        skip_when_all_empty: List[str] = None,\n",
        "        instance_format: str = \"drop\",\n",
        "        relaxed_span_match_for_finding_labels: bool = True,\n",
        "        **kwargs,\n",
        "    ) -> None:\n",
        "        super().__init__(**kwargs)\n",
        "        self._tokenizer = tokenizer or SpacyTokenizer()\n",
        "        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n",
        "        self.passage_length_limit = passage_length_limit\n",
        "        self.question_length_limit = question_length_limit\n",
        "        self.skip_when_all_empty = skip_when_all_empty if skip_when_all_empty is not None else []\n",
        "        for item in self.skip_when_all_empty:\n",
        "            assert item in [\n",
        "                \"passage_span\",\n",
        "                \"question_span\",\n",
        "                \"addition_subtraction\",\n",
        "                \"counting\",\n",
        "                \"invalid\",\n",
        "            ], f\"Unsupported skip type: {item}\"\n",
        "        self.instance_format = instance_format\n",
        "        self.relaxed_span_match_for_finding_labels = relaxed_span_match_for_finding_labels\n",
        "\n",
        "    @overrides\n",
        "    def _read(self, file_path: str):\n",
        "        # if `file_path` is a URL, redirect to the cache\n",
        "        file_path = cached_path(file_path, extract_archive=True)\n",
        "        logger.info(\"Reading file at %s\", file_path)\n",
        "        with open(file_path) as dataset_file:\n",
        "            dataset = json.load(dataset_file)\n",
        "        logger.info(\"Reading the dataset\")\n",
        "        kept_count, skip_count = 0, 0\n",
        "        for passage_id, passage_info in dataset.items():\n",
        "            passage_text = passage_info[\"passage\"]\n",
        "            passage_tokens = self._tokenizer.tokenize(passage_text)\n",
        "            passage_tokens = split_tokens_by_hyphen(passage_tokens)\n",
        "            for question_answer in passage_info[\"qa_pairs\"]:\n",
        "                question_id = question_answer[\"query_id\"]\n",
        "                question_text = question_answer[\"question\"].strip()\n",
        "                \n",
        "\n",
        "                answer_annotations = []\n",
        "                if \"answer\" in question_answer:\n",
        "                    answer_annotations.append(question_answer[\"answer\"])\n",
        "                if \"validated_answers\" in question_answer:\n",
        "                    answer_annotations += question_answer[\"validated_answers\"]\n",
        "\n",
        "                instance = self.text_to_instance(\n",
        "                    question_text,\n",
        "                    passage_text,\n",
        "                    question_id,\n",
        "                    passage_id,\n",
        "                    answer_annotations,\n",
        "                    passage_tokens,\n",
        "                )\n",
        "                if instance is not None:\n",
        "                    kept_count += 1\n",
        "                    yield instance\n",
        "                else:\n",
        "                    skip_count += 1\n",
        "        print(f\"Skipped {skip_count} questions, kept {kept_count} questions.\")\n",
        "        logger.info(f\"Skipped {skip_count} questions, kept {kept_count} questions.\")\n",
        "\n",
        "    @overrides\n",
        "    def text_to_instance(\n",
        "        self,  # type: ignore\n",
        "        question_text: str,\n",
        "        passage_text: str,\n",
        "        question_id: str = None,\n",
        "        passage_id: str = None,\n",
        "        answer_annotations: List[Dict] = None,\n",
        "        passage_tokens: List[Token] = None,\n",
        "    ) -> Union[Instance, None]:\n",
        "\n",
        "        if not passage_tokens:\n",
        "            passage_tokens = self._tokenizer.tokenize(passage_text)\n",
        "            passage_tokens = split_tokens_by_hyphen(passage_tokens)\n",
        "        question_tokens = self._tokenizer.tokenize(question_text)\n",
        "        question_tokens = split_tokens_by_hyphen(question_tokens)\n",
        "\n",
        "        if self.passage_length_limit is not None:\n",
        "            passage_tokens = passage_tokens[: self.passage_length_limit]\n",
        "        if self.question_length_limit is not None:\n",
        "            question_tokens = question_tokens[: self.question_length_limit]\n",
        "\n",
        "        answer_type: str = None\n",
        "        answer_texts: List[str] = []\n",
        "        if answer_annotations:\n",
        "\n",
        "            answer_type, answer_texts = self.extract_answer_info_from_annotation(\n",
        "                answer_annotations[0]\n",
        "            )\n",
        "\n",
        "        # Tokenize the answer text in order to find the matched span based on token\n",
        "        tokenized_answer_texts = []\n",
        "        for answer_text in answer_texts:\n",
        "            answer_tokens = self._tokenizer.tokenize(answer_text)\n",
        "            answer_tokens = split_tokens_by_hyphen(answer_tokens)\n",
        "            tokenized_answer_texts.append(\" \".join(token.text for token in answer_tokens))\n",
        "\n",
        "        if self.instance_format == \"squad\":\n",
        "            valid_passage_spans = (\n",
        "                self.find_valid_spans(passage_tokens, tokenized_answer_texts)\n",
        "                if tokenized_answer_texts\n",
        "                else []\n",
        "            )\n",
        "            if not valid_passage_spans:\n",
        "                if \"passage_span\" in self.skip_when_all_empty:\n",
        "                    return None\n",
        "                else:\n",
        "                    valid_passage_spans.append((len(passage_tokens) - 1, len(passage_tokens) - 1))\n",
        "            return make_reading_comprehension_instance(\n",
        "                question_tokens,\n",
        "                passage_tokens,\n",
        "                self._token_indexers,\n",
        "                passage_text,\n",
        "                valid_passage_spans,\n",
        "                answer_texts,\n",
        "                additional_metadata={\n",
        "                    \"original_passage\": passage_text,\n",
        "                    \"original_question\": question_text,\n",
        "                    \"passage_id\": passage_id,\n",
        "                    \"question_id\": question_id,\n",
        "                    \"valid_passage_spans\": valid_passage_spans,\n",
        "                    \"answer_annotations\": answer_annotations,\n",
        "                },\n",
        "            )\n",
        "        elif self.instance_format == \"bert\":\n",
        "            question_concat_passage_tokens = question_tokens + [Token(\"[SEP]\")] + passage_tokens\n",
        "            valid_passage_spans = []\n",
        "            for span in self.find_valid_spans(passage_tokens, tokenized_answer_texts):\n",
        "                # This span is for `question + [SEP] + passage`.\n",
        "                valid_passage_spans.append(\n",
        "                    (span[0] + len(question_tokens) + 1, span[1] + len(question_tokens) + 1)\n",
        "                )\n",
        "            if not valid_passage_spans:\n",
        "                if \"passage_span\" in self.skip_when_all_empty:\n",
        "                    return None\n",
        "                else:\n",
        "                    valid_passage_spans.append(\n",
        "                        (\n",
        "                            len(question_concat_passage_tokens) - 1,\n",
        "                            len(question_concat_passage_tokens) - 1,\n",
        "                        )\n",
        "                    )\n",
        "            answer_info = {\n",
        "                \"answer_texts\": answer_texts,  \n",
        "                \"answer_passage_spans\": valid_passage_spans,\n",
        "            }\n",
        "            return self.make_bert_drop_instance(\n",
        "                question_tokens,\n",
        "                passage_tokens,\n",
        "                question_concat_passage_tokens,\n",
        "                self._token_indexers,\n",
        "                passage_text,\n",
        "                answer_info,\n",
        "                additional_metadata={\n",
        "                    \"original_passage\": passage_text,\n",
        "                    \"original_question\": question_text,\n",
        "                    \"passage_id\": passage_id,\n",
        "                    \"question_id\": question_id,\n",
        "                    \"answer_annotations\": answer_annotations,\n",
        "                },\n",
        "            )\n",
        "\n",
        "        elif self.instance_format == \"drop\":\n",
        "            numbers_in_passage = []\n",
        "            number_indices = []\n",
        "            for token_index, token in enumerate(passage_tokens):\n",
        "                number = self.convert_word_to_number(token.text)\n",
        "                if number is not None:\n",
        "                    numbers_in_passage.append(number)\n",
        "                    number_indices.append(token_index)\n",
        "            # hack to guarantee minimal length of padded number\n",
        "            numbers_in_passage.append(0)\n",
        "            number_indices.append(-1)\n",
        "            numbers_as_tokens = [Token(str(number)) for number in numbers_in_passage]\n",
        "\n",
        "            valid_passage_spans = (\n",
        "                self.find_valid_spans(passage_tokens, tokenized_answer_texts)\n",
        "                if tokenized_answer_texts\n",
        "                else []\n",
        "            )\n",
        "            valid_question_spans = (\n",
        "                self.find_valid_spans(question_tokens, tokenized_answer_texts)\n",
        "                if tokenized_answer_texts\n",
        "                else []\n",
        "            )\n",
        "\n",
        "\n",
        "            target_numbers = []\n",
        "            for answer_text in answer_texts:\n",
        "                number = self.convert_word_to_number(answer_text)\n",
        "                if number is not None:\n",
        "                    target_numbers.append(number)\n",
        "            valid_signs_for_add_sub_expressions: List[List[int]] = []\n",
        "            valid_counts: List[int] = []\n",
        "            invalid_answer: List[int] = []\n",
        "            if answer_type in [\"number\", \"date\"]:\n",
        "                valid_signs_for_add_sub_expressions = self.find_valid_add_sub_expressions(\n",
        "                    numbers_in_passage, target_numbers\n",
        "                )\n",
        "            if answer_type in [\"number\"]:\n",
        "                # Currently we only support count number 0 ~ 9\n",
        "                numbers_for_count = list(range(10))\n",
        "                valid_counts = self.find_valid_counts(numbers_for_count, target_numbers)\n",
        "\n",
        "            if answer_type in ['invalid']:\n",
        "                invalid_answer= target_numbers\n",
        "            \n",
        "            type_to_answer_map = {\n",
        "                \"passage_span\": valid_passage_spans,\n",
        "                \"question_span\": valid_question_spans,\n",
        "                \"addition_subtraction\": valid_signs_for_add_sub_expressions,\n",
        "                \"counting\": valid_counts,\n",
        "                \"invalid\": invalid_answer,\n",
        "            }\n",
        "\n",
        "            if self.skip_when_all_empty and not any(\n",
        "                type_to_answer_map[skip_type] for skip_type in self.skip_when_all_empty\n",
        "            ):\n",
        "                return None\n",
        "\n",
        "            \n",
        "\n",
        "            answer_info = {\n",
        "                \"answer_texts\": answer_texts,  \n",
        "                \"answer_passage_spans\": valid_passage_spans,\n",
        "                \"answer_question_spans\": valid_question_spans,\n",
        "                \"signs_for_add_sub_expressions\": valid_signs_for_add_sub_expressions,\n",
        "                \"counts\": valid_counts,\n",
        "                \"invalid\":invalid_answer,\n",
        "            }\n",
        "\n",
        "            return self.make_marginal_drop_instance(\n",
        "                question_tokens,\n",
        "                passage_tokens,\n",
        "                numbers_as_tokens,\n",
        "                number_indices,\n",
        "                self._token_indexers,\n",
        "                passage_text,\n",
        "                answer_info,\n",
        "                additional_metadata={\n",
        "                    \"original_passage\": passage_text,\n",
        "                    \"original_question\": question_text,\n",
        "                    \"original_numbers\": numbers_in_passage,\n",
        "                    \"passage_id\": passage_id,\n",
        "                    \"question_id\": question_id,\n",
        "                    \"answer_info\": answer_info,\n",
        "                    \"answer_annotations\": answer_annotations,\n",
        "                },\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f'Expect the instance format to be \"drop\", \"squad\" or \"bert\", '\n",
        "                f\"but got {self.instance_format}\"\n",
        "            )\n",
        "\n",
        "   \n",
        "\n",
        "    @staticmethod\n",
        "    def make_marginal_drop_instance(\n",
        "        question_tokens: List[Token],\n",
        "        passage_tokens: List[Token],\n",
        "        number_tokens: List[Token],\n",
        "        number_indices: List[int],\n",
        "        token_indexers: Dict[str, TokenIndexer],\n",
        "        passage_text: str,\n",
        "        answer_info: Dict[str, Any] = None,\n",
        "        additional_metadata: Dict[str, Any] = None,\n",
        "    ) -> Instance:\n",
        "        additional_metadata = additional_metadata or {}\n",
        "        fields: Dict[str, Field] = {}\n",
        "        passage_offsets = [(token.idx, token.idx + len(token.text)) for token in passage_tokens]\n",
        "        question_offsets = [(token.idx, token.idx + len(token.text)) for token in question_tokens]\n",
        "\n",
        "        passage_field = TextField(passage_tokens, token_indexers)\n",
        "        question_field = TextField(question_tokens, token_indexers)\n",
        "\n",
        "        fields[\"passage\"] = passage_field\n",
        "        fields[\"question\"] = question_field\n",
        "\n",
        "        number_index_fields: List[Field] = [\n",
        "            IndexField(index, passage_field) for index in number_indices\n",
        "        ]\n",
        "        fields[\"number_indices\"] = ListField(number_index_fields)\n",
        "      \n",
        "        numbers_in_passage_field = TextField(number_tokens, token_indexers)\n",
        "        metadata = {\n",
        "            \"original_passage\": passage_text,\n",
        "            \"passage_token_offsets\": passage_offsets,\n",
        "            \"question_token_offsets\": question_offsets,\n",
        "            \"question_tokens\": [token.text for token in question_tokens],\n",
        "            \"passage_tokens\": [token.text for token in passage_tokens],\n",
        "            \"number_tokens\": [token.text for token in number_tokens],\n",
        "            \"number_indices\": number_indices,\n",
        "        }\n",
        "        if answer_info:\n",
        "            metadata[\"answer_texts\"] = answer_info[\"answer_texts\"]\n",
        "\n",
        "            passage_span_fields: List[Field] = [\n",
        "                SpanField(span[0], span[1], passage_field)\n",
        "                for span in answer_info[\"answer_passage_spans\"]\n",
        "            ]\n",
        "            if not passage_span_fields:\n",
        "                passage_span_fields.append(SpanField(-1, -1, passage_field))\n",
        "            fields[\"answer_as_passage_spans\"] = ListField(passage_span_fields)\n",
        "\n",
        "            question_span_fields: List[Field] = [\n",
        "                SpanField(span[0], span[1], question_field)\n",
        "                for span in answer_info[\"answer_question_spans\"]\n",
        "            ]\n",
        "            if not question_span_fields:\n",
        "                question_span_fields.append(SpanField(-1, -1, question_field))\n",
        "            fields[\"answer_as_question_spans\"] = ListField(question_span_fields)\n",
        "\n",
        "            add_sub_signs_field: List[Field] = []\n",
        "            for signs_for_one_add_sub_expression in answer_info[\"signs_for_add_sub_expressions\"]:\n",
        "                add_sub_signs_field.append(\n",
        "                    SequenceLabelField(signs_for_one_add_sub_expression, numbers_in_passage_field)\n",
        "                )\n",
        "            if not add_sub_signs_field:\n",
        "                add_sub_signs_field.append(\n",
        "                    SequenceLabelField([0] * len(number_tokens), numbers_in_passage_field)\n",
        "                )\n",
        "            fields[\"answer_as_add_sub_expressions\"] = ListField(add_sub_signs_field)\n",
        "\n",
        "            count_fields: List[Field] = [\n",
        "                LabelField(count_label, skip_indexing=True) for count_label in answer_info[\"counts\"]\n",
        "            ]\n",
        "            if not count_fields:\n",
        "                count_fields.append(LabelField(-1, skip_indexing=True))\n",
        "            fields[\"answer_as_counts\"] = ListField(count_fields)\n",
        "\n",
        "            \n",
        "            invalid_fields: List[Field] = [\n",
        "                LabelField(invalid_label, skip_indexing=True) for invalid_label in answer_info[\"invalid\"]\n",
        "            ]\n",
        "            if not invalid_fields:\n",
        "                invalid_fields.append(LabelField(-1, skip_indexing=True))\n",
        "            fields[\"answer_invalid\"] = ListField(invalid_fields)\n",
        "\n",
        "        metadata.update(additional_metadata)\n",
        "        fields[\"metadata\"] = MetadataField(metadata)\n",
        "        return Instance(fields)\n",
        "\n",
        "    @staticmethod\n",
        "    def make_bert_drop_instance(\n",
        "        question_tokens: List[Token],\n",
        "        passage_tokens: List[Token],\n",
        "        question_concat_passage_tokens: List[Token],\n",
        "        token_indexers: Dict[str, TokenIndexer],\n",
        "        passage_text: str,\n",
        "        answer_info: Dict[str, Any] = None,\n",
        "        additional_metadata: Dict[str, Any] = None,\n",
        "    ) -> Instance:\n",
        "        additional_metadata = additional_metadata or {}\n",
        "        fields: Dict[str, Field] = {}\n",
        "        passage_offsets = [(token.idx, token.idx + len(token.text)) for token in passage_tokens]\n",
        "\n",
        "        # This is separate so we can reference it later with a known type.\n",
        "        passage_field = TextField(passage_tokens, token_indexers)\n",
        "        question_field = TextField(question_tokens, token_indexers)\n",
        "        fields[\"passage\"] = passage_field\n",
        "        fields[\"question\"] = question_field\n",
        "        question_and_passage_field = TextField(question_concat_passage_tokens, token_indexers)\n",
        "        fields[\"question_and_passage\"] = question_and_passage_field\n",
        "\n",
        "        metadata = {\n",
        "            \"original_passage\": passage_text,\n",
        "            \"passage_token_offsets\": passage_offsets,\n",
        "            \"question_tokens\": [token.text for token in question_tokens],\n",
        "            \"passage_tokens\": [token.text for token in passage_tokens],\n",
        "        }\n",
        "\n",
        "        if answer_info:\n",
        "            metadata[\"answer_texts\"] = answer_info[\"answer_texts\"]\n",
        "\n",
        "            passage_span_fields: List[Field] = [\n",
        "                SpanField(span[0], span[1], question_and_passage_field)\n",
        "                for span in answer_info[\"answer_passage_spans\"]\n",
        "            ]\n",
        "            if not passage_span_fields:\n",
        "                passage_span_fields.append(SpanField(-1, -1, question_and_passage_field))\n",
        "            fields[\"answer_as_passage_spans\"] = ListField(passage_span_fields)\n",
        "\n",
        "        metadata.update(additional_metadata)\n",
        "        fields[\"metadata\"] = MetadataField(metadata)\n",
        "        return Instance(fields)\n",
        "\n",
        "    @staticmethod\n",
        "    def extract_answer_info_from_annotation(\n",
        "        answer_annotation: Dict[str, Any]\n",
        "    ) -> Tuple[str, List[str]]:\n",
        "        answer_type = None\n",
        "        if answer_annotation[\"spans\"]:\n",
        "            answer_type = \"spans\"\n",
        "        elif answer_annotation[\"number\"]:\n",
        "            answer_type = \"number\"\n",
        "        elif any(answer_annotation[\"date\"].values()):\n",
        "            answer_type = \"date\"\n",
        "        elif answer_annotation[\"invalid\"]:\n",
        "            answer_type= \"invalid\"\n",
        "\n",
        "        answer_content = answer_annotation[answer_type] if answer_type is not None else None\n",
        "\n",
        "        answer_texts: List[str] = []\n",
        "        if answer_type is None:  # No answer\n",
        "            pass\n",
        "        elif answer_type == \"spans\":\n",
        "            # answer_content is a list of string in this case\n",
        "            answer_texts = answer_content\n",
        "        elif answer_type == \"date\":\n",
        "            # answer_content is a dict with \"month\", \"day\", \"year\" as the keys\n",
        "            date_tokens = [\n",
        "                answer_content[key]\n",
        "                for key in [\"month\", \"day\", \"year\"]\n",
        "                if key in answer_content and answer_content[key]\n",
        "            ]\n",
        "            answer_texts = date_tokens\n",
        "        elif answer_type == \"number\":\n",
        "            # answer_content is a string of number\n",
        "            answer_texts = [answer_content]\n",
        "\n",
        "        elif answer_type == \"invalid\":\n",
        "            answer_texts = [answer_content]\n",
        "\n",
        "        return answer_type, answer_texts\n",
        "\n",
        "    @staticmethod\n",
        "    def convert_word_to_number(word: str, try_to_include_more_numbers=False):\n",
        "        \"\"\"\n",
        "        Currently we only support limited types of conversion.\n",
        "        \"\"\"\n",
        "        if try_to_include_more_numbers:\n",
        "            # strip all punctuations from the sides of the word, except for the negative sign\n",
        "            punctruations = string.punctuation.replace(\"-\", \"\")\n",
        "            word = word.strip(punctruations)\n",
        "            # some words may contain the comma as deliminator\n",
        "            word = word.replace(\",\", \"\")\n",
        "            # word2num will convert hundred, thousand ... to number, but we skip it.\n",
        "            if word in [\"hundred\", \"thousand\", \"million\", \"billion\", \"trillion\"]:\n",
        "                return None\n",
        "            try:\n",
        "                number = word_to_num(word)\n",
        "            except ValueError:\n",
        "                try:\n",
        "                    number = int(word)\n",
        "                except ValueError:\n",
        "                    try:\n",
        "                        number = float(word)\n",
        "                    except ValueError:\n",
        "                        number = None\n",
        "            return number\n",
        "        else:\n",
        "            no_comma_word = word.replace(\",\", \"\")\n",
        "            if no_comma_word in WORD_NUMBER_MAP:\n",
        "                number = WORD_NUMBER_MAP[no_comma_word]\n",
        "            else:\n",
        "                try:\n",
        "                    number = int(no_comma_word)\n",
        "                except ValueError:\n",
        "                    number = None\n",
        "            return number\n",
        "\n",
        "    @staticmethod\n",
        "    def find_valid_spans(\n",
        "        passage_tokens: List[Token], answer_texts: List[str]\n",
        "    ) -> List[Tuple[int, int]]:\n",
        "        normalized_tokens = [\n",
        "            token.text.lower().strip(STRIPPED_CHARACTERS) for token in passage_tokens\n",
        "        ]\n",
        "        word_positions: Dict[str, List[int]] = defaultdict(list)\n",
        "        for i, token in enumerate(normalized_tokens):\n",
        "            word_positions[token].append(i)\n",
        "        spans = []\n",
        "        for answer_text in answer_texts:\n",
        "            answer_tokens = answer_text.lower().strip(STRIPPED_CHARACTERS).split()\n",
        "            num_answer_tokens = len(answer_tokens)\n",
        "            if answer_tokens[0] not in word_positions:\n",
        "                continue\n",
        "            for span_start in word_positions[answer_tokens[0]]:\n",
        "                span_end = span_start  # span_end is _inclusive_\n",
        "                answer_index = 1\n",
        "                while answer_index < num_answer_tokens and span_end + 1 < len(normalized_tokens):\n",
        "                    token = normalized_tokens[span_end + 1]\n",
        "                    if answer_tokens[answer_index].strip(STRIPPED_CHARACTERS) == token:\n",
        "                        answer_index += 1\n",
        "                        span_end += 1\n",
        "                    elif token in IGNORED_TOKENS:\n",
        "                        span_end += 1\n",
        "                    else:\n",
        "                        break\n",
        "                if num_answer_tokens == answer_index:\n",
        "                    spans.append((span_start, span_end))\n",
        "        return spans\n",
        "\n",
        "    @staticmethod\n",
        "    def find_valid_add_sub_expressions(\n",
        "        numbers: List[int], targets: List[int], max_number_of_numbers_to_consider: int = 2\n",
        "    ) -> List[List[int]]:\n",
        "        valid_signs_for_add_sub_expressions = []\n",
        "        for number_of_numbers_to_consider in range(2, max_number_of_numbers_to_consider + 1):\n",
        "            possible_signs = list(itertools.product((-1, 1), repeat=number_of_numbers_to_consider))\n",
        "            for number_combination in itertools.combinations(\n",
        "                enumerate(numbers), number_of_numbers_to_consider\n",
        "            ):\n",
        "                indices = [it[0] for it in number_combination]\n",
        "                values = [it[1] for it in number_combination]\n",
        "                for signs in possible_signs:\n",
        "                    eval_value = sum(sign * value for sign, value in zip(signs, values))\n",
        "                    if eval_value in targets:\n",
        "                        labels_for_numbers = [0] * len(numbers)  \n",
        "                        for index, sign in zip(indices, signs):\n",
        "                            labels_for_numbers[index] = (\n",
        "                                1 if sign == 1 else 2\n",
        "                            )  # 1 for positive, 2 for negative\n",
        "                        valid_signs_for_add_sub_expressions.append(labels_for_numbers)\n",
        "        return valid_signs_for_add_sub_expressions\n",
        "\n",
        "    @staticmethod\n",
        "    def find_valid_counts(count_numbers: List[int], targets: List[int]) -> List[int]:\n",
        "        valid_indices = []\n",
        "        for index, number in enumerate(count_numbers):\n",
        "            if number in targets:\n",
        "                valid_indices.append(index)\n",
        "        return valid_indices\n",
        "\n",
        "from allennlp.nn.util import dist_reduce_sum\n",
        "from allennlp_models.rc.tools.squad import metric_max_over_ground_truths\n",
        "from allennlp_models.rc.tools.drop import (\n",
        "    get_metrics as drop_em_and_f1\n",
        ")\n",
        "\n",
        "def custom_answer_json_to_strings(answer: Dict[str, Any]) -> Tuple[Tuple[str, ...], str]:\n",
        "    \"\"\"\n",
        "    Takes an answer JSON blob from the DROP data release and converts it into strings used for\n",
        "    evaluation.\n",
        "    \"\"\"\n",
        "    if \"number\" in answer and answer[\"number\"]:\n",
        "        return tuple([str(answer[\"number\"])]), \"number\"\n",
        "    elif \"spans\" in answer and answer[\"spans\"]:\n",
        "        return tuple(answer[\"spans\"]), \"span\" if len(answer[\"spans\"]) == 1 else \"spans\"\n",
        "    elif \"invalid\" in answer and answer[\"invalid\"]:\n",
        "        return tuple([str(answer[\"invalid\"])]), 'invalid'\n",
        "    elif \"date\" in answer:\n",
        "        return (\n",
        "            tuple(\n",
        "                [\n",
        "                    \"{0} {1} {2}\".format(\n",
        "                        answer[\"date\"][\"day\"], answer[\"date\"][\"month\"], answer[\"date\"][\"year\"]\n",
        "                    )\n",
        "                ]\n",
        "            ),\n",
        "            \"date\",\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f\"Answer type not found, should be one of number, spans or date at: {json.dumps(answer)}\"\n",
        "        )\n",
        "        \n",
        "class CustomDropEmAndF1(DropEmAndF1):\n",
        "\n",
        "\n",
        "  def __call__(self, prediction: Union[str, List], ground_truths: List):  # type: ignore\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        prediction: ``Union[str, List]``\n",
        "            The predicted answer from the model evaluated. This could be a string, or a list of string\n",
        "            when multiple spans are predicted as answer.\n",
        "        ground_truths: ``List``\n",
        "            All the ground truth answer annotations.\n",
        "        \"\"\"\n",
        "        # If you wanted to split this out by answer type, you could look at [1] here and group by\n",
        "        # that, instead of only keeping [0].\n",
        "        ground_truth_answer_strings = [\n",
        "            custom_answer_json_to_strings(annotation)[0] for annotation in ground_truths\n",
        "        ]\n",
        "        exact_match, f1_score = metric_max_over_ground_truths(\n",
        "            drop_em_and_f1, prediction, ground_truth_answer_strings\n",
        "        )\n",
        "\n",
        "        # Converting to int here, since we want to count the number of exact matches.\n",
        "        self._total_em += dist_reduce_sum(int(exact_match))\n",
        "        self._total_f1 += dist_reduce_sum(f1_score)\n",
        "        self._count += dist_reduce_sum(1)\n",
        "\n",
        "class FINumericallyAugmentedQaNet(Model):\n",
        "    \"\"\"\n",
        "    This class augments the QANet model with some rudimentary numerical reasoning abilities, as\n",
        "    published in the original DROP paper.\n",
        "    The main idea here is that instead of just predicting a passage span after doing all of the\n",
        "    QANet modeling stuff, we add several different \"answer abilities\": predicting a span from the\n",
        "    question, predicting a count, or predicting an arithmetic expression, in addition to predicting\n",
        "    whether the input contains invalid sequences.  Near the end of the QANet model, we have a variable \n",
        "    that predicts what kind of answer type we need, and each branch has separate modeling logic to \n",
        "    predict that answer type.  We then marginalize over all possible ways of getting to the right \n",
        "    answer through each of these answer types.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab: Vocabulary,\n",
        "        text_field_embedder: TextFieldEmbedder,\n",
        "        num_highway_layers: int,\n",
        "        phrase_layer: Seq2SeqEncoder,\n",
        "        matrix_attention_layer: MatrixAttention,\n",
        "        modeling_layer: Seq2SeqEncoder,\n",
        "        dropout_prob: float = 0.1,\n",
        "        initializer: InitializerApplicator = InitializerApplicator(),\n",
        "        regularizer: Optional[RegularizerApplicator] = None,\n",
        "        answering_abilities: List[str] = None,\n",
        "    ) -> None:\n",
        "        super().__init__(vocab, regularizer)\n",
        "\n",
        "        if answering_abilities is None:\n",
        "            self.answering_abilities = [\n",
        "                \"passage_span_extraction\",\n",
        "                \"question_span_extraction\",\n",
        "                \"addition_subtraction\",\n",
        "                \"counting\",\n",
        "                \"invalid\",\n",
        "            ]\n",
        "        else:\n",
        "            self.answering_abilities = answering_abilities\n",
        "\n",
        "        text_embed_dim = text_field_embedder.get_output_dim()\n",
        "        encoding_in_dim = phrase_layer.get_input_dim()\n",
        "        encoding_out_dim = phrase_layer.get_output_dim()\n",
        "        modeling_in_dim = modeling_layer.get_input_dim()\n",
        "        modeling_out_dim = modeling_layer.get_output_dim()\n",
        "\n",
        "        self._text_field_embedder = text_field_embedder\n",
        "\n",
        "        self._embedding_proj_layer = torch.nn.Linear(text_embed_dim, encoding_in_dim)\n",
        "        self._highway_layer = Highway(encoding_in_dim, num_highway_layers)\n",
        "\n",
        "        self._encoding_proj_layer = torch.nn.Linear(encoding_in_dim, encoding_in_dim, bias=True)\n",
        "        self._phrase_layer = phrase_layer\n",
        "\n",
        "        self._matrix_attention = matrix_attention_layer\n",
        "\n",
        "        self._modeling_proj_layer = torch.nn.Linear(\n",
        "            encoding_out_dim * 4, modeling_in_dim, bias=True\n",
        "        )\n",
        "        self._modeling_layer = modeling_layer\n",
        "\n",
        "        self._passage_weights_predictor = torch.nn.Linear(modeling_out_dim, 1)\n",
        "        self._question_weights_predictor = torch.nn.Linear(encoding_out_dim, 1)\n",
        "\n",
        "        if len(self.answering_abilities) > 1:\n",
        "            self._answer_ability_predictor = FeedForward(\n",
        "                modeling_out_dim + encoding_out_dim,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, len(self.answering_abilities)],\n",
        "                num_layers=2,\n",
        "                dropout=dropout_prob,\n",
        "            )\n",
        "\n",
        "        if \"passage_span_extraction\" in self.answering_abilities:\n",
        "            self._passage_span_extraction_index = self.answering_abilities.index(\n",
        "                \"passage_span_extraction\"\n",
        "            )\n",
        "            self._passage_span_start_predictor = FeedForward(\n",
        "                modeling_out_dim * 2,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 1],\n",
        "                num_layers=2,\n",
        "            )\n",
        "            self._passage_span_end_predictor = FeedForward(\n",
        "                modeling_out_dim * 2,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 1],\n",
        "                num_layers=2,\n",
        "            )\n",
        "\n",
        "        if \"question_span_extraction\" in self.answering_abilities:\n",
        "            self._question_span_extraction_index = self.answering_abilities.index(\n",
        "                \"question_span_extraction\"\n",
        "            )\n",
        "            self._question_span_start_predictor = FeedForward(\n",
        "                modeling_out_dim * 2,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 1],\n",
        "                num_layers=2,\n",
        "            )\n",
        "            self._question_span_end_predictor = FeedForward(\n",
        "                modeling_out_dim * 2,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 1],\n",
        "                num_layers=2,\n",
        "            )\n",
        "\n",
        "        if \"addition_subtraction\" in self.answering_abilities:\n",
        "            self._addition_subtraction_index = self.answering_abilities.index(\n",
        "                \"addition_subtraction\"\n",
        "            )\n",
        "            self._number_sign_predictor = FeedForward(\n",
        "                modeling_out_dim * 3,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 3],\n",
        "                num_layers=2,\n",
        "            )\n",
        "\n",
        "        if \"counting\" in self.answering_abilities:\n",
        "            self._counting_index = self.answering_abilities.index(\"counting\")\n",
        "            self._count_number_predictor = FeedForward(\n",
        "                modeling_out_dim,\n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 10],\n",
        "                num_layers=2,\n",
        "            )\n",
        "\n",
        "        if \"invalid\" in self.answering_abilities:\n",
        "            self._invalid_index = self.answering_abilities.index(\"invalid\")\n",
        "            \n",
        "            self._invalid_predictor = FeedForward(\n",
        "                modeling_out_dim + encoding_out_dim, \n",
        "                activations=[Activation.by_name(\"relu\")(), Activation.by_name(\"linear\")()],\n",
        "                hidden_dims=[modeling_out_dim, 2],\n",
        "                num_layers=2,\n",
        "            )\n",
        "\n",
        "        self._drop_metrics = CustomDropEmAndF1()\n",
        "        self._dropout = torch.nn.Dropout(p=dropout_prob)\n",
        "\n",
        "        initializer(self)\n",
        "\n",
        "\n",
        "    def forward( \n",
        "        self,\n",
        "        question: Dict[str, torch.LongTensor],\n",
        "        passage: Dict[str, torch.LongTensor],\n",
        "        number_indices: torch.LongTensor,\n",
        "        answer_as_passage_spans: torch.LongTensor = None,\n",
        "        answer_as_question_spans: torch.LongTensor = None,\n",
        "        answer_as_add_sub_expressions: torch.LongTensor = None,\n",
        "        answer_as_counts: torch.LongTensor = None,\n",
        "        answer_invalid: torch.LongTensor = None, \n",
        "        metadata: List[Dict[str, Any]] = None,\n",
        "    ) -> Dict[str, torch.Tensor]:\n",
        "\n",
        "        question_mask = util.get_text_field_mask(question)\n",
        "        passage_mask = util.get_text_field_mask(passage)\n",
        "        embedded_question = self._dropout(self._text_field_embedder(question))\n",
        "        embedded_passage = self._dropout(self._text_field_embedder(passage))\n",
        "        embedded_question = self._highway_layer(self._embedding_proj_layer(embedded_question))\n",
        "        embedded_passage = self._highway_layer(self._embedding_proj_layer(embedded_passage))\n",
        "\n",
        "        batch_size = embedded_question.size(0)\n",
        "\n",
        "        projected_embedded_question = self._encoding_proj_layer(embedded_question)\n",
        "        projected_embedded_passage = self._encoding_proj_layer(embedded_passage)\n",
        "\n",
        "        encoded_question = self._dropout(\n",
        "            self._phrase_layer(projected_embedded_question, question_mask)\n",
        "        )\n",
        "        encoded_passage = self._dropout(\n",
        "            self._phrase_layer(projected_embedded_passage, passage_mask)\n",
        "        )\n",
        "\n",
        "        # Shape: (batch_size, passage_length, question_length)\n",
        "        passage_question_similarity = self._matrix_attention(encoded_passage, encoded_question)\n",
        "        # Shape: (batch_size, passage_length, question_length)\n",
        "        passage_question_attention = masked_softmax(\n",
        "            passage_question_similarity, question_mask, memory_efficient=True\n",
        "        )\n",
        "        # Shape: (batch_size, passage_length, encoding_dim)\n",
        "        passage_question_vectors = util.weighted_sum(encoded_question, passage_question_attention)\n",
        "\n",
        "        # Shape: (batch_size, question_length, passage_length)\n",
        "        question_passage_attention = masked_softmax(\n",
        "            passage_question_similarity.transpose(1, 2), passage_mask, memory_efficient=True\n",
        "        )\n",
        "\n",
        "        # Shape: (batch_size, passage_length, passage_length)\n",
        "        passsage_attention_over_attention = torch.bmm(\n",
        "            passage_question_attention, question_passage_attention\n",
        "        )\n",
        "        # Shape: (batch_size, passage_length, encoding_dim)\n",
        "        passage_passage_vectors = util.weighted_sum(\n",
        "            encoded_passage, passsage_attention_over_attention\n",
        "        )\n",
        "\n",
        "        # Shape: (batch_size, passage_length, encoding_dim * 4)\n",
        "        merged_passage_attention_vectors = self._dropout(\n",
        "            torch.cat(\n",
        "                [\n",
        "                    encoded_passage,\n",
        "                    passage_question_vectors,\n",
        "                    encoded_passage * passage_question_vectors,\n",
        "                    encoded_passage * passage_passage_vectors,\n",
        "                ],\n",
        "                dim=-1,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # The recurrent modeling layers. Since these layers share the same parameters,\n",
        "        # we don't construct them conditioned on answering abilities.\n",
        "        modeled_passage_list = [self._modeling_proj_layer(merged_passage_attention_vectors)]\n",
        "        for _ in range(4):\n",
        "            modeled_passage = self._dropout(\n",
        "                self._modeling_layer(modeled_passage_list[-1], passage_mask)\n",
        "            )\n",
        "            modeled_passage_list.append(modeled_passage)\n",
        "        # Pop the first one, which is input\n",
        "        modeled_passage_list.pop(0)\n",
        "\n",
        "        # The first modeling layer is used to calculate the vector representation of passage\n",
        "        passage_weights = self._passage_weights_predictor(modeled_passage_list[0]).squeeze(-1)\n",
        "        passage_weights = masked_softmax(passage_weights, passage_mask)\n",
        "        passage_vector = util.weighted_sum(modeled_passage_list[0], passage_weights)\n",
        "        # The vector representation of question is calculated based on the unmatched encoding,\n",
        "        # because we may want to infer the answer ability only based on the question words.\n",
        "        question_weights = self._question_weights_predictor(encoded_question).squeeze(-1)\n",
        "        question_weights = masked_softmax(question_weights, question_mask)\n",
        "        question_vector = util.weighted_sum(encoded_question, question_weights)\n",
        "\n",
        "        if len(self.answering_abilities) > 1:\n",
        "            # Shape: (batch_size, number_of_abilities)\n",
        "            answer_ability_logits = self._answer_ability_predictor(\n",
        "                torch.cat([passage_vector, question_vector], -1)\n",
        "            )\n",
        "            answer_ability_log_probs = torch.nn.functional.log_softmax(answer_ability_logits, -1)\n",
        "            best_answer_ability = torch.argmax(answer_ability_log_probs, 1)\n",
        "\n",
        "        if \"invalid\" in self.answering_abilities:\n",
        "            # Shape: (batch_size, number_of_abilities)\n",
        "            invalid_logits = self._invalid_predictor(\n",
        "                torch.cat([passage_vector, question_vector], -1)\n",
        "            )\n",
        "            invalid_log_probs = torch.nn.functional.log_softmax(invalid_logits, -1)\n",
        "\n",
        "            best_is_invalid_pred = torch.argmax(invalid_log_probs, -1)\n",
        "            best_invalid_log_prob = torch.gather(\n",
        "                invalid_log_probs, 1, best_is_invalid_pred.unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                best_invalid_log_prob += answer_ability_log_probs[:, self._invalid_index]\n",
        "\n",
        "\n",
        "        if \"counting\" in self.answering_abilities:\n",
        "            # Shape: (batch_size, 10)\n",
        "            count_number_logits = self._count_number_predictor(passage_vector)\n",
        "            count_number_log_probs = torch.nn.functional.log_softmax(count_number_logits, -1)\n",
        "            # Info about the best count number prediction\n",
        "            # Shape: (batch_size,)\n",
        "            best_count_number = torch.argmax(count_number_log_probs, -1)\n",
        "            best_count_log_prob = torch.gather(\n",
        "                count_number_log_probs, 1, best_count_number.unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                best_count_log_prob += answer_ability_log_probs[:, self._counting_index]\n",
        "\n",
        "        if \"passage_span_extraction\" in self.answering_abilities:\n",
        "            # Shape: (batch_size, passage_length, modeling_dim * 2))\n",
        "            passage_for_span_start = torch.cat(\n",
        "                [modeled_passage_list[0], modeled_passage_list[1]], dim=-1\n",
        "            )\n",
        "            # Shape: (batch_size, passage_length)\n",
        "            passage_span_start_logits = self._passage_span_start_predictor(\n",
        "                passage_for_span_start\n",
        "            ).squeeze(-1)\n",
        "            # Shape: (batch_size, passage_length, modeling_dim * 2)\n",
        "            passage_for_span_end = torch.cat(\n",
        "                [modeled_passage_list[0], modeled_passage_list[2]], dim=-1\n",
        "            )\n",
        "            # Shape: (batch_size, passage_length)\n",
        "            passage_span_end_logits = self._passage_span_end_predictor(\n",
        "                passage_for_span_end\n",
        "            ).squeeze(-1)\n",
        "            # Shape: (batch_size, passage_length)\n",
        "            passage_span_start_log_probs = util.masked_log_softmax(\n",
        "                passage_span_start_logits, passage_mask\n",
        "            )\n",
        "            passage_span_end_log_probs = util.masked_log_softmax(\n",
        "                passage_span_end_logits, passage_mask\n",
        "            )\n",
        "\n",
        "            # Info about the best passage span prediction\n",
        "            passage_span_start_logits = replace_masked_values_with_big_negative_number(\n",
        "                passage_span_start_logits, passage_mask\n",
        "            )\n",
        "            passage_span_end_logits = replace_masked_values_with_big_negative_number(\n",
        "                passage_span_end_logits, passage_mask\n",
        "            )\n",
        "            # Shape: (batch_size, 2)\n",
        "            best_passage_span = get_best_span(passage_span_start_logits, passage_span_end_logits)\n",
        "            # Shape: (batch_size, 2)\n",
        "            best_passage_start_log_probs = torch.gather(\n",
        "                passage_span_start_log_probs, 1, best_passage_span[:, 0].unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            best_passage_end_log_probs = torch.gather(\n",
        "                passage_span_end_log_probs, 1, best_passage_span[:, 1].unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            # Shape: (batch_size,)\n",
        "            best_passage_span_log_prob = best_passage_start_log_probs + best_passage_end_log_probs\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                best_passage_span_log_prob += answer_ability_log_probs[\n",
        "                    :, self._passage_span_extraction_index\n",
        "                ]\n",
        "\n",
        "        if \"question_span_extraction\" in self.answering_abilities:\n",
        "            # Shape: (batch_size, question_length)\n",
        "            encoded_question_for_span_prediction = torch.cat(\n",
        "                [\n",
        "                    encoded_question,\n",
        "                    passage_vector.unsqueeze(1).repeat(1, encoded_question.size(1), 1),\n",
        "                ],\n",
        "                -1,\n",
        "            )\n",
        "            question_span_start_logits = self._question_span_start_predictor(\n",
        "                encoded_question_for_span_prediction\n",
        "            ).squeeze(-1)\n",
        "            # Shape: (batch_size, question_length)\n",
        "            question_span_end_logits = self._question_span_end_predictor(\n",
        "                encoded_question_for_span_prediction\n",
        "            ).squeeze(-1)\n",
        "            question_span_start_log_probs = util.masked_log_softmax(\n",
        "                question_span_start_logits, question_mask\n",
        "            )\n",
        "            question_span_end_log_probs = util.masked_log_softmax(\n",
        "                question_span_end_logits, question_mask\n",
        "            )\n",
        "\n",
        "            # Info about the best question span prediction\n",
        "            question_span_start_logits = replace_masked_values_with_big_negative_number(\n",
        "                question_span_start_logits, question_mask\n",
        "            )\n",
        "            question_span_end_logits = replace_masked_values_with_big_negative_number(\n",
        "                question_span_end_logits, question_mask\n",
        "            )\n",
        "            # Shape: (batch_size, 2)\n",
        "            best_question_span = get_best_span(question_span_start_logits, question_span_end_logits)\n",
        "            # Shape: (batch_size, 2)\n",
        "            best_question_start_log_probs = torch.gather(\n",
        "                question_span_start_log_probs, 1, best_question_span[:, 0].unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            best_question_end_log_probs = torch.gather(\n",
        "                question_span_end_log_probs, 1, best_question_span[:, 1].unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            # Shape: (batch_size,)\n",
        "            best_question_span_log_prob = (\n",
        "                best_question_start_log_probs + best_question_end_log_probs\n",
        "            )\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                best_question_span_log_prob += answer_ability_log_probs[\n",
        "                    :, self._question_span_extraction_index\n",
        "                ]\n",
        "\n",
        "        if \"addition_subtraction\" in self.answering_abilities:\n",
        "            # Shape: (batch_size, # of numbers in the passage)\n",
        "            number_indices = number_indices.squeeze(-1)\n",
        "            number_mask = number_indices != -1\n",
        "            clamped_number_indices = util.replace_masked_values(number_indices, number_mask, 0)\n",
        "            encoded_passage_for_numbers = torch.cat(\n",
        "                [modeled_passage_list[0], modeled_passage_list[3]], dim=-1\n",
        "            )\n",
        "            # Shape: (batch_size, # of numbers in the passage, encoding_dim)\n",
        "            encoded_numbers = torch.gather(\n",
        "                encoded_passage_for_numbers,\n",
        "                1,\n",
        "                clamped_number_indices.unsqueeze(-1).expand(\n",
        "                    -1, -1, encoded_passage_for_numbers.size(-1)\n",
        "                ),\n",
        "            )\n",
        "            # Shape: (batch_size, # of numbers in the passage)\n",
        "            encoded_numbers = torch.cat(\n",
        "                [\n",
        "                    encoded_numbers,\n",
        "                    passage_vector.unsqueeze(1).repeat(1, encoded_numbers.size(1), 1),\n",
        "                ],\n",
        "                -1,\n",
        "            )\n",
        "\n",
        "            # Shape: (batch_size, # of numbers in the passage, 3)\n",
        "            number_sign_logits = self._number_sign_predictor(encoded_numbers)\n",
        "            number_sign_log_probs = torch.nn.functional.log_softmax(number_sign_logits, -1)\n",
        "\n",
        "            # Shape: (batch_size, # of numbers in passage).\n",
        "            best_signs_for_numbers = torch.argmax(number_sign_log_probs, -1)\n",
        "            # For padding numbers, the best sign masked as 0 (not included).\n",
        "            best_signs_for_numbers = util.replace_masked_values(\n",
        "                best_signs_for_numbers, number_mask, 0\n",
        "            )\n",
        "            # Shape: (batch_size, # of numbers in passage)\n",
        "            best_signs_log_probs = torch.gather(\n",
        "                number_sign_log_probs, 2, best_signs_for_numbers.unsqueeze(-1)\n",
        "            ).squeeze(-1)\n",
        "            # the probs of the masked positions should be 1 so that it will not affect the joint probability\n",
        "            # TODO: this is not quite right, since if there are many numbers in the passage,\n",
        "            # TODO: the joint probability would be very small.\n",
        "            best_signs_log_probs = util.replace_masked_values(best_signs_log_probs, number_mask, 0)\n",
        "            # Shape: (batch_size,)\n",
        "            best_combination_log_prob = best_signs_log_probs.sum(-1)\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                best_combination_log_prob += answer_ability_log_probs[\n",
        "                    :, self._addition_subtraction_index\n",
        "                ]\n",
        "\n",
        "        output_dict = {}\n",
        "\n",
        "        # If answer is given, compute the loss.\n",
        "        if (\n",
        "            answer_as_passage_spans is not None\n",
        "            or answer_as_question_spans is not None\n",
        "            or answer_as_add_sub_expressions is not None\n",
        "            or answer_as_counts is not None\n",
        "            or answer_invalid is not None\n",
        "        ):\n",
        "\n",
        "            log_marginal_likelihood_list = []\n",
        "\n",
        "            for answering_ability in self.answering_abilities:\n",
        "                if answering_ability == \"passage_span_extraction\":\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    gold_passage_span_starts = answer_as_passage_spans[:, :, 0]\n",
        "                    gold_passage_span_ends = answer_as_passage_spans[:, :, 1]\n",
        "                    # Some spans are padded with index -1,\n",
        "                    # so we clamp those paddings to 0 and then mask after `torch.gather()`.\n",
        "                    gold_passage_span_mask = gold_passage_span_starts != -1\n",
        "                    clamped_gold_passage_span_starts = util.replace_masked_values(\n",
        "                        gold_passage_span_starts, gold_passage_span_mask, 0\n",
        "                    )\n",
        "                    clamped_gold_passage_span_ends = util.replace_masked_values(\n",
        "                        gold_passage_span_ends, gold_passage_span_mask, 0\n",
        "                    )\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    log_likelihood_for_passage_span_starts = torch.gather(\n",
        "                        passage_span_start_log_probs, 1, clamped_gold_passage_span_starts\n",
        "                    )\n",
        "                    log_likelihood_for_passage_span_ends = torch.gather(\n",
        "                        passage_span_end_log_probs, 1, clamped_gold_passage_span_ends\n",
        "                    )\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    log_likelihood_for_passage_spans = (\n",
        "                        log_likelihood_for_passage_span_starts\n",
        "                        + log_likelihood_for_passage_span_ends\n",
        "                    )\n",
        "                    # For those padded spans, we set their log probabilities to be very small negative value\n",
        "                    log_likelihood_for_passage_spans = (\n",
        "                        replace_masked_values_with_big_negative_number(\n",
        "                            log_likelihood_for_passage_spans,\n",
        "                            gold_passage_span_mask,\n",
        "                        )\n",
        "                    )\n",
        "                    # Shape: (batch_size, )\n",
        "                    log_marginal_likelihood_for_passage_span = util.logsumexp(\n",
        "                        log_likelihood_for_passage_spans\n",
        "                    )\n",
        "                    log_marginal_likelihood_list.append(log_marginal_likelihood_for_passage_span)\n",
        "\n",
        "                elif answering_ability == \"question_span_extraction\":\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    gold_question_span_starts = answer_as_question_spans[:, :, 0]\n",
        "                    gold_question_span_ends = answer_as_question_spans[:, :, 1]\n",
        "                    # Some spans are padded with index -1,\n",
        "                    # so we clamp those paddings to 0 and then mask after `torch.gather()`.\n",
        "                    gold_question_span_mask = gold_question_span_starts != -1\n",
        "                    clamped_gold_question_span_starts = util.replace_masked_values(\n",
        "                        gold_question_span_starts, gold_question_span_mask, 0\n",
        "                    )\n",
        "                    clamped_gold_question_span_ends = util.replace_masked_values(\n",
        "                        gold_question_span_ends, gold_question_span_mask, 0\n",
        "                    )\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    log_likelihood_for_question_span_starts = torch.gather(\n",
        "                        question_span_start_log_probs, 1, clamped_gold_question_span_starts\n",
        "                    )\n",
        "                    log_likelihood_for_question_span_ends = torch.gather(\n",
        "                        question_span_end_log_probs, 1, clamped_gold_question_span_ends\n",
        "                    )\n",
        "                    # Shape: (batch_size, # of answer spans)\n",
        "                    log_likelihood_for_question_spans = (\n",
        "                        log_likelihood_for_question_span_starts\n",
        "                        + log_likelihood_for_question_span_ends\n",
        "                    )\n",
        "                    # For those padded spans, we set their log probabilities to be very small negative value\n",
        "                    log_likelihood_for_question_spans = (\n",
        "                        replace_masked_values_with_big_negative_number(\n",
        "                            log_likelihood_for_question_spans,\n",
        "                            gold_question_span_mask,\n",
        "                        )\n",
        "                    )\n",
        "                    # Shape: (batch_size, )\n",
        "\n",
        "                    log_marginal_likelihood_for_question_span = util.logsumexp(\n",
        "                        log_likelihood_for_question_spans\n",
        "                    )\n",
        "                    log_marginal_likelihood_list.append(log_marginal_likelihood_for_question_span)\n",
        "\n",
        "                elif answering_ability == \"addition_subtraction\":\n",
        "                    # The padded add-sub combinations use 0 as the signs for all numbers, and we mask them here.\n",
        "                    # Shape: (batch_size, # of combinations)\n",
        "                    gold_add_sub_mask = answer_as_add_sub_expressions.sum(-1) > 0\n",
        "                    # Shape: (batch_size, # of numbers in the passage, # of combinations)\n",
        "                    gold_add_sub_signs = answer_as_add_sub_expressions.transpose(1, 2)\n",
        "                    # Shape: (batch_size, # of numbers in the passage, # of combinations)\n",
        "                    log_likelihood_for_number_signs = torch.gather(\n",
        "                        number_sign_log_probs, 2, gold_add_sub_signs\n",
        "                    )\n",
        "                    # the log likelihood of the masked positions should be 0\n",
        "                    # so that it will not affect the joint probability\n",
        "                    log_likelihood_for_number_signs = util.replace_masked_values(\n",
        "                        log_likelihood_for_number_signs, number_mask.unsqueeze(-1), 0\n",
        "                    )\n",
        "                    # Shape: (batch_size, # of combinations)\n",
        "                    log_likelihood_for_add_subs = log_likelihood_for_number_signs.sum(1)\n",
        "                    # For those padded combinations, we set their log probabilities to be very small negative value\n",
        "                    log_likelihood_for_add_subs = replace_masked_values_with_big_negative_number(\n",
        "                        log_likelihood_for_add_subs, gold_add_sub_mask\n",
        "                    )\n",
        "                    # Shape: (batch_size, )\n",
        "                    log_marginal_likelihood_for_add_sub = util.logsumexp(\n",
        "                        log_likelihood_for_add_subs\n",
        "                    )\n",
        "                    log_marginal_likelihood_list.append(log_marginal_likelihood_for_add_sub)\n",
        "\n",
        "                elif answering_ability == \"counting\":\n",
        "                    # Count answers are padded with label -1,\n",
        "                    # so we clamp those paddings to 0 and then mask after `torch.gather()`.\n",
        "                    # Shape: (batch_size, # of count answers)\n",
        "                    gold_count_mask = answer_as_counts != -1\n",
        "                    # Shape: (batch_size, # of count answers)\n",
        "                    clamped_gold_counts = util.replace_masked_values(\n",
        "                        answer_as_counts, gold_count_mask, 0\n",
        "                    )\n",
        "                    log_likelihood_for_counts = torch.gather(\n",
        "                        count_number_log_probs, 1, clamped_gold_counts\n",
        "                    )\n",
        "                    # For those padded spans, we set their log probabilities to be very small negative value\n",
        "                    log_likelihood_for_counts = replace_masked_values_with_big_negative_number(\n",
        "                        log_likelihood_for_counts, gold_count_mask\n",
        "                    )\n",
        "                    # Shape: (batch_size, )\n",
        "                    log_marginal_likelihood_for_count = util.logsumexp(log_likelihood_for_counts)\n",
        "                    log_marginal_likelihood_list.append(log_marginal_likelihood_for_count)\n",
        "                    \n",
        "                elif answering_ability == \"invalid\":\n",
        "                    gold_invalid_mask = answer_invalid != -1\n",
        "                    # Shape: (batch_size, # of count answers)\n",
        "                    clamped_gold_invalid = util.replace_masked_values(\n",
        "                        answer_invalid, gold_invalid_mask, 0\n",
        "                    ) \n",
        "                    log_likelihood_for_invalid = torch.gather(\n",
        "                        invalid_log_probs, 1, clamped_gold_invalid\n",
        "                    )\n",
        "                    # For those padded spans, we set their log probabilities to be very small negative value\n",
        "                    log_likelihood_for_invalid = replace_masked_values_with_big_negative_number(\n",
        "                        log_likelihood_for_invalid, gold_invalid_mask\n",
        "                    )\n",
        "                    # Shape: (batch_size, )\n",
        "                    log_marginal_likelihood_for_invalid = util.logsumexp(log_likelihood_for_invalid)\n",
        "                    log_marginal_likelihood_list.append(log_marginal_likelihood_for_invalid)\n",
        "\n",
        "\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported answering ability: {answering_ability}\")\n",
        "\n",
        "            if len(self.answering_abilities) > 1:\n",
        "                # Add the ability probabilities if there are more than one abilities\n",
        "                all_log_marginal_likelihoods = torch.stack(log_marginal_likelihood_list, dim=-1)\n",
        "                all_log_marginal_likelihoods = (\n",
        "                    all_log_marginal_likelihoods + answer_ability_log_probs\n",
        "                )\n",
        "                marginal_log_likelihood = util.logsumexp(all_log_marginal_likelihoods)\n",
        "            else:\n",
        "                marginal_log_likelihood = log_marginal_likelihood_list[0]\n",
        "\n",
        "            output_dict[\"loss\"] = -marginal_log_likelihood.mean()\n",
        "\n",
        "        # Compute the metrics and add the tokenized input to the output.\n",
        "        if metadata is not None:\n",
        "            output_dict[\"question_id\"] = []\n",
        "            output_dict[\"answer\"] = []\n",
        "            question_tokens = []\n",
        "            passage_tokens = []\n",
        "            for i in range(batch_size):\n",
        "                question_tokens.append(metadata[i][\"question_tokens\"])\n",
        "                passage_tokens.append(metadata[i][\"passage_tokens\"])\n",
        "\n",
        "                if len(self.answering_abilities) > 1:\n",
        "                    predicted_ability_str = self.answering_abilities[\n",
        "                        best_answer_ability[i].detach().cpu().numpy()\n",
        "                    ]\n",
        "                else:\n",
        "                    predicted_ability_str = self.answering_abilities[0]\n",
        "\n",
        "                answer_json: Dict[str, Any] = {}\n",
        "\n",
        "                # We did not consider multi-mention answers here\n",
        "                if predicted_ability_str == \"passage_span_extraction\":\n",
        "                    answer_json[\"answer_type\"] = \"passage_span\"\n",
        "                    passage_str = metadata[i][\"original_passage\"]\n",
        "                    offsets = metadata[i][\"passage_token_offsets\"]\n",
        "                    predicted_span = tuple(best_passage_span[i].detach().cpu().numpy())\n",
        "                    start_offset = offsets[predicted_span[0]][0]\n",
        "                    end_offset = offsets[predicted_span[1]][1]\n",
        "                    predicted_answer = passage_str[start_offset:end_offset]\n",
        "                    answer_json[\"value\"] = predicted_answer\n",
        "                    answer_json[\"spans\"] = [(start_offset, end_offset)]\n",
        "                elif predicted_ability_str == \"question_span_extraction\":\n",
        "                    answer_json[\"answer_type\"] = \"question_span\"\n",
        "                    question_str = metadata[i][\"original_question\"]\n",
        "                    offsets = metadata[i][\"question_token_offsets\"]\n",
        "                    predicted_span = tuple(best_question_span[i].detach().cpu().numpy())\n",
        "                    start_offset = offsets[predicted_span[0]][0]\n",
        "                    end_offset = offsets[predicted_span[1]][1]\n",
        "                    predicted_answer = question_str[start_offset:end_offset]\n",
        "                    answer_json[\"value\"] = predicted_answer\n",
        "                    answer_json[\"spans\"] = [(start_offset, end_offset)]\n",
        "                elif (\n",
        "                    predicted_ability_str == \"addition_subtraction\"\n",
        "                ):  # plus_minus combination answer\n",
        "                    answer_json[\"answer_type\"] = \"arithmetic\"\n",
        "                    original_numbers = metadata[i][\"original_numbers\"]\n",
        "                    sign_remap = {0: 0, 1: 1, 2: -1}\n",
        "                    predicted_signs = [\n",
        "                        sign_remap[it] for it in best_signs_for_numbers[i].detach().cpu().numpy()\n",
        "                    ]\n",
        "                    result = sum(\n",
        "                        [sign * number for sign, number in zip(predicted_signs, original_numbers)]\n",
        "                    )\n",
        "                    predicted_answer = str(result)\n",
        "                    offsets = metadata[i][\"passage_token_offsets\"]\n",
        "                    number_indices = metadata[i][\"number_indices\"]\n",
        "                    number_positions = [offsets[index] for index in number_indices]\n",
        "                    answer_json[\"numbers\"] = []\n",
        "                    for offset, value, sign in zip(\n",
        "                        number_positions, original_numbers, predicted_signs\n",
        "                    ):\n",
        "                        answer_json[\"numbers\"].append(\n",
        "                            {\"span\": offset, \"value\": value, \"sign\": sign}\n",
        "                        )\n",
        "                    if number_indices[-1] == -1:\n",
        "                        # There is a dummy 0 number at position -1 added in some cases; we are\n",
        "                        # removing that here.\n",
        "                        answer_json[\"numbers\"].pop()\n",
        "                    answer_json[\"value\"] = result\n",
        "                elif predicted_ability_str == \"counting\":\n",
        "                    answer_json[\"answer_type\"] = \"count\"\n",
        "                    predicted_count = best_count_number[i].detach().cpu().numpy()\n",
        "                    predicted_answer = str(predicted_count)\n",
        "                    answer_json[\"count\"] = predicted_count\n",
        "                elif predicted_ability_str == \"invalid\":\n",
        "                    answer_json[\"answer_type\"] = \"invalid\"\n",
        "                    predicted_is_invalid = best_is_invalid_pred[i].detach().cpu().numpy()\n",
        "                    predicted_answer = str(predicted_is_invalid)\n",
        "                    answer_json[\"invalid\"] = predicted_is_invalid\n",
        "                else:\n",
        "                    raise ValueError(f\"Unsupported answer ability: {predicted_ability_str}\")\n",
        "\n",
        "                output_dict[\"question_id\"].append(metadata[i][\"question_id\"])\n",
        "                output_dict[\"answer\"].append(answer_json)\n",
        "                answer_annotations = metadata[i].get(\"answer_annotations\", [])\n",
        "                if answer_annotations:\n",
        "                    self._drop_metrics(predicted_answer, answer_annotations)\n",
        "            # This is used for the demo.\n",
        "            output_dict[\"passage_question_attention\"] = passage_question_attention\n",
        "            output_dict[\"question_tokens\"] = question_tokens\n",
        "            output_dict[\"passage_tokens\"] = passage_tokens\n",
        "        return output_dict\n",
        "\n",
        "    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n",
        "        exact_match, f1_score = self._drop_metrics.get_metric(reset)\n",
        "        return {\"em\": exact_match, \"f1\": f1_score}\n",
        "\n",
        "    default_predictor = \"reading_comprehension\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9Hf5WxLAUJ_"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GyVK7MATAQ62",
        "outputId": "3162cb7e-d39e-4cce-ff44-54e868534c3d"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading instances: 0it [00:00, ?it/s]/usr/local/lib/python3.8/dist-packages/spacy/pipeline/lemmatizer.py:211: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n",
            "loading instances: 131178it [39:34, 55.25it/s]\n",
            "loading instances: 0it [00:00, ?it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipped 7592 questions, kept 131178 questions.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "loading instances: 15578it [04:25, 58.76it/s]\n",
            "building vocab: 82it [00:00, 813.83it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Skipped 0 questions, kept 15578 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "building vocab: 146756it [02:56, 831.19it/s]\n",
            "downloading: 100%|##########| 1687192311/1687192311 [01:46<00:00, 15860588.54B/s]\n",
            "1702926it [00:31, 53566.71it/s]\n",
            "1702926it [00:34, 49235.65it/s]\n",
            "em: 0.5199, f1: 0.5343, batch_loss: 1.9508, loss: 2.4818, batch_reg_loss: 0.0010, reg_loss: 0.0009 ||: 100%|##########| 8199/8199 [49:23<00:00,  2.77it/s]\n",
            "em: 0.5557, f1: 0.5713, batch_loss: 21267646664908053738231511467782307840.0000, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:17<00:00,  7.06it/s]\n",
            "em: 0.5725, f1: 0.5904, batch_loss: 1.7033, loss: 1.9934, batch_reg_loss: 0.0012, reg_loss: 0.0011 ||: 100%|##########| 8199/8199 [49:55<00:00,  2.74it/s]\n",
            "em: 0.5795, f1: 0.5957, batch_loss: 21267646664908053738231511467782307840.0000, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:17<00:00,  7.07it/s]\n",
            "em: 0.5973, f1: 0.6174, batch_loss: 0.7118, loss: 1.8231, batch_reg_loss: 0.0014, reg_loss: 0.0013 ||: 100%|##########| 8199/8199 [49:33<00:00,  2.76it/s]\n",
            "em: 0.6005, f1: 0.6178, batch_loss: inf, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:16<00:00,  7.15it/s]   \n",
            "em: 0.6131, f1: 0.6331, batch_loss: 0.8972, loss: 1.7312, batch_reg_loss: 0.0017, reg_loss: 0.0016 ||: 100%|##########| 8199/8199 [48:16<00:00,  2.83it/s]\n",
            "em: 0.6118, f1: 0.6296, batch_loss: 21267646664908053738231511467782307840.0000, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:13<00:00,  7.28it/s]\n",
            "em: 0.6254, f1: 0.6458, batch_loss: 0.6699, loss: 1.6663, batch_reg_loss: 0.0019, reg_loss: 0.0018 ||: 100%|##########| 8199/8199 [47:51<00:00,  2.85it/s]\n",
            "em: 0.6231, f1: 0.6420, batch_loss: 1.9062, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:13<00:00,  7.28it/s]\n",
            "em: 0.6348, f1: 0.6557, batch_loss: 1.5531, loss: 1.6124, batch_reg_loss: 0.0021, reg_loss: 0.0020 ||: 100%|##########| 8199/8199 [47:41<00:00,  2.86it/s]\n",
            "em: 0.6315, f1: 0.6506, batch_loss: 1.8782, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:13<00:00,  7.29it/s]\n",
            "em: 0.6395, f1: 0.6607, batch_loss: 0.1697, loss: 1.5667, batch_reg_loss: 0.0023, reg_loss: 0.0022 ||: 100%|##########| 8199/8199 [47:43<00:00,  2.86it/s]\n",
            "em: 0.6371, f1: 0.6560, batch_loss: 21267646664908053738231511467782307840.0000, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.25it/s]\n",
            "em: 0.6463, f1: 0.6683, batch_loss: 2.3627, loss: 1.5247, batch_reg_loss: 0.0025, reg_loss: 0.0024 ||: 100%|##########| 8199/8199 [47:58<00:00,  2.85it/s]\n",
            "em: 0.6430, f1: 0.6624, batch_loss: 1.5961, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.25it/s]\n",
            "em: 0.6560, f1: 0.6782, batch_loss: 0.5938, loss: 1.4771, batch_reg_loss: 0.0027, reg_loss: 0.0026 ||: 100%|##########| 8199/8199 [47:56<00:00,  2.85it/s]\n",
            "em: 0.6506, f1: 0.6707, batch_loss: 21267646664908053738231511467782307840.0000, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.26it/s]\n",
            "em: 0.6625, f1: 0.6857, batch_loss: 1.3846, loss: 1.4335, batch_reg_loss: 0.0029, reg_loss: 0.0028 ||: 100%|##########| 8199/8199 [47:46<00:00,  2.86it/s]\n",
            "em: 0.6609, f1: 0.6806, batch_loss: 21267646664908053738231511467782307840.0000, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.26it/s]\n",
            "em: 0.6718, f1: 0.6951, batch_loss: 1.2970, loss: 1.3867, batch_reg_loss: 0.0031, reg_loss: 0.0030 ||: 100%|##########| 8199/8199 [47:43<00:00,  2.86it/s]\n",
            "em: 0.6708, f1: 0.6919, batch_loss: 1.0699, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:13<00:00,  7.28it/s]\n",
            "em: 0.6776, f1: 0.7017, batch_loss: 2.3795, loss: 1.3571, batch_reg_loss: 0.0033, reg_loss: 0.0032 ||: 100%|##########| 8199/8199 [47:49<00:00,  2.86it/s]\n",
            "em: 0.6773, f1: 0.6992, batch_loss: inf, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.25it/s]\n",
            "em: 0.6840, f1: 0.7084, batch_loss: 1.7967, loss: 1.3242, batch_reg_loss: 0.0035, reg_loss: 0.0034 ||: 100%|##########| 8199/8199 [48:02<00:00,  2.84it/s]\n",
            "em: 0.6819, f1: 0.7033, batch_loss: 2.5904, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.25it/s]\n",
            "em: 0.6880, f1: 0.7123, batch_loss: 2.2996, loss: 1.3008, batch_reg_loss: 0.0037, reg_loss: 0.0036 ||: 100%|##########| 8199/8199 [48:04<00:00,  2.84it/s]\n",
            "em: 0.6854, f1: 0.7075, batch_loss: inf, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:15<00:00,  7.19it/s]\n",
            "em: 0.6912, f1: 0.7158, batch_loss: 1.1860, loss: 1.2806, batch_reg_loss: 0.0039, reg_loss: 0.0038 ||: 100%|##########| 8199/8199 [48:20<00:00,  2.83it/s]\n",
            "em: 0.6892, f1: 0.7110, batch_loss: 1.2927, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:15<00:00,  7.17it/s]\n",
            "em: 0.6949, f1: 0.7202, batch_loss: 0.8434, loss: 1.2607, batch_reg_loss: 0.0041, reg_loss: 0.0040 ||: 100%|##########| 8199/8199 [48:10<00:00,  2.84it/s]\n",
            "em: 0.6898, f1: 0.7122, batch_loss: 0.8706, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:13<00:00,  7.27it/s]\n",
            "em: 0.6983, f1: 0.7233, batch_loss: 0.8208, loss: 1.2419, batch_reg_loss: 0.0044, reg_loss: 0.0042 ||: 100%|##########| 8199/8199 [47:54<00:00,  2.85it/s]\n",
            "em: 0.6937, f1: 0.7160, batch_loss: 1.5934, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.22it/s]\n",
            "em: 0.7015, f1: 0.7266, batch_loss: 0.7169, loss: 1.2245, batch_reg_loss: 0.0046, reg_loss: 0.0045 ||: 100%|##########| 8199/8199 [48:26<00:00,  2.82it/s]\n",
            "em: 0.6972, f1: 0.7194, batch_loss: 1.0117, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:15<00:00,  7.18it/s]\n",
            "em: 0.7018, f1: 0.7275, batch_loss: 1.2530, loss: 1.2135, batch_reg_loss: 0.0048, reg_loss: 0.0047 ||: 100%|##########| 8199/8199 [48:33<00:00,  2.81it/s]\n",
            "em: 0.6973, f1: 0.7200, batch_loss: inf, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.23it/s]\n",
            "em: 0.7065, f1: 0.7320, batch_loss: 1.4095, loss: 1.1963, batch_reg_loss: 0.0050, reg_loss: 0.0049 ||: 100%|##########| 8199/8199 [47:54<00:00,  2.85it/s]\n",
            "em: 0.6975, f1: 0.7197, batch_loss: 1.2672, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:13<00:00,  7.27it/s]\n",
            "em: 0.7080, f1: 0.7338, batch_loss: 0.8467, loss: 1.1863, batch_reg_loss: 0.0052, reg_loss: 0.0051 ||: 100%|##########| 8199/8199 [47:57<00:00,  2.85it/s]\n",
            "em: 0.7005, f1: 0.7229, batch_loss: inf, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.23it/s]\n",
            "em: 0.7090, f1: 0.7346, batch_loss: 0.8988, loss: 1.1719, batch_reg_loss: 0.0054, reg_loss: 0.0053 ||: 100%|##########| 8199/8199 [47:48<00:00,  2.86it/s]\n",
            "em: 0.7025, f1: 0.7246, batch_loss: 1.8658, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.22it/s]\n",
            "em: 0.7119, f1: 0.7380, batch_loss: 1.6809, loss: 1.1647, batch_reg_loss: 0.0056, reg_loss: 0.0055 ||: 100%|##########| 8199/8199 [48:16<00:00,  2.83it/s]\n",
            "em: 0.7037, f1: 0.7259, batch_loss: 2.5195, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:15<00:00,  7.19it/s]\n",
            "em: 0.7137, f1: 0.7398, batch_loss: 2.1629, loss: 1.1480, batch_reg_loss: 0.0058, reg_loss: 0.0057 ||: 100%|##########| 8199/8199 [48:06<00:00,  2.84it/s]\n",
            "em: 0.7027, f1: 0.7260, batch_loss: 1.7579, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:16<00:00,  7.14it/s]\n",
            "em: 0.7143, f1: 0.7401, batch_loss: 0.6558, loss: 1.1484, batch_reg_loss: 0.0060, reg_loss: 0.0059 ||: 100%|##########| 8199/8199 [48:39<00:00,  2.81it/s]\n",
            "em: 0.7050, f1: 0.7285, batch_loss: 0.7021, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.25it/s]                                     \n",
            "em: 0.7166, f1: 0.7428, batch_loss: 1.5272, loss: 1.1318, batch_reg_loss: 0.0062, reg_loss: 0.0061 ||: 100%|##########| 8199/8199 [48:07<00:00,  2.84it/s]\n",
            "em: 0.7069, f1: 0.7299, batch_loss: 21267646664908053738231511467782307840.0000, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:14<00:00,  7.23it/s]\n",
            "em: 0.7191, f1: 0.7454, batch_loss: 2.1551, loss: 1.1258, batch_reg_loss: 0.0064, reg_loss: 0.0063 ||: 100%|##########| 8199/8199 [47:54<00:00,  2.85it/s]\n",
            "em: 0.7084, f1: 0.7313, batch_loss: 21267646664908053738231511467782307840.0000, loss: inf, batch_reg_loss: 0.0000, reg_loss: 0.0000 ||: 100%|##########| 974/974 [02:13<00:00,  7.28it/s]\n",
            "em: 0.7226, f1: 0.7487, batch_loss: 0.8340, loss: 1.0988, batch_reg_loss: 0.0065, reg_loss: 0.0064 ||:  16%|#6        | 1314/8199 [07:44<33:11,  3.46it/s]"
          ]
        }
      ],
      "source": [
        "#from training util\n",
        "from allennlp.data import Instance, Vocabulary, Batch, DataLoader\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "\n",
        "#me\n",
        "from allennlp_models.rc.dataset_readers.drop import DropReader\n",
        "from allennlp.data.token_indexers import TokenCharactersIndexer, SingleIdTokenIndexer\n",
        "\n",
        "from typing import Any, Dict, Iterable, Optional, Union, Tuple, Set, List\n",
        "\n",
        "\n",
        "\n",
        "##data reader\n",
        "dataset_reader= DropShuffReaderInvalid(token_indexers= {'tokens':SingleIdTokenIndexer(lowercase_tokens=True), 'token_characters': TokenCharactersIndexer(min_padding_length=5)}, \\\n",
        "                           passage_length_limit= 400, question_length_limit=50, \\\n",
        "                           skip_when_all_empty=[\"passage_span\", \"question_span\", \"addition_subtraction\", \"counting\", 'invalid'], \\\n",
        "                           instance_format=\"drop\")\n",
        "\n",
        "validation_dataset_reader= DropShuffReaderInvalid(token_indexers= {'tokens':SingleIdTokenIndexer(lowercase_tokens=True), 'token_characters': TokenCharactersIndexer(min_padding_length=5)}, \\\n",
        "                           passage_length_limit= 1000, question_length_limit=100, \\\n",
        "                           skip_when_all_empty=[], \\\n",
        "                           instance_format=\"drop\")\n",
        "\n",
        "import random\n",
        "from allennlp.data.samplers.bucket_batch_sampler import BucketBatchSampler\n",
        "from allennlp.data.data_loaders.multiprocess_data_loader import MultiProcessDataLoader\n",
        "\n",
        "dataloaders={}\n",
        "#changed batch size from 16 to 8\n",
        "\n",
        "logger.info(\"Reading original training data from %s\", augmented_train_data_path)\n",
        "dataloaders['train_o']=MultiProcessDataLoader(reader= dataset_reader, \\\n",
        "                                              data_path=augmented_train_data_path, batch_sampler= BucketBatchSampler(batch_size=16), #8\n",
        "                                              cuda_device=torch.cuda.current_device())\n",
        "\n",
        "\n",
        "logger.info(\"Reading original dev data from %s\", augmented_dev_data_path)\n",
        "dataloaders['dev_o']=MultiProcessDataLoader(reader= validation_dataset_reader, \\\n",
        "                                            data_path=augmented_dev_data_path, batch_sampler= BucketBatchSampler(batch_size=16), #8\n",
        "                                            cuda_device=torch.cuda.current_device())\n",
        "\n",
        "\n",
        "##voca\n",
        "vocab_dir = os.path.join(serialization_dir, \"vocabulary\")\n",
        "\n",
        "if os.path.isdir(vocab_dir) and os.listdir(vocab_dir) is not None:\n",
        "  raise ConfigurationError(\n",
        "      \"The 'vocabulary' directory in the provided serialization directory is non-empty\"\n",
        "  )\n",
        "\n",
        "datasets_for_vocab_creation=None\n",
        "\n",
        "instances: Iterable[Instance] = (\n",
        "        instance\n",
        "        for key, data_loader in dataloaders.items()\n",
        "        if datasets_for_vocab_creation is None or key in datasets_for_vocab_creation\n",
        "        for instance in data_loader.iter_instances()\n",
        "    )\n",
        "vocab = Vocabulary.from_instances(min_count= {\"token_characters\": 200}, \\\n",
        "                  pretrained_files= {\"tokens\": \"https://allennlp.s3.amazonaws.com/datasets/glove/glove.840B.300d.lower.converted.zip\"}, \\\n",
        "                  only_include_pretrained_words= True,\n",
        "                  instances=instances)\n",
        "\n",
        "logger.info(f\"writing the vocabulary to {vocab_dir}.\")\n",
        "vocab.save_to_files(vocab_dir)\n",
        "logger.info(\"done creating vocab\")\n",
        "\n",
        "##\n",
        "from allennlp.nn.regularizers.regularizer_applicator import RegularizerApplicator\n",
        "from allennlp.nn.regularizers.regularizers import L2Regularizer\n",
        "from allennlp_models.rc.modules.seq2seq_encoders.qanet_encoder import QaNetEncoder\n",
        "from allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\n",
        "from allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention\n",
        "from allennlp.modules.text_field_embedders.basic_text_field_embedder import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding, TokenCharactersEncoder\n",
        "\n",
        "logger.info(\"indexing dataloaders with vocab\")\n",
        "\n",
        "dataloaders['train_o'].index_with(vocab)\n",
        "dataloaders['dev_o'].index_with(vocab)\n",
        "\n",
        "##model creation\n",
        "model=FINumericallyAugmentedQaNet(\n",
        "        vocab=vocab,\n",
        "        text_field_embedder=  BasicTextFieldEmbedder(\n",
        "        {\"tokens\": Embedding(embedding_dim=300, trainable=False, \\\n",
        "          pretrained_file=\"https://allennlp.s3.amazonaws.com/datasets/glove/glove.840B.300d.lower.converted.zip\",\n",
        "           vocab=vocab), \n",
        "         \"token_characters\": TokenCharactersEncoder(embedding=Embedding(embedding_dim=64, \n",
        "                                                                         vocab=vocab), \n",
        "                        encoder=CnnEncoder(embedding_dim= 64,\n",
        "                        num_filters= 200,\n",
        "                        ngram_filter_sizes=[5]))}),\n",
        "        num_highway_layers=2, \n",
        "        phrase_layer= QaNetEncoder(input_dim=128,\n",
        "            hidden_dim= 128,\n",
        "            attention_projection_dim=128,\n",
        "            feedforward_hidden_dim=128,\n",
        "            num_blocks= 1,\n",
        "            num_convs_per_block= 4,\n",
        "            conv_kernel_size= 7,\n",
        "            num_attention_heads= 8,\n",
        "            dropout_prob= 0.1,\n",
        "            layer_dropout_undecayed_prob= 0.1,\n",
        "            attention_dropout_prob= 0) ,\n",
        "        matrix_attention_layer= LinearMatrixAttention(tensor_1_dim= 128,\n",
        "            tensor_2_dim= 128,\n",
        "            combination= \"x,y,x*y\"),\n",
        "        modeling_layer= QaNetEncoder(input_dim=128,\n",
        "            hidden_dim= 128,\n",
        "            attention_projection_dim=128,\n",
        "            feedforward_hidden_dim=128,\n",
        "            num_blocks= 6,\n",
        "            num_convs_per_block= 2,\n",
        "            conv_kernel_size= 5,\n",
        "            num_attention_heads= 8,\n",
        "            dropout_prob= 0.1,\n",
        "            layer_dropout_undecayed_prob= 0.1,\n",
        "            attention_dropout_prob= 0) ,\n",
        "        dropout_prob = 0.1,\n",
        "        regularizer= RegularizerApplicator(regexes=[(\".*\", L2Regularizer(alpha= 1e-07))]) ,\n",
        "        answering_abilities= [\n",
        "            \"passage_span_extraction\",\n",
        "            \"question_span_extraction\",\n",
        "            \"addition_subtraction\",\n",
        "            \"counting\",\n",
        "            \"invalid\"]\n",
        "    )\n",
        "\n",
        "from allennlp.training.trainer import Trainer, GradientDescentTrainer\n",
        "from allennlp.training.optimizers import AdamOptimizer, SgdOptimizer\n",
        "from allennlp.training.moving_average import ExponentialMovingAverage\n",
        "\n",
        "adam=AdamOptimizer(model_parameters=model.named_parameters(), lr=5e-4, \n",
        "              betas=[0.8, 0.999], eps= 1e-07)\n",
        "\n",
        "model.cuda()\n",
        "\n",
        "trainer= GradientDescentTrainer(model=model, optimizer=adam, data_loader= dataloaders['train_o'], \n",
        "                                patience=10, validation_metric=\"+f1\", validation_data_loader=dataloaders['dev_o'], \n",
        "                                num_epochs=50, serialization_dir=serialization_dir, grad_norm=5,\n",
        "                                moving_average=ExponentialMovingAverage(model.named_parameters(), decay=0.9999)\n",
        "                              )\n",
        "\n",
        "trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-mivy-NAe1J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xM6INVp2aDKE"
      },
      "source": [
        "# Eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nYqnpemaPpp"
      },
      "outputs": [],
      "source": [
        "path= serialization_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNSQy9vXoulI"
      },
      "outputs": [],
      "source": [
        "from allennlp.data import Instance, Vocabulary, Batch, DataLoader\n",
        "from allennlp.data.dataset_readers import DatasetReader\n",
        "from allennlp_models.rc.dataset_readers.drop import DropReader\n",
        "from allennlp.data.token_indexers import TokenCharactersIndexer, SingleIdTokenIndexer\n",
        "\n",
        "from typing import Any, Dict, Iterable, Optional, Union, Tuple, Set, List\n",
        "import os\n",
        "from allennlp.nn.regularizers.regularizer_applicator import RegularizerApplicator\n",
        "from allennlp.nn.regularizers.regularizers import L2Regularizer\n",
        "from allennlp_models.rc.modules.seq2seq_encoders.qanet_encoder import QaNetEncoder\n",
        "from allennlp.modules.seq2vec_encoders.cnn_encoder import CnnEncoder\n",
        "from allennlp.modules.matrix_attention.linear_matrix_attention import LinearMatrixAttention\n",
        "from allennlp.modules.text_field_embedders.basic_text_field_embedder import BasicTextFieldEmbedder\n",
        "from allennlp.modules.token_embedders import Embedding, TokenCharactersEncoder\n",
        "import random\n",
        "from allennlp.data.samplers.bucket_batch_sampler import BucketBatchSampler\n",
        "from allennlp.data.data_loaders.multiprocess_data_loader import MultiProcessDataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "akCKjpEVaFzH"
      },
      "outputs": [],
      "source": [
        "vocab= Vocabulary.from_files(path+'/vocabulary')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09ESgt49a2_2"
      },
      "outputs": [],
      "source": [
        "pretrained_dict=torch.load(path+'/best.th')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azmTwWv_aEXz"
      },
      "outputs": [],
      "source": [
        "validation_dataset_reader= DropShuffReaderInvalid(token_indexers= {'tokens':SingleIdTokenIndexer(lowercase_tokens=True), 'token_characters': TokenCharactersIndexer(min_padding_length=5)}, \\\n",
        "                           passage_length_limit= 1000, question_length_limit=100, \\\n",
        "                           skip_when_all_empty=[], \\\n",
        "                           instance_format=\"drop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4NajFmdtr-9V"
      },
      "outputs": [],
      "source": [
        "dataset_reader=validation_dataset_reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPj9vM3oaut7",
        "outputId": "b2643222-6c0e-4f59-a9d8-180141167ecd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "1702926it [00:35, 47533.71it/s]\n"
          ]
        }
      ],
      "source": [
        "model=FINumericallyAugmentedQaNet(\n",
        "        vocab=vocab,\n",
        "        text_field_embedder=  BasicTextFieldEmbedder(\n",
        "        {\"tokens\": Embedding(embedding_dim=300, trainable=False, \\\n",
        "          pretrained_file=\"https://allennlp.s3.amazonaws.com/datasets/glove/glove.840B.300d.lower.converted.zip\",\n",
        "           vocab=vocab), #num_embeddings=vocab.get_vocab_size(\"tokens\")),\n",
        "         \"token_characters\": TokenCharactersEncoder(embedding=Embedding(embedding_dim=64, \n",
        "                                                                         vocab=vocab), \n",
        "                        encoder=CnnEncoder(embedding_dim= 64,\n",
        "                        num_filters= 200,\n",
        "                        ngram_filter_sizes=[5]))}),\n",
        "        num_highway_layers=2, \n",
        "        phrase_layer= QaNetEncoder(input_dim=128,\n",
        "            hidden_dim= 128,\n",
        "            attention_projection_dim=128,\n",
        "            feedforward_hidden_dim=128,\n",
        "            num_blocks= 1,\n",
        "            num_convs_per_block= 4,\n",
        "            conv_kernel_size= 7,\n",
        "            num_attention_heads= 8,\n",
        "            dropout_prob= 0.1,\n",
        "            layer_dropout_undecayed_prob= 0.1,\n",
        "            attention_dropout_prob= 0) ,\n",
        "        matrix_attention_layer= LinearMatrixAttention(tensor_1_dim= 128,\n",
        "            tensor_2_dim= 128,\n",
        "            combination= \"x,y,x*y\"),\n",
        "        modeling_layer= QaNetEncoder(input_dim=128,\n",
        "            hidden_dim= 128,\n",
        "            attention_projection_dim=128,\n",
        "            feedforward_hidden_dim=128,\n",
        "            num_blocks= 6,\n",
        "            num_convs_per_block= 2,\n",
        "            conv_kernel_size= 5,\n",
        "            num_attention_heads= 8,\n",
        "            dropout_prob= 0.1,\n",
        "            layer_dropout_undecayed_prob= 0.1,\n",
        "            attention_dropout_prob= 0) ,\n",
        "        dropout_prob = 0.1,\n",
        "        regularizer= RegularizerApplicator(regexes=[(\".*\", L2Regularizer(alpha= 1e-07))]) ,\n",
        "        answering_abilities= [\n",
        "            \"passage_span_extraction\",\n",
        "            \"question_span_extraction\",\n",
        "            \"addition_subtraction\",\n",
        "            \"counting\",\n",
        "            \"invalid\"]\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpGjnRaha9QK",
        "outputId": "de412eeb-7c5b-4b0f-ee83-83178666628c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "model.load_state_dict(pretrained_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4v2ZFrXMa91L",
        "outputId": "f67e81da-f91d-43f4-f08d-c321a13172f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SiameseNumericallyAugmentedQaNet(\n",
              "  (_text_field_embedder): BasicTextFieldEmbedder(\n",
              "    (token_embedder_tokens): Embedding()\n",
              "    (token_embedder_token_characters): TokenCharactersEncoder(\n",
              "      (_embedding): TimeDistributed(\n",
              "        (_module): Embedding()\n",
              "      )\n",
              "      (_encoder): TimeDistributed(\n",
              "        (_module): CnnEncoder(\n",
              "          (_activation): ReLU()\n",
              "          (conv_layer_0): Conv1d(64, 200, kernel_size=(5,), stride=(1,))\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (_embedding_proj_layer): Linear(in_features=500, out_features=128, bias=True)\n",
              "  (_highway_layer): Highway(\n",
              "    (_layers): ModuleList(\n",
              "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (_encoding_proj_layer): Linear(in_features=128, out_features=128, bias=True)\n",
              "  (_phrase_layer): QaNetEncoder(\n",
              "    (_encoder_blocks): ModuleList(\n",
              "      (0): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(3, 3), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(3, 3), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (2): Sequential(\n",
              "            (0): ConstantPad1d(padding=(3, 3), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (3): Sequential(\n",
              "            (0): ConstantPad1d(padding=(3, 3), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (_matrix_attention): LinearMatrixAttention(\n",
              "    (_activation): Linear()\n",
              "  )\n",
              "  (_modeling_proj_layer): Linear(in_features=512, out_features=128, bias=True)\n",
              "  (_modeling_layer): QaNetEncoder(\n",
              "    (_encoder_blocks): ModuleList(\n",
              "      (0): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "      (1): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "      (2): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "      (3): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "      (4): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "      (5): QaNetEncoderBlock(\n",
              "        (_conv_norm_layers): ModuleList(\n",
              "          (0): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (_conv_layers): ModuleList(\n",
              "          (0): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "          (1): Sequential(\n",
              "            (0): ConstantPad1d(padding=(2, 2), value=0)\n",
              "            (1): Conv1d(128, 128, kernel_size=(5,), stride=(1,), groups=128)\n",
              "            (2): Conv1d(128, 128, kernel_size=(1,), stride=(1,))\n",
              "            (3): ReLU()\n",
              "          )\n",
              "        )\n",
              "        (attention_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (attention_layer): MultiHeadSelfAttention(\n",
              "          (_combined_projection): Linear(in_features=128, out_features=384, bias=True)\n",
              "          (_output_projection): Linear(in_features=128, out_features=128, bias=True)\n",
              "          (_attention_dropout): Dropout(p=0, inplace=False)\n",
              "        )\n",
              "        (feedforward_norm_layer): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "        (feedforward): FeedForward(\n",
              "          (_activations): ModuleList(\n",
              "            (0): ReLU()\n",
              "            (1): Linear()\n",
              "          )\n",
              "          (_linear_layers): ModuleList(\n",
              "            (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "            (1): Linear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (_dropout): ModuleList(\n",
              "            (0): Dropout(p=0.1, inplace=False)\n",
              "            (1): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "        (residual_with_layer_dropout): ResidualWithLayerDropout()\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (_passage_weights_predictor): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (_question_weights_predictor): Linear(in_features=128, out_features=1, bias=True)\n",
              "  (_answer_ability_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=5, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.1, inplace=False)\n",
              "      (1): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_passage_span_start_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_passage_span_end_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_question_span_start_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_question_span_end_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_number_sign_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=384, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=3, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_count_number_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=128, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=10, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_invalid_predictor): FeedForward(\n",
              "    (_activations): ModuleList(\n",
              "      (0): ReLU()\n",
              "      (1): Linear()\n",
              "    )\n",
              "    (_linear_layers): ModuleList(\n",
              "      (0): Linear(in_features=256, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=2, bias=True)\n",
              "    )\n",
              "    (_dropout): ModuleList(\n",
              "      (0): Dropout(p=0.0, inplace=False)\n",
              "      (1): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (_dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs5QsS44bDlc"
      },
      "outputs": [],
      "source": [
        "from allennlp.training.util import evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hfP1_iUMbEPI"
      },
      "outputs": [],
      "source": [
        "dataset_reader= validation_dataset_reader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "paIL9JQ6bHQz"
      },
      "outputs": [],
      "source": [
        "prediction_dir='predictions/'\n",
        "def evaluate_dataset(model, data_path, model_name=None):\n",
        "  logger.info(\"Reading data from %s\", data_path)\n",
        "  file_name=os.path.basename(data_path)[:-5]+\"_preds.json\"\n",
        "  predictions_path=os.path.join(prediction_dir, model_name, file_name)\n",
        "  dataloader=MultiProcessDataLoader(reader= dataset_reader, \\\n",
        "                                              data_path=data_path, batch_sampler= BucketBatchSampler(batch_size=8), \n",
        "                                              cuda_device=torch.cuda.current_device())\n",
        "  dataloader.index_with(vocab)\n",
        "  return evaluate(model, dataloader, cuda_device=0, \\\n",
        "                  predictions_output_file=predictions_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZPiLQW4bZsP"
      },
      "outputs": [],
      "source": [
        "original_num_data_path='drop/drop_dataset_num.json'\n",
        "shuffled_1g_dev_data_path='drop/drop_dataset_num_sh_q_1gram.json'\n",
        "shuffled_2g_dev_data_path='drop/drop_dataset_num_sh_q_2gram.json'\n",
        "shuffled_3g_dev_data_path='drop/drop_dataset_num_sh_q_3gram.json'\n",
        "sh_p_3g= 'drop/drop_dataset_num_sh_p_3g.json'\n",
        "sh_p_2g= 'drop/drop_dataset_num_sh_p_2g.json'\n",
        "sh_p_1g= 'drop/drop_dataset_num_sh_p_1g.json'\n",
        "dev= 'drop/drop_dataset_dev.json'\n",
        "contrast_sets= 'drop/drop_contrast_sets_test.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49uR2bkHfFDF",
        "outputId": "5ea6f0ec-aefd-4d62-a490-652091727858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:50, 135.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.47, f1: 0.48, loss: inf ||: : 857it [01:36,  8.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of numset original\n",
            "{'em': 0.46795152577018545, 'f1': 0.47773835596437486, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, original_num_data_path,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of numset original')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SAIzdFmtBbn",
        "outputId": "ee96af2d-a826-4cd9-c02d-3f925756bda9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 9536it [01:07, 140.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 9536 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.45, f1: 0.49, loss: inf ||: : 1192it [02:13,  8.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  original devset\n",
            "{'em': 0.4540687919463087, 'f1': 0.48862730704698026, 'loss': inf}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, dev,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  original devset')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQVK7obpwG_r",
        "outputId": "bf0e2bdc-02f9-4018-91b0-c74f066b2361"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 947it [00:09, 95.22it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 947 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.28, f1: 0.34, loss: inf ||: : 119it [00:13,  8.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  original contrast sets\n",
            "{'em': 0.27666314677930304, 'f1': 0.34233368532206965, 'loss': inf}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, contrast_sets,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  original contrast sets')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W1BonVENwX0O",
        "outputId": "37149362-9e9e-479c-ae74-be23c8d4a6e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:48, 140.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.07, f1: 0.07, loss: inf ||: : 857it [01:34,  9.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled q 1g\n",
            "{'em': 0.06760110965104395, 'f1': 0.06782595999415973, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, shuffled_1g_dev_data_path,  f'naqanet_ngrams_final_seed_{seed}')\n",
        "print('results of  shuffled q 1g')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an5-naCqwg0d",
        "outputId": "e8f58f23-d589-476c-9a1e-86d198ad4f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:48, 140.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.08, f1: 0.08, loss: inf ||: : 857it [01:34,  9.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled q 2g\n",
            "{'em': 0.07563147904803622, 'f1': 0.07684041465907433, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, shuffled_2g_dev_data_path,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  shuffled q 2g')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqf61jg5wkhK",
        "outputId": "1fe48cd9-c6c4-4d01-b2a1-6d3187e160c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:48, 140.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.09, f1: 0.09, loss: inf ||: : 857it [01:34,  9.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled q 3g\n",
            "{'em': 0.08745802306906118, 'f1': 0.08944955467951524, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, shuffled_3g_dev_data_path,  f'naqanet_ngrams_final_seed_{seed}')\n",
        "print('results of  shuffled q 3g')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oINCtUqzwv0j",
        "outputId": "848ffbe5-cfa9-454a-9cf8-eb3ea29d98bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:48, 140.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.01, f1: 0.01, loss: inf ||: : 857it [01:33,  9.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled p 1g\n",
            "{'em': 0.012994597751496568, 'f1': 0.013067601109651043, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, sh_p_1g,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  shuffled p 1g')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9b3yNeiwxnt",
        "outputId": "afa8c94a-c9ea-4bd7-c637-09da10655c2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [00:49, 138.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.01, f1: 0.01, loss: inf ||: : 857it [01:34,  9.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled p 2g\n",
            "{'em': 0.012994597751496568, 'f1': 0.013067601109651043, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, sh_p_2g,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  shuffled p 2g')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbNGmSBFwx3k",
        "outputId": "a0cd3d17-9399-407d-833f-b3adf358d639"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading instances: 6849it [01:10, 96.48it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipped 0 questions, kept 6849 questions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "em: 0.02, f1: 0.02, loss: inf ||: : 857it [01:33,  9.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results of  shuffled p 3g\n",
            "{'em': 0.017082785808147174, 'f1': 0.01715578916630165, 'loss': inf}\n"
          ]
        }
      ],
      "source": [
        "result= evaluate_dataset(model, sh_p_3g,  f'naqanet_ngrams_final_seed_{seed}' )\n",
        "print('results of  shuffled p 3g')\n",
        "print(result)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyM0mDBCGhYHCLkMlcS2jD0w",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}